[{"authors":null,"categories":null,"content":"Zijian Hu is a Machine Learning Research Engineer at Scale AI. He is interested in building efficient machine learning systems that can operate in noisy environments and can be trained and adapted with minimal supervision.\n  Download my CV (last updated on Nov 17, 2022) for more details.\n","date":1682578800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1682578800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Zijian Hu is a Machine Learning Research Engineer at Scale AI. He is interested in building efficient machine learning systems that can operate in noisy environments and can be trained and adapted with minimal supervision.","tags":null,"title":"Zijian Hu","type":"authors"},{"authors":["ram-nevatia"],"categories":null,"content":"The following paragraph is taken from Dr Ram Nevatia\u0026rsquo;s personal website.\nRamakant Nevatia (nevatia at usc.edu) received his Ph.D. degree from Stanford University with specialty in the area of computer vision. He is currently a Professor of Computer Science and Electrical Engineering and Director of the Institute for Robotics and Intelligent Systems at the University of Southern California. Dr. Nevatia has made important contributions to several areas of computer vision including the topics of shape description, object recognition, stereo analysis aerial image analysis, tracking of humans and event recognition. He is author of two books, several book chapters and over 150 referred technical papers. He is a Fellow of the Institute of Electrical and Electronics Engineers (IEEE) and of the American Association for Artificial Intelligence (AAAI).\nDr. Nevatia has been a principal investigator of major Government funded computer vision research programs and has been funded by agencies such as DARPA, IARPA, ONR and DoJ. His work has been cited more than 25,000 times and he has a citation h-index of 76. He has supervised more than 30 Ph.D. dissertations. His former students are employed in top companies such as Google, Facebook, IBM, Amazon, GE Research, Siemens Research, Samsung, Qualcomm and many others\n","date":1624086000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624086000,"objectID":"e9639d05b72b8e8ea8b935dcc6005295","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Ram Nevatia\u0026rsquo;s personal website.\nRamakant Nevatia (nevatia at usc.edu) received his Ph.D. degree from Stanford University with specialty in the area of computer vision.","tags":null,"title":"Ram Nevatia","type":"authors"},{"authors":["chris-birmingham"],"categories":null,"content":"The following paragraph is taken from Chris Birmingham\u0026rsquo;s personal website.\nI am a PhD student in Maja Mataric’s Interaction Lab at the University of Southern California. I am driven by the question - how can we use technology to bring people together rather than isolate them? I believe social robots may bring people out of the digital world and back together in the physical world. I currently focus on understanding the dynamics of multi-party interaction between groups of people and a robot. You can read more about this work here.\nAs part of this work I spend most of my time developing deep learning models for understanding and predicting human behavior in order to support more intelligent interactions.\n","date":1590908400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590908400,"objectID":"36e157e86a1548efc035a34affb613cf","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Chris Birmingham\u0026rsquo;s personal website.\nI am a PhD student in Maja Mataric’s Interaction Lab at the University of Southern California. I am driven by the question - how can we use technology to bring people together rather than isolate them?","tags":null,"title":"Chris Birmingham","type":"authors"},{"authors":["maja-mataric"],"categories":null,"content":"The following paragraph is taken from Dr Maja Matarić\u0026rsquo;s personal website.\nMaja Matarić is distinguished professor and Chan Soon-Shiong chair in Computer Science Department, Neuroscience Program, and the Department of Pediatrics and Interim Vice President for Research at the University of Southern California, founding director of the USC Robotics and Autonomous Systems Center (RASC), co-director of the USC Robotics Research Lab, and the lead of the Viterbi K-12 STEM Center. She received her PhD in Computer Science and Artificial Intelligence from MIT in 1994, MS in Computer Science from MIT in 1990, and BS in Computer Science from the University of Kansas in 1987.\n","date":1590908400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590908400,"objectID":"0f3f111ba48ba5c63190e969f242e69d","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Maja Matarić\u0026rsquo;s personal website.\nMaja Matarić is distinguished professor and Chan Soon-Shiong chair in Computer Science Department, Neuroscience Program, and the Department of Pediatrics and Interim Vice President for Research at the University of Southern California, founding director of the USC Robotics and Autonomous Systems Center (RASC), co-director of the USC Robotics Research Lab, and the lead of the Viterbi K-12 STEM Center.","tags":null,"title":"Maja J. Matarić","type":"authors"},{"authors":["houston-claure"],"categories":null,"content":"AI Researcher | Robotics \u0026amp; Human-Computer Interaction Researcher | PhD Candidate at Cornell University\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"382433cc53e7038d1622efec500432ee","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"AI Researcher | Robotics \u0026amp; Human-Computer Interaction Researcher | PhD Candidate at Cornell University","tags":null,"title":"Houston Claure","type":"authors"},{"authors":["stefanos-nikolaidis"],"categories":null,"content":"The following paragraph is taken from Dr Stefanos Nikolaidis' personal website.\nI am an assistant professor in computer science at the University of Southern California, where I run the Interactive and Collaborative Autonomous Robotic Systems (ICAROS) lab. I graduated with a PhD from the CMU Robotics Institute and with a MS from MIT.\nThe ICAROS lab focuses on the core computational challenges in human-robot interaction: what are good models of human behavior, how to learn such models from noisy samples, and how to robustly generate actions for robotic teammates in large scale real-world applications. Our research spans the whole spectrum of human-robot interaction science: from distilling the fundamental mathematical principles that govern interactive behaviors, to developing approximation algorithms for deployed robotic systems and testing them \u0026ldquo;in the wild\u0026rdquo; with actual end users.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"30593b4448225b8efe979c6b06f52a76","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Stefanos Nikolaidis' personal website.\nI am an assistant professor in computer science at the University of Southern California, where I run the Interactive and Collaborative Autonomous Robotic Systems (ICAROS) lab.","tags":null,"title":"Stefanos Nikolaidis","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://www.zijianhu.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Zijian Hu"],"categories":[],"content":"This is an ongoing project. The details are subject to change.\n","date":1682578800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682578800,"objectID":"ff5ce4c1fc59a4e950adef3bcaaf4d3c","permalink":"https://www.zijianhu.com/project/fedllm/","publishdate":"2023-04-27T00:00:00-07:00","relpermalink":"/project/fedllm/","section":"project","summary":"FedML AI platform is democratizing large language models (LLMs) by enabling enterprises to train their own models on proprietary data. Today, we release FedLLM, an MLOps-supported training pipeline that allows for building domain-specific LLMs on proprietary data. The platform enables data collaboration, computation collaboration, and model collaboration, and supporting training on centralized and geo-distributed GPU clusters, as well as federated learning for data silos. FedLLM is compatible with popular LLM libraries such as HuggingFace and DeepSpeed, and is designed to improve efficiency and security/privacy. To get started, FedML users \u0026 developers only need to add 100 lines of source code. The complex steps of deployment and orchestration of training in enterprise environments are all handled by FedML MLOps platform.","tags":["Machine Learning","Natural Language Processing","Large Language Model","Federated Learning"],"title":"FedLLM: Build Your Own Large Language Models on Proprietary Data using the FedML Platform","type":"project"},{"authors":["Zijian Hu","Zhengyu Yang","Xuefeng Hu","Ram Nevatia"],"categories":[],"content":"Table of Contents  Introduction  Semi-supervised Learning   Method  Problem Description Augmentation Strategy Pseudo-labeling Loss SimPLE Algorithm      Introduction Deep learning has recently achieved state-of-the-art performance on many computer vision tasks. labeling large datasets is very expensive and often not feasible, especially in domains that require expertise to provide labels. Semi-Supervised Learning (SSL), on the other hand, can take advantage of partially labeled data, which is much more easily available, as shown in Figure 1, for example.\n Illustration of an image set with a limited amount of labeled images among a large number of unlabeled images.  Semi-supervised Learning A critical problem in semi-supervised learning is how to generalize the information learned from limited label data to unlabeled data. Following the continuity assumption that close data have a higher probability of sharing the same label, many approaches have been developed, including the recently proposed Label Propagation.\nAnother critical problem in semi-supervised learning is how to directly learn from the large amount of unlabeled data. Maintaining consistency between differently augmented unlabeled data has been recently studied and proved to be an effective way to learn from unlabeled data in both self-supervised learning.\nThe recently proposed MixMatch combines the above techniques and designed a unified loss term to let the model learn from differently augmented labeled and unlabeled data, together with the mix-up technique, which encourages convex behavior between samples to increase model\u0026rsquo;s generalization ability. ReMixMatch further improves the MixMatch by introducing the distribution alignment and Augmentation Anchoring techniques, which allows the model to accommodate and leverage from the heavily augmented samples. FixMatch simplified its previous works by introducing a confidence threshold into its unsupervised objective function and achieves state-of-the-art performance over the standard benchmarks.\nIn this paper, we propose to take advantage of the relationship between different unlabeled samples. We introduce a novel Pair Loss, which encourages a pair of similar unlabeled samples (in the augmented space) to have similar predictions if at least one of them is of high confidence in its prediction. Combining the techniques developed by the MixMatch family, we propose the SimPLE algorithm. As shown in Figure 2, the SimPLE algorithm generates pseudo labels of unlabeled samples by averaging and sharpening the predictions on multiple weakly augmented variations of the same sample.\n An overview of the proposed SimPLE algorithm.  Then, we use both the labels and pseudo labels to compute the supervised cross-entropy loss and unsupervised $L2$ distance loss. These two terms push the decision boundaries to go through low-density areas and encourage consistency among different variations of the same samples. Finally, with the newly proposed Pair Loss, we harness the relationships among the pseudo labels of different samples by encouraging consistency among different unlabeled samples which share a great similarity.\nMethod To take full advantage of the vast quantity of unlabeled samples in SSL problems, we propose the SimPLE algorithm that focuses on the relationship between unlabeled samples.\nProblem Description We define the semi-supervised image classification problem as following. In a $L$-class classification setting, we have:\n $\\mathcal{X}=\\left(\\left(x_{b}, y_{b}\\right) ; b \\in(1, \\ldots, B)\\right)$: a batch of labeled data $\\mathcal{U}=\\left(u_{b} ; b \\in(1, \\ldots, B)\\right)$: a batch of unlabeled data $\\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid x ; \\theta\\right)$: the model\u0026rsquo;s predicted softmax class probability of input $x$ parameterized by weight $\\theta$  Augmentation Strategy Our algorithm uses Augmentation Anchoring, in which pseudo labels come from weakly augmented samples act as \u0026ldquo;anchor,\u0026rdquo; and we align the strongly augmented samples to the \u0026ldquo;anchor.\u0026rdquo; Our weak augmentation, follows that of MixMatch family, contains a random cropping followed by a random horizontal flip. We use RandAugment or a fixed augmentation strategy that contains difficult transformations such as random affine and color jitter as strong augmentation. For every batch, RandAugment randomly selects a fixed number of augmentations from a predefined pool; the intensity of each transformation is determined by a magnitude parameter.\nPseudo-labeling Our pseudo labeling is based on the label guessing technique used in MixMatch. We first take the average of the model\u0026rsquo;s predictions of several weakly augmented versions of the same unlabeled sample as its pseudo label. As the prediction is averaged from $K$ slight perturbations of the same input instead of $K$ severe perturbation or a single perturbation, the guessed pseudo label should be more stable. Then, we use the sharpening operation defined in MixMatch to increase the temperature of the label\u0026rsquo;s distribution: $$\\operatorname{Sharpen}(p, T):= \\frac{p^{\\frac{1}{T}}}{\\textbf{1}^\\top p^{\\frac{1}{T}}}$$\nAs the peak of the pseudo label\u0026rsquo;s distribution is \u0026ldquo;sharpened,\u0026rdquo; the network will push this sample further away from the decision boundary. Additionally, following the practice of MixMatch, we use the exponential moving average of the model at each time step to guess the labels.\nLoss Our loss consists of three terms, $\\mathcal{L_X}$, $\\mathcal{L_U}$, and $\\mathcal{L_P}$, representing the supervised loss, the unsupervised loss, and the Pair Loss respectively.\n$$ \\begin{align} \\mathcal{L} \u0026amp;= \\mathcal{L_X} + \\lambda_{\\mathcal{U}} \\mathcal{L_U} + \\lambda_{\\mathcal{P}} \\mathcal{L_P} \\\\\n\\mathcal{L_X} \u0026amp;= \\frac{1}{\\left| \\mathcal{X}' \\right|} \\sum_{x,y \\in \\hat{\\mathcal{X}}} H\\left(y, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid x ; \\theta\\right)\\right) \\\\\n\\mathcal{L_U} \u0026amp;= \\frac{ \\sum_{u,q \\in \\hat{\\mathcal{U}}} \\mathbb{1}_{\\left(\\max\\left(q\\right) \u0026gt; \\tau_c\\right)} \\left| q - \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid u ; \\theta\\right) \\right|^{2}_{2} }{L \\left| \\hat{\\mathcal{U}} \\right|} \\end{align} $$\n$\\mathcal{L_X}$ calculates the cross-entropy of weakly augmented labeled samples, and $\\mathcal{L_U}$ represents the $L_2$ distance between strongly augmented samples and their pseudo labels, filtered by high confidence threshold. Notice that $\\mathcal{L_U}$ only enforces the consistency among different perturbations of the same samples but not the consistency among different samples.\nAs we aim to exploit the relationship among unlabeled samples, we hereby introduce a novel loss term, Pair Loss, that allows information to propagate implicitly between different unlabeled samples. In Pair Loss, we use a high confidence pseudo label of an unlabeled point, $p$, as an \u0026ldquo;anchor.\u0026rdquo; All unlabeled samples whose pseudo labels are similar enough to $p$ need to align their predictions under severe perturbation to the \u0026ldquo;anchor.\u0026rdquo;\n Pair Loss Overview.  Figure 3 offers an overview of this selection process. During this process, the similarity threshold \u0026ldquo;extended\u0026rdquo; our confidence threshold in an adaptive manner, as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level. Formally, we defined the Pair Loss as following:\n$$ \\begin{aligned} \\mathcal{L_P} \u0026amp;= \\frac{1}{\\binom{K\u0026rsquo;B}{2}} \\sum_{ \\substack{ i,j \\in \\left[\\left|\\mathcal{U}'\\right|\\right], i \\ne j \\\\\n\\left(v_l, q_l\\right) = \\mathcal{U}'_{i} \\\\\n\\left(v_r, q_r\\right) = \\mathcal{U}'_{j} } } \\varphi_{\\tau_c}\\left(\\max\\left(q_l\\right)\\right) \\\\\n\u0026amp;\\cdot \\varphi_{\\tau_s}\\left(f_{\\operatorname{sim}}\\left(q_l, q_r\\right)\\right) \\\\\n\u0026amp;\\cdot f_{\\operatorname{dist}}\\left(q_l, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid v_r ; \\theta\\right)\\right) \\end{aligned} $$\nHere, $\\tau_c$ and $\\tau_s$ denote the confidence threshold and similarity threshold respectively. $\\varphi_t(x)=\\mathbb{1}_{\\left(x \u0026gt; t\\right)} x$ is a hard threshold function controlled by threshold $t$. $f_{\\operatorname{sim}}\\left(p, q\\right)$ measures the similarity between two probability vectors $p, q$ by Bhattacharyya coefficient. The coefficient is bounded between $[0, 1]$, and represents the size of the overlapping portion of the two discrete distributions:\n$$f_{\\operatorname{sim}}\\left(p, q\\right) = \\sqrt{p} \\cdot \\sqrt{q}$$\n$f_{\\operatorname{dist}}\\left(p, q\\right)$ measures the distance between two probability vectors $p, q$. As $f_{\\operatorname{sim}}\\left(p, q\\right)$ is bounded between $[0, 1]$, we simply choose the distance function to be $f_{\\operatorname{dist}}\\left(p, q\\right) = 1 - f_{\\operatorname{sim}}\\left(p, q\\right)$.\nAlthough based on analysis, we found that $\\cos(\\cos^{-1}(\\sqrt{\\tau_c})+\\cos^{-1}(\\tau_s))^2$ is the minimal confidence a label need to have for it to be selected by both thresholds, such low confidence label are rarely selected in practice. Based on empirical evidence, we believe this is caused by the fact a label $p$ that can pass through the high confidence threshold typically has a near one hot distribution. Thus, for another label $q$ to fall in the similarity threshold of $q$, it must also have relatively high confidence. Due to this property, the Pair Loss is not very sensitive to the choices of hyperparameters $\\tau_s$, $\\tau_c$, which we will show empirically in later section.\nSimPLE Algorithm By putting together all the components introduced in this section, we now present the SimPLE algorithm. During training, for a mini-batch of samples, SimPLE will first augment both labeled and unlabeled samples with both weak and strong augmentations. The pseudo labels of the unlabeled samples are obtained by averaging and then sharpening the models' predictions on the weakly augmented unlabeled samples. Finally, we optimize the three loss terms based on augmented samples and pseudo labels. During testing, SimPLE uses the exponential moving average of the weights of the model to do prediction, as what is done by MixMatch. Figure 2 gives an overview of the algorithm, and the complete training algorithm is described in Alg. 1.\n  SimPLE algorithm.  The experiment section will be updated soon\n","date":1624086000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624086000,"objectID":"cb7c155e9547637654e2d0947afb3b0b","permalink":"https://www.zijianhu.com/publication/hu-2020-simple/","publishdate":"2020-12-05T00:00:00-07:00","relpermalink":"/publication/hu-2020-simple/","section":"publication","summary":"We propose a novel algorithm that focuses on the less studied relationship between the unlabeled data. Our algorithm achieves state-of-the-art performance on standard benchmarks","tags":["Machine Learning","Semi-Supervised Learning","Low-Shot Learning"],"title":"SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":"This is an ongoing project. The details are subject to change.\nThe following project description is taken from DARPA.\n In supervised machine learning (ML), the ML system learns by example to recognize things, such as objects in images or speech. Humans provide these examples to ML systems during their training in the form of labeled data. With enough labeled data, we can generally build accurate pattern recognition models.\nThe problem is that training accurate models currently requires lots of labeled data. For tasks like machine translation, speech recognition or object recognition, deep neural networks (DNNs) have emerged as the state of the art, due to the superior accuracy they can achieve. To gain this advantage over other techniques, however, DNN models need more data, typically requiring 109 or 1010 labeled training examples to achieve good performance.\nThe commercial world has harvested and created large sets of labeled data for training models. These datasets are often created via crowdsourcing: a cheap and efficient way to create labeled data. Unfortunately, crowdsourcing techniques are often not possible for proprietary or sensitive data. Creating data sets for these sorts of problems can result in 100x higher costs and 50x longer time to label.\nTo make matters worse, machine learning models are brittle, in that their performance can degrade severely with small changes in their operating environment. For instance, the performance of computer vision systems degrades when data is collected from a new sensor and new collection viewpoints. Similarly, dialog and text understanding systems are very sensitive to changes in formality and register. As a result, additional labels are needed after initial training to adapt these models to new environments and data collection conditions. For many problems, the labeled data required to adapt models to new environments approaches the amount required to train a new model from scratch.\nThe Learning with Less Labeling (LwLL) program aims to make the process of training machine learning models more efficient by reducing the amount of labeled data required to build a model by six or more orders of magnitude, and by reducing the amount of data needed to adapt models to new environments to tens to hundreds of labeled examples.\n LwLL Evaluation The evaluation has 3 types of tasks: Image Classification, Object Detection and Machine Translation. Our team is focusing on the image classification task. Each task includes a base phase and an adaptation pha where each phase consists of 6 to 8 stages. For image classification, pre-trained models on predefined whitelisted datasets are allowed.\n Number of labels in each stage  The full training set without any label is given at the beginning. At stage 1, can request 1 label per category in the training set (label budget is 1). As the training progress, the label budget is increased.\n LwLL evaluation model pipeline  Our team used a few-shot learning method for the first 2 to 3 checkpoints when the labeled set size is small. Once sufficient number of labeled data samples become available, the training was handed to our semi-supervised algorithm.\nSimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification  SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification Zijian Hu, Zhengyu Yang, Xuefeng Hu, Ram Nevatia  PDF  Cite  Code  Project  Poster  Slides  Video  DOI     SimPLE algorithm  We proposed a novel semi-supervised classification algorithm, SimPLE (Figure 3), that focuses on the less studied relationship between the high confidence unlabeled data that are similar to each other.\n Pair Loss Overview.  As shown in Figure 4, the new proposed Pair Loss minimizes the statistical distance between high confidence pseudo labels with similarity above a certain threshold. The similarity threshold \u0026ldquo;extended\u0026rdquo; our confidence threshold in an adaptive manner, as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level.\nFormally, we defined the Pair Loss as the following:\n$$ \\begin{aligned} \\mathcal{L_P} \u0026amp;= \\frac{1}{\\binom{KB}{2}} \\sum_{\\mathcal{U}'} \\mathbb{1}_{\\max\\left(q_l\\right) \u0026gt; \\tau_c} \\cdot \\mathbb{1}_{f_{\\operatorname{sim}}\\left(q_l,q_r\\right) \u0026gt; \\tau_s} \\\\\n\u0026amp;\\cdot f_{\\operatorname{dist}}\\left(q_l, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid v_r ; \\theta\\right)\\right) \\end{aligned} $$\nNotations:\n $K$: number of augmentations $B$: batch size $\\mathcal{U}'$: unlabeled batch $\\tau_c$: confidence threshold $\\tau_s$: similarity threshold $f_{\\operatorname{sim}}\\left(\\cdot,\\cdot\\right)$: similarity function  We use Bhattacharyya coefficient.   $f_{\\operatorname{dist}}\\left(\\cdot,\\cdot\\right)$: distance function  We use $1-$ Bhattacharyya coefficient.    Experimental Results    Dataset Num. Labels Method Backbone Top-1 Accuracy     CIFAR-100 10000 MixMatch WRN 28-8 71.69%   ReMixMatch WRN 28-8 76.97%   FixMatch WRN 28-8 77.40%   SimPLE WRN 28-8 78.11%   Mini-ImageNet 4000 MixMatch WRN 28-2 55.47%   MixMatch Enhanced WRN 28-2 60.50%   SimPLE WRN 28-2 66.55%   ImageNet to DomainNet-Real 3795 MixMatch ResNet-50 35.34%   MixMatch Enhanced ResNet-50 35.16%   SimPLE ResNet-50 50.90%   DomainNet-Real to Mini-ImageNet 4000 MixMatch WRN 28-2 53.39%   MixMatch Enhanced WRN 28-2 55.75%   SimPLE WRN 28-2 58.73%    Our algorithm, SimPLE, achieved state-of-the-art performance on standard SSL benchmarks and achieved the best accuracy on some tasks in LwLL evaluation. We also evaluated our method in the transfer setting where our algorithm outperforms prior works and supervised baseline by a large margin.\nDevelopment Detail  Designed a novel algorithm for semi-supervised classification Evaluated our algorithm on standard benchmarks (CIFAR-10, CIFAR-100, SVHN, Mini-ImageNet) Evaluated our algorithm in the transfer learning setting (on Mini-ImageNet, DomainNet-Real, AID, RESISC45), where models are initialized models pretrained on ImageNet or DomainNet-Real Distributed training with PyTorch Distributed Data Parallel GPU accelerated data augmentation with Kornia  Related Publications  Zijian Hu, Zhengyu Yang, Xuefeng Hu, Ram Nevatia  (2021). SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification. 2021 Conference on Computer Vision and Pattern Recognition (CVPR).  PDF  Cite  Code  Project  Poster  Slides  Video  DOI   ","date":1607151600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607151600,"objectID":"5815644c6c414909f37084eb0375343a","permalink":"https://www.zijianhu.com/project/lwll/","publishdate":"2020-12-05T00:00:00-07:00","relpermalink":"/project/lwll/","section":"project","summary":"We propose a novel algorithm for semi-supervised classification that achieves state-of-the-art performance on standard benchmarks and outperform previous works on transfer setting by a large margin.","tags":["Computer Vision","Machine Learning","Semi-Supervised Learning"],"title":"Learning with Less Labeling (LwLL)","type":"project"},{"authors":["Zijian Hu"],"categories":[],"content":"Introduction This example carefully replicates the behavior of TensorFlow\u0026rsquo;s tf.train.ExponentialMovingAverage.\nNotice that when applying EMA, only the trainable parameters should be changed; for PyTorch, we can get the trainable parameters by model.parameters() or model.named_parameters() where model is a torch.nn.Module.\nSince my implementation creates a copy of the input model (i.e. shadow), the buffers needs to be copied to shadow whenever update() is invoked.\nAlternative Implementation You could implement shadow as a dict, for detail of this version see 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现. One problem with that implementation is that shadow needs to be manually saved since shadow parameters are not stored in state_dict; a simple fix to this problem is to register all shadow parameters by calling register_parameter(\u0026lt;parameter name\u0026gt;).\nImplementations import torch\rfrom torch import nn\rfrom copy import deepcopy\rfrom collections import OrderedDict\rfrom sys import stderr\r# for type hint\rfrom torch import Tensor\rclass EMA(nn.Module):\rdef __init__(self, model: nn.Module, decay: float):\rsuper().__init__()\rself.decay = decay\rself.model = model\rself.shadow = deepcopy(self.model)\rfor param in self.shadow.parameters():\rparam.detach_()\r@torch.no_grad()\rdef update(self):\rif not self.training:\rprint(\u0026quot;EMA update should only be called during training\u0026quot;, file=stderr, flush=True)\rreturn\rmodel_params = OrderedDict(self.model.named_parameters())\rshadow_params = OrderedDict(self.shadow.named_parameters())\r# check if both model contains the same set of keys\rassert model_params.keys() == shadow_params.keys()\rfor name, param in model_params.items():\r# see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\r# shadow_variable -= (1 - decay) * (shadow_variable - variable)\rshadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))\rmodel_buffers = OrderedDict(self.model.named_buffers())\rshadow_buffers = OrderedDict(self.shadow.named_buffers())\r# check if both model contains the same set of keys\rassert model_buffers.keys() == shadow_buffers.keys()\rfor name, buffer in model_buffers.items():\r# buffers are copied\rshadow_buffers[name].copy_(buffer)\rdef forward(self, inputs: Tensor, return_feature: bool = False) -\u0026gt; Tensor:\rif self.training:\rreturn self.model(inputs, return_feature)\relse:\rreturn self.shadow(inputs, return_feature)\r Reference  tf.train.ExponentialMovingAverage 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现  ","date":1595487600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595487600,"objectID":"ade87a5ed3dd7bd98a96c8ab9d9891c4","permalink":"https://www.zijianhu.com/post/pytorch/ema/","publishdate":"2020-07-23T00:00:00-07:00","relpermalink":"/post/pytorch/ema/","section":"post","summary":"This tutorial shows how to correctly implement EMA for PyTorch","tags":["Machine Learning","PyTorch","Tutorial"],"title":"PyTorch: Exponential Moving Average (EMA) Example","type":"post"},{"authors":["Chris Birmingham","Zijian Hu","Kartik Mahajan","Eli Reber","Maja J. Matarić"],"categories":[],"content":"","date":1590908400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590908400,"objectID":"faec5e510f3f74c79cf99b94ff1c2a4b","permalink":"https://www.zijianhu.com/publication/birmingham-icra-2020/","publishdate":"2020-02-22T00:00:00-07:00","relpermalink":"/publication/birmingham-icra-2020/","section":"publication","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction"],"title":"Can I Trust You? A User Study of Robot Mediation of a Support Group","type":"publication"},{"authors":["Zijian Hu","Chris Birmingham"],"categories":[],"content":"In this project, we designed and evaluated a novel framework for robot mediation of a support group. We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (Wizard-of-Oz). At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings. In a second-author paper at International Conference on Robotics and Automation (ICRA), we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.\nThe following project description is taken from Interaction Lab website.\n Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\n Study Design  Volunteers demonstrating the study setup  In each study session, three participants were seated around the end of a table with a seated NAO robot as shown in Figure 1. The NAO robot, acting as a group moderator, was positioned towards the participants. On the table, a 360-degree microphone and 3 cameras facing directly to the participants' face were placed. Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members. The robot operator was seated behind a one-way mirror hidden from participants.\n   Sensitivity Question Disclosure     Low What do you like about school? When I feel stressed, I think my circuits might overload.\nDoes anyone else feel the same way?   Medium What are some of the hardest parts of school for you? Sometimes I worry I am inadequate for this school.\nDoes anyone else sometimes feel that too?   Hard What will happen if you don’t succeed in school? Sometimes I worry about if I belong here. Does anyone else feel the same way?    During the interaction, the robot can ask questions or make disclosures. A total of 16 questions and 6 disclosures are available. On average, 12 questions and 3 disclosures were made by the robot in each session.\nThe questions and disclosures are grouped into low, medium, and high sensitivity as illustrated in the below table.\n Self-annotation at the end of each session   Self-annotation UI  To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.\nThe detail for the procedure of the study can be found here.\nDevelopment Detail  Control architecture  As shown in Figure 3. The wizard controls the robot through the Wizard-of-Oz web interface.\nFor project development, my contributions includes:\n Developed NAO control program Designed and implemented web-based Wizard of Oz controller Designed and implemented self-annotation website Developed data collection program for one depth camera, three webcams and one 360 degree microphone Data post-processing for data whitening and fast data loading Turn-taking prediction with Temporal Convolutional Networks (TCN) and LSTM for multi-modal input  Related Publications  Chris Birmingham, Zijian Hu, Kartik Mahajan, Eli Reber, Maja J. Matarić  (2020). Can I Trust You? A User Study of Robot Mediation of a Support Group. 2020 International Conference on Robotics and Automation (ICRA).  PDF  Cite  Project  DOI   ","date":1590908400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590908400,"objectID":"52cee06d58cafb39b25827e34b3c439f","permalink":"https://www.zijianhu.com/project/multi_party/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/multi_party/","section":"project","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction","Machine Learning"],"title":"Trust in Multi-Party Human-Robot Interaction","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Marton Demeter","Zijian Hu","Maja J. Matarić"],"categories":[],"content":"Linked material YouTube video for \u0026ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot\u0026rdquo;\n  ","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"1c5583df9fe020a761882594c6f66d02","permalink":"https://www.zijianhu.com/publication/fitter-roman-2019-hardware/","publishdate":"2019-09-13T00:00:00-07:00","relpermalink":"/publication/fitter-roman-2019-hardware/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":"In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.\nThe following project description is taken from Interaction Lab website.\n Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\n Development Detail We used an Ohmni robot equipped with an arm and a Linux PC.\n  For project development, my contributions consist of the following:\n Developed customized web user interface for robot arm control Designed and implemented communication protocol and software between the Linux PC and the Ohmni server Developed user interface with a turning dial for robot arm control  Related Publications   Naomi T. Fitter, Youngseok Joung, Marton Demeter, Zijian Hu, Maja J. Matarić  (2019). Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot. 2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man).  PDF  Cite  Project  Video  DOI    Naomi T. Fitter, Youngseok Joung, Zijian Hu, Marton Demeter, Maja J. Matarić  (2019). User Interface Tradeoffs for Remote Deictic Gesturing. 2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man).  PDF  Cite  Project  DOI   ","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"352b7f240bc94841fab78c090a703aa5","permalink":"https://www.zijianhu.com/project/nri_kids/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/nri_kids/","section":"project","summary":"We developed and evaluated various control methods and interfaces for mobile remote presence robots (we used [Ohmni](https://ohmnilabs.com/products/ohmnirobot/) robot) for remote K-12 education","tags":["Robotics","Human-Robot Interaction"],"title":"Telepresence Robot for K-12 Remote Education","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Zijian Hu","Marton Demeter","Maja J. Matarić"],"categories":[],"content":"","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"7f10e633ecbf290aab868bbe523130d0","permalink":"https://www.zijianhu.com/publication/fitter-roman-2019-ui/","publishdate":"2019-09-13T00:00:00-07:00","relpermalink":"/publication/fitter-roman-2019-ui/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"User Interface Tradeoffs for Remote Deictic Gesturing","type":"publication"},{"authors":["Zijian Hu","Lauren Klein"],"categories":[],"content":"The following project description is taken from Interaction Lab website.\n Infants engage in motor babbling that allows them to explore their space and learn what movements produce desired outcomes. Less motor babbling from infants can lead to developmental delays. Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice.\nTowards this end, we are collaborating with physical therapists to create approaches to predict the developmental status of infants using wearable sensors; running user studies that explore various robot rewards for contingent activities for the infant, as well as measuring the infant\u0026rsquo;s ability to mimic the robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to increase the infant\u0026rsquo;s engagement with the task.\n Development Detail   For project development, my contributions includes:\n Detecting and Tracking two Sphero SPRK+ robots with a wall-mounted camera Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset Visual Tracking:  With SiamRPN: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold With CSRT tracker: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location    ","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"4d1e0e9c7b1d701a2c5554b22de01a26","permalink":"https://www.zijianhu.com/project/baby/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/baby/","section":"project","summary":"Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice","tags":["Computer Vision","Robotics","Human-Robot Interaction"],"title":"Infant-Robot Interaction as an Early Intervention Strategy","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.zijianhu.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Zijian Hu"],"categories":[],"content":"Make sure you have installed all the dependencies and configured PYTHONPATH system variable correctly\n*see installation guide for detail\nControlling Robot   Turn on the robot. See this guide for detail\n  Start the robot bridge on your computer\n$ roslaunch nao_bringup nao_full_py.launch nao_ip:=\u0026lt;robot_ip\u0026gt; \\ roscore_ip:=\u0026lt;roscore_ip\u0026gt;  This will start the robot\u0026rsquo;s default configuration with the following publisher:\n joint_states tf top camera bottom camera left sonar right sonar microphone    To visualize the robot, open rviz\n$ rosrun rviz rviz   In top bar, go to File-\u0026gt;Open Config navigate to \u0026lt;your catkin workspace\u0026gt;/src/nao_robot/nao_description/config and open the file with .rviz extension  make sure you have nao_meshes installed   you should see something similar to the below screenshot     Controlling the robot\n  execute rosnode list to check if /nao_walker node is running\n  To turn on the motors\n$ rosservice call /body_stiffness/enable \u0026quot;{}\u0026quot;  To turn off the motors\n$ rosservice call /body_stiffness/disable \u0026quot;{}\u0026quot;    once the motors are on, use the following command to move the robot in x-direction\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 1.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'  To stop the robot, run:\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 0.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'      Next: NAOqi SDK Guide Reference  Getting started with ROS for Nao, including NAOqi and rviz nao_bringup  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554274800,"objectID":"19be522e21a2bbc9bfc08057e9b4a940","permalink":"https://www.zijianhu.com/post/nao-tutorial/getting-started/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/getting-started/","section":"post","summary":"This tutorial shows how to work with Physical NAO Robot","tags":["Tutorial","Robotics","NAO","ROS"],"title":"NAO Tutorial: Getting Started with NAO + ROS","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Installing the SDK   Make sure you have the following installed\n Python 2.7 CMake version 2.8.3 or higher    Download the following from Aldebaran Community website (you need to register an account in order to download the files)\n pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz naoqi-sdk-2.1.2.x-linux64.tar.gz [optional] choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar    Execute the following command and replace 2.1.2.x with the version you downloaded\nUnzip the tar files\n$ mkdir ~/naoqi\r$ tar xzf \u0026lt;path to NAOqi C++ SDK\u0026gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64\r$ tar xzf \u0026lt;path to NAOqi Python SDK\u0026gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64\r Check the installation by executing NAOqi\n$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi\r You should see output similiar to\nStarting NAOqi version 2.1.2.17\r.\r.\r.\rNAOqi is ready...\r Press CTRL-C to exit\nNow we need to add NAOqi SDK to system variables. Add the following lines at the end of ~/.bashrc\n$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH\r$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64\r$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64\r Execute source ~/.bashrc to apply the changes\nVerify in python console\nimport naoqi\r if correctly installed, there should be no error\n  Install ROS  See Official ROS Installation Tutorial  Install NAO package for ROS   Install the packages needed\n  replace kinetic to your ROS version if needed\n$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \\\rros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \\\rros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \\\rros-kinetic-camera-info-manager-py\r     Install the main package with sudo apt-get install ros-kinetic-nao-robot\n  Install packages for robot control\n$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \\\rros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \\\rros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \\\rros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \\\rros-kinetic-moveit\r   Install packages for simulation\nNotice: to install nao_meshes package, you need to agree the policy sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes\n  Next: Getting Started Reference  Python SDK Install Guide C++ SDK Installation Installation of ROS for usage with or on a NAO robot  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"d1a155d4c74134a706498e44f00eb8d9","permalink":"https://www.zijianhu.com/post/nao-tutorial/installation/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/installation/","section":"post","summary":"Installing NAOqi SDK and configure NAO with ROS","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: Installation","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Before starting  Make sure you have Choregraphe suite installed  See this tutorial for detail    Using Choregraphe  Follow this tutorial  Using Python in Choregraphe  Follow this tutorial  Using Dialog topic in Choregraphe  Follow this tutorial  Using Python  Follow this tutorial  Reference  Choregraphe Suite Installation Hello World 1 - using Choregraphe  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"26a08ddd1577f0e9da8d89fead5714d0","permalink":"https://www.zijianhu.com/post/nao-tutorial/nao-sdk/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/nao-sdk/","section":"post","summary":"This tutorial shows how to use NAOqi SDK","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: NAOqi SDK","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://www.zijianhu.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]