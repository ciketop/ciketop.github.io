[{"authors":["admin"],"categories":null,"content":"Zijian Hu is an undergraduate computer science student at the University of Southern California. He is interested in building robots that frees human from tedious tasks so that we can focus on things that matter.\nMy CV was last updated on Feb 2, 2020\n","date":1548230400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554274800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Zijian Hu is an undergraduate computer science student at the University of Southern California. He is interested in building robots that frees human from tedious tasks so that we can focus on things that matter.\nMy CV was last updated on Feb 2, 2020","tags":null,"title":"Zijian Hu","type":"authors"},{"authors":["Chris Birmingham","Zijian Hu","Kartik Mahajan","Eli Reber","Maja J. Matarić"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"5efa90d7c674fc4815c49bc5e9ac6d08","permalink":"/publication/chris-icra-2020/","publishdate":"2020-02-22T08:54:04.94898Z","relpermalink":"/publication/chris-icra-2020/","section":"publication","summary":"Socially assistive robots have the potential to improve group dynamics when interacting with groups of people in social settings. This work contributes to the understanding of those dynamics through a user study of trust dynamics in the novel context of a robot mediated support group. For this study, a novel framework for robot mediation of a support group was developed and validated. To evaluate interpersonal trust in the multi-party setting, a dyadic trust scale was implemented and found to be uni-factorial, validating it as an appropriate measure of general trust. The results of this study demonstrate a significant increase in average interpersonal trust after the group interaction session, and qualitative post-session interview data report that participants found the interaction helpful and successfully supported and learned from one other. The results of the study validate that a robot-mediated support group can improve trust among strangers and allow them to share and receive support for their academic stress.","tags":null,"title":"Can I Trust You? A User Study of Robot Mediation of a Support Group","type":"publication"},{"authors":["Naomi T. Fitter","Youngseok Joung","Marton Demeter","Zijian Hu","Maja J. Matarić"],"categories":null,"content":"Linked material YouTube video for \u0026ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot\u0026rdquo;\n\r","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f726d83983b9025c10a8c10e760cd2cc","permalink":"/publication/fitter-2019-1048/","publishdate":"2019-09-13T07:21:51.38791Z","relpermalink":"/publication/fitter-2019-1048/","section":"publication","summary":"Although nonverbal expressive abilities are an essential element of human-to-human communication, telepresence robots support only select nonverbal behaviors. As a result, telepresence users can experience difficulties taking turns in conversation and using various cues to obtain the attention of others. To expand telepresence robot users’ abilities to hold the floor during conversation, this work proposes and evaluates new types of expressive telepresence robot hardware. The described within-subjects study compared robot user and copresent person experiences during teamwork activity conditions involving basic robot functions, expressive LED lights, and an expressive robot arm. We found that among participants who preferred the arm-based expressiveness, individuals in both study roles felt the robot operator to be more in control of the robot during the arm condition, and participants co-located with the robot felt closer to their teammate during the arm phase. Participants also noted advantages of the LED lights for notification-type information and advantages of the arm for increasing perceptions of the robot as a human-like entity. Overall, these findings can inform future work on augmenting the nonverbal expressiveness of telepresence robots.","tags":null,"title":"Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot","type":"publication"},{"authors":["Naomi T. Fitter","Youngseok Joung","Zijian Hu","Marton Demeter","Maja J. Matarić"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7149aa3a0033379b8789f0a2f314f726","permalink":"/publication/fitter-2019-1049/","publishdate":"2019-09-13T07:21:51.606444Z","relpermalink":"/publication/fitter-2019-1049/","section":"publication","summary":"Telepresence robots can help to connect people by providing videoconferencing and navigation abilities in faraway environments. Despite this potential, current commercial telepresence robots lack certain nonverbal expressive abilities that are important for permitting the operator to communicate effectively in the remote environment. To help improve the utility of telepresence robots, we added an expressive, nonmanipulating arm to our custom telepresence robot system and developed three user interfaces to control deictic gesturing by the arm: onscreen, dial-based, and skeleton tracking methods. A usability study helped us evaluate user presence feelings, task load, preferences, and opinions while performing deictic gestures with the robot arm during a mock order packing task. The majority of participants preferred the dial-based method of controlling the robot, and survey responses revealed differences in physical demand and effort level across user interfaces. These results can guide robotics researchers interested in extending the nonverbal communication abilities of telepresence robots","tags":null,"title":"User Interface Tradeoffs for Remote Deictic Gesturing","type":"publication"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1559890800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559890800,"objectID":"0db6e08d4cded809a3c1c898b263c15f","permalink":"/project/dpa_sgd/","publishdate":"2019-06-07T00:00:00-07:00","relpermalink":"/project/dpa_sgd/","section":"project","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Machine Learning"],"title":"\r\nDecentralized Federated Multi-Task Learning and System Design\r\n","type":"project"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"b2274dabed25d20a4e40cb0885bc5d72","permalink":"/project/nri_kids/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/nri_kids/","section":"project","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics"],"title":"\r\nSocially Aware, Expressive, and Personalized Mobile Remote Presence:\r\nCo-Robots as Gateways to Access to K-12 In-School Education\r\n","type":"project"},{"authors":null,"categories":null,"content":"Infants engage in motor babbling that allows them to explore their space and learn what movements produce desired outcomes. Less motor babbling from infants can lead to developmental delays. Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice.\nTowards this end, we are collaborating with physical therapists to create approaches to predict the developmental status of infants using wearable sensors; running user studies that explore various robot rewards for contingent activities for the infant, as well as measuring the infant\u0026rsquo;s ability to mimic the robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to increase the infant\u0026rsquo;s engagement with the task.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"4d1e0e9c7b1d701a2c5554b22de01a26","permalink":"/project/baby/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/baby/","section":"project","summary":"\r\nOur goal is to develop a socially assistive, non-contact, infant-robot interaction\r\nsystem to provide contingent positive feedback to increase exploration and expand\r\nearly movement practice\r\n","tags":["Computer Vision","Machine Learning","Robotics"],"title":"Infant-Robot Interaction as an Early Intervention Strategy","type":"project"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"cc4047c61e23bbd30ef4d64a64a5f563","permalink":"/project/multi_party/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/multi_party/","section":"project","summary":"Building computational model of trust for human-robot interaction in group therapy setting","tags":["Robotics"],"title":"Trust in Multi-Party Human-Robot Interaction","type":"project"},{"authors":["Zijian Hu"],"categories":null,"content":"This tutorial shows how to work with Physical NAO.\nMake sure you have installed all the dependencies and configured PYTHONPATH system variable correctly\n*see installation guide for detail\nControlling Robot   Turn on the robot. See this guide for detail\n  Start the robot bridge on your computer\n$ roslaunch nao_bringup nao_full_py.launch nao_ip:=\u0026lt;robot_ip\u0026gt; \\\rroscore_ip:=\u0026lt;roscore_ip\u0026gt;\r This will start the robot\u0026rsquo;s default configuration with the following publisher:\n joint_states tf top camera bottom camera left sonar right sonar microphone    To visualize the robot, open rviz\n$ rosrun rviz rviz\r  In top bar, go to File-\u0026gt;Open Config navigate to \u0026lt;your catkin workspace\u0026gt;/src/nao_robot/nao_description/config and open the file with .rviz extension  make sure you have nao_meshes installed   you should see something similar to the below screenshot     Controlling the robot\n  execute rosnode list to check if /nao_walker node is running\n  To turn on the motors\n$ rosservice call /body_stiffness/enable \u0026quot;{}\u0026quot;\r To turn off the motors\n$ rosservice call /body_stiffness/disable \u0026quot;{}\u0026quot;\r   once the motors are on, use the following command to move the robot in x-direction\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\\r'{linear: {x: 1.0, y: 0.0, z: 0.0}, \\\rangular: {x: 0.0, y: 0.0, z: 0.0}}'\r To stop the robot, run:\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\\r'{linear: {x: 0.0, y: 0.0, z: 0.0}, \\\rangular: {x: 0.0, y: 0.0, z: 0.0}}'\r     Next: NAOqi SDK Guide Reference  \rGetting started with ROS for Nao, including NAOqi and rviz \rnao_bringup  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554274800,"objectID":"19be522e21a2bbc9bfc08057e9b4a940","permalink":"/post/nao-tutorial/getting-started/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/getting-started/","section":"post","summary":"This tutorial shows how to work with Physical NAO.","tags":["Tutorial","Robotics","NAO","ROS"],"title":"NAO Tutorial: Getting Started with NAO + ROS","type":"post"},{"authors":["Zijian Hu"],"categories":null,"content":"Installing the SDK   Make sure you have the following installed\n \rPython 2.7 \rCMake version 2.8.3 or higher    Download the following from Aldebaran Community website (you need to register an account in order to download the files)\n pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz naoqi-sdk-2.1.2.x-linux64.tar.gz [optional] choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar    Execute the following command and replace 2.1.2.x with the version you downloaded\nUnzip the tar files\n$ mkdir ~/naoqi\r$ tar xzf \u0026lt;path to NAOqi C++ SDK\u0026gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64\r$ tar xzf \u0026lt;path to NAOqi Python SDK\u0026gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64\r Check the installation by executing NAOqi\n$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi\r You should see output similiar to\nStarting NAOqi version 2.1.2.17\r.\r.\r.\rNAOqi is ready...\r Press CTRL-C to exit\nNow we need to add NAOqi SDK to system variables. Add the following lines at the end of ~/.bashrc\n$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH\r$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64\r$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64\r Execute source ~/.bashrc to apply the changes\nVerify in python console\nimport naoqi\r if correctly installed, there should be no error\n  Install ROS  See Official ROS Installation Tutorial  Install NAO package for ROS   Install the packages needed\n  replace kinetic to your ROS version if needed\n$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \\\rros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \\\rros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \\\rros-kinetic-camera-info-manager-py\r     Install the main package with sudo apt-get install ros-kinetic-nao-robot\n  Install packages for robot control\n$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \\\rros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \\\rros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \\\rros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \\\rros-kinetic-moveit\r   Install packages for simulation\nNotice: to install nao_meshes package, you need to agree the policy sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes\n  Next: Getting Started Reference  \rPython SDK Install Guide \rC++ SDK Installation \rInstallation of ROS for usage with or on a NAO robot  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"d1a155d4c74134a706498e44f00eb8d9","permalink":"/post/nao-tutorial/installation/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/installation/","section":"post","summary":"Installing NAOqi SDK and configure NAO with ROS.","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: Installation","type":"post"},{"authors":["Zijian Hu"],"categories":null,"content":"Before starting  Make sure you have Choregraphe suite installed  See this tutorial for detail    Using Choregraphe  Follow this tutorial  Using Python in Choregraphe  Follow this tutorial  Using Dialog topic in Choregraphe  Follow this tutorial  Using Python  Follow this tutorial  Reference  \rChoregraphe Suite Installation \rHello World 1 - using Choregraphe  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"26a08ddd1577f0e9da8d89fead5714d0","permalink":"/post/nao-tutorial/nao-sdk/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/nao-sdk/","section":"post","summary":"This tutorial shows how to use NAOqi SDK","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: NAOqi SDK","type":"post"}]