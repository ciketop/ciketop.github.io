[{"authors":null,"categories":null,"content":"Zijian Hu is a research assistant at the University of Southern California Viterbi School of Engineering. He is interested in building robots that frees human from tedious tasks.\nMy CV was last updated on Sep 24, 2020\n","date":1595491200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1595491200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/zijian-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zijian-hu/","section":"authors","summary":"Zijian Hu is a research assistant at the University of Southern California Viterbi School of Engineering. He is interested in building robots that frees human from tedious tasks.\nMy CV was last updated on Sep 24, 2020","tags":null,"title":"Zijian Hu","type":"authors"},{"authors":["Zijian Hu"],"categories":[],"content":"Introduction This example carefully replicates the behavior of TensorFlow\u0026rsquo;s tf.train.ExponentialMovingAverage.\nNotice that when applying EMA, only the trainable parameters should be changed; for PyTorch, we can get the trainable parameters by model.parameters() or model.named_parameters() where model is a torch.nn.Module.\nSince my implementation creates a copy of the input model (i.e. shadow), the buffers needs to be copied to shadow whenever update() is invoked.\nAlternative Implementation You could implement shadow as a dict, for detail of this version see 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现. One problem with that implementation is that shadow needs to be manually saved since shadow parameters are not stored in state_dict; a simple fix to this problem is to register all shadow parameters by calling register_parameter(\u0026lt;parameter name\u0026gt;).\nImplementations import torch from torch import nn from copy import deepcopy from collections import OrderedDict from sys import stderr # for type hint from torch import Tensor class EMA(nn.Module): def __init__(self, model: nn.Module, decay: float): super().__init__() self.decay = decay self.model = model self.shadow = deepcopy(self.model) for param in self.shadow.parameters(): param.detach_() @torch.no_grad() def update(self): if not self.training: print(\u0026quot;EMA update should only be called during training\u0026quot;, file=stderr, flush=True) return model_params = OrderedDict(self.model.named_parameters()) shadow_params = OrderedDict(self.shadow.named_parameters()) # check if both model contains the same set of keys assert model_params.keys() == shadow_params.keys() for name, param in model_params.items(): # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage # shadow_variable -= (1 - decay) * (shadow_variable - variable) shadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param)) model_buffers = OrderedDict(self.model.named_buffers()) shadow_buffers = OrderedDict(self.shadow.named_buffers()) # check if both model contains the same set of keys assert model_buffers.keys() == shadow_buffers.keys() for name, buffer in model_buffers.items(): # buffers are copied shadow_buffers[name].copy_(buffer) def forward(self, inputs: Tensor, return_feature: bool = False) -\u0026gt; Tensor: if self.training: return self.model(inputs, return_feature) else: return self.shadow(inputs, return_feature)  Reference  tf.train.ExponentialMovingAverage 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现  ","date":1595491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595491200,"objectID":"ade87a5ed3dd7bd98a96c8ab9d9891c4","permalink":"/post/pytorch/ema/","publishdate":"2020-07-23T00:00:00-08:00","relpermalink":"/post/pytorch/ema/","section":"post","summary":"This tutorial shows how to correctly implement EMA for PyTorch","tags":["Machine Learning","PyTorch","Tutorial"],"title":"PyTorch: Exponential Moving Average (EMA) Example","type":"post"},{"authors":["Chris Birmingham","Zijian Hu","Kartik Mahajan","Eli Reber","Maja J. Matarić"],"categories":[],"content":"","date":1590912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590912000,"objectID":"5efa90d7c674fc4815c49bc5e9ac6d08","permalink":"/publication/chris-icra-2020/","publishdate":"2020-02-22T00:00:00-08:00","relpermalink":"/publication/chris-icra-2020/","section":"publication","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction"],"title":"Can I Trust You? A User Study of Robot Mediation of a Support Group","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":" See the Interaction Lab website for details and related publications.  Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee the Interaction Lab website for details and related publications\n","date":1590912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590912000,"objectID":"52cee06d58cafb39b25827e34b3c439f","permalink":"/project/multi_party/","publishdate":"2019-03-26T00:00:00-08:00","relpermalink":"/project/multi_party/","section":"project","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction"],"title":"Trust in Multi-Party Human-Robot Interaction","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Marton Demeter","Zijian Hu","Maja J. Matarić"],"categories":[],"content":"Linked material YouTube video for \u0026ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot\u0026rdquo;\n\r","date":1571040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571040000,"objectID":"f726d83983b9025c10a8c10e760cd2cc","permalink":"/publication/fitter-2019-1048/","publishdate":"2019-09-13T00:00:00-08:00","relpermalink":"/publication/fitter-2019-1048/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1571040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571040000,"objectID":"352b7f240bc94841fab78c090a703aa5","permalink":"/project/nri_kids/","publishdate":"2019-03-26T00:00:00-08:00","relpermalink":"/project/nri_kids/","section":"project","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"Socially Aware, Expressive, and Personalized Mobile Remote Presence: Co-Robots as Gateways to Access to K-12 In-School Education","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Zijian Hu","Marton Demeter","Maja J. Matarić"],"categories":[],"content":"","date":1571040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571040000,"objectID":"7149aa3a0033379b8789f0a2f314f726","permalink":"/publication/fitter-2019-1049/","publishdate":"2019-09-13T00:00:00-08:00","relpermalink":"/publication/fitter-2019-1049/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"User Interface Tradeoffs for Remote Deictic Gesturing","type":"publication"},{"authors":[],"categories":[],"content":" See Interaction Lab website for details and related publications  Infants engage in motor babbling that allows them to explore their space and learn what movements produce desired outcomes. Less motor babbling from infants can lead to developmental delays. Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice.\nTowards this end, we are collaborating with physical therapists to create approaches to predict the developmental status of infants using wearable sensors; running user studies that explore various robot rewards for contingent activities for the infant, as well as measuring the infant\u0026rsquo;s ability to mimic the robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to increase the infant\u0026rsquo;s engagement with the task.\nSee Interaction Lab website for details and related publications\n","date":1553587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553587200,"objectID":"4d1e0e9c7b1d701a2c5554b22de01a26","permalink":"/project/baby/","publishdate":"2019-03-26T00:00:00-08:00","relpermalink":"/project/baby/","section":"project","summary":"Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice","tags":["Computer Vision","Machine Learning","Robotics","Human-Robot Interaction"],"title":"Infant-Robot Interaction as an Early Intervention Strategy","type":"project"},{"authors":["Zijian Hu"],"categories":[],"content":"Make sure you have installed all the dependencies and configured PYTHONPATH system variable correctly\n*see installation guide for detail\nControlling Robot   Turn on the robot. See this guide for detail\n  Start the robot bridge on your computer\n$ roslaunch nao_bringup nao_full_py.launch nao_ip:=\u0026lt;robot_ip\u0026gt; \\ roscore_ip:=\u0026lt;roscore_ip\u0026gt;  This will start the robot\u0026rsquo;s default configuration with the following publisher:\n joint_states tf top camera bottom camera left sonar right sonar microphone    To visualize the robot, open rviz\n$ rosrun rviz rviz   In top bar, go to File-\u0026gt;Open Config navigate to \u0026lt;your catkin workspace\u0026gt;/src/nao_robot/nao_description/config and open the file with .rviz extension  make sure you have nao_meshes installed   you should see something similar to the below screenshot     Controlling the robot\n  execute rosnode list to check if /nao_walker node is running\n  To turn on the motors\n$ rosservice call /body_stiffness/enable \u0026quot;{}\u0026quot;  To turn off the motors\n$ rosservice call /body_stiffness/disable \u0026quot;{}\u0026quot;    once the motors are on, use the following command to move the robot in x-direction\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 1.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'  To stop the robot, run:\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 0.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'      Next: NAOqi SDK Guide Reference  Getting started with ROS for Nao, including NAOqi and rviz nao_bringup  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554278400,"objectID":"19be522e21a2bbc9bfc08057e9b4a940","permalink":"/post/nao-tutorial/getting-started/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/getting-started/","section":"post","summary":"This tutorial shows how to work with Physical NAO Robot","tags":["Tutorial","Robotics","NAO","ROS"],"title":"NAO Tutorial: Getting Started with NAO + ROS","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Installing the SDK   Make sure you have the following installed\n Python 2.7 CMake version 2.8.3 or higher    Download the following from Aldebaran Community website (you need to register an account in order to download the files)\n pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz naoqi-sdk-2.1.2.x-linux64.tar.gz [optional] choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar    Execute the following command and replace 2.1.2.x with the version you downloaded\nUnzip the tar files\n$ mkdir ~/naoqi $ tar xzf \u0026lt;path to NAOqi C++ SDK\u0026gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64 $ tar xzf \u0026lt;path to NAOqi Python SDK\u0026gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64  Check the installation by executing NAOqi\n$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi  You should see output similiar to\nStarting NAOqi version 2.1.2.17 . . . NAOqi is ready...  Press CTRL-C to exit\nNow we need to add NAOqi SDK to system variables. Add the following lines at the end of ~/.bashrc\n$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH $ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64 $ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64  Execute source ~/.bashrc to apply the changes\nVerify in python console\nimport naoqi  if correctly installed, there should be no error\n  Install ROS  See Official ROS Installation Tutorial  Install NAO package for ROS   Install the packages needed\n  replace kinetic to your ROS version if needed\n$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \\ ros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \\ ros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \\ ros-kinetic-camera-info-manager-py      Install the main package with sudo apt-get install ros-kinetic-nao-robot\n  Install packages for robot control\n$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \\ ros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \\ ros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \\ ros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \\ ros-kinetic-moveit    Install packages for simulation\nNotice: to install nao_meshes package, you need to agree the policy sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes\n  Next: Getting Started Reference  Python SDK Install Guide C++ SDK Installation Installation of ROS for usage with or on a NAO robot  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554192000,"objectID":"d1a155d4c74134a706498e44f00eb8d9","permalink":"/post/nao-tutorial/installation/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/installation/","section":"post","summary":"Installing NAOqi SDK and configure NAO with ROS","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: Installation","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Before starting  Make sure you have Choregraphe suite installed  See this tutorial for detail    Using Choregraphe  Follow this tutorial  Using Python in Choregraphe  Follow this tutorial  Using Dialog topic in Choregraphe  Follow this tutorial  Using Python  Follow this tutorial  Reference  Choregraphe Suite Installation Hello World 1 - using Choregraphe  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554192000,"objectID":"26a08ddd1577f0e9da8d89fead5714d0","permalink":"/post/nao-tutorial/nao-sdk/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/nao-sdk/","section":"post","summary":"This tutorial shows how to use NAOqi SDK","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: NAOqi SDK","type":"post"}]