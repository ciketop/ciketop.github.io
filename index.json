[{"authors":["admin"],"categories":null,"content":"I am an undergraduate computer science student at the University of Southern California. I am interested in building robots that frees human from tedious tasks so that we can focus on things that matter.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am an undergraduate computer science student at the University of Southern California. I am interested in building robots that frees human from tedious tasks so that we can focus on things that matter.","tags":null,"title":"Zijian Hu","type":"author"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1559890800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559890800,"objectID":"0db6e08d4cded809a3c1c898b263c15f","permalink":"/project/dpa_sgd/","publishdate":"2019-06-07T00:00:00-07:00","relpermalink":"/project/dpa_sgd/","section":"project","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Machine Learning"],"title":"\r\nDecentralized Federated Multi-Task Learning and System Design\r\n","type":"project"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"b2274dabed25d20a4e40cb0885bc5d72","permalink":"/project/nri_kids/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/nri_kids/","section":"project","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics"],"title":"\r\nSocially Aware, Expressive, and Personalized Mobile Remote Presence:\r\nCo-Robots as Gateways to Access to K-12 In-School Education\r\n","type":"project"},{"authors":null,"categories":null,"content":"Infants engage in motor babbling that allows them to explore their space and learn what movements produce desired outcomes. Less motor babbling from infants can lead to developmental delays. Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice.\nTowards this end, we are collaborating with physical therapists to create approaches to predict the developmental status of infants using wearable sensors; running user studies that explore various robot rewards for contingent activities for the infant, as well as measuring the infant\u0026rsquo;s ability to mimic the robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to increase the infant\u0026rsquo;s engagement with the task.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"4d1e0e9c7b1d701a2c5554b22de01a26","permalink":"/project/baby/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/baby/","section":"project","summary":"\r\nOur goal is to develop a socially assistive, non-contact, infant-robot interaction\r\nsystem to provide contingent positive feedback to increase exploration and expand\r\nearly movement practice\r\n","tags":["Computer Vision","Machine Learning","Robotics"],"title":"Infant-Robot Interaction as an Early Intervention Strategy","type":"project"},{"authors":null,"categories":null,"content":"Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\nSee Interaction Lab website for details and related publications\n","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"cc4047c61e23bbd30ef4d64a64a5f563","permalink":"/project/multi_party/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/multi_party/","section":"project","summary":"Building computational model of trust for human-robot interaction in group therapy setting","tags":["Robotics"],"title":"Trust in Multi-Party Human-Robot Interaction","type":"project"},{"authors":["Zijian Hu"],"categories":null,"content":" This tutorial shows how to work with Physical NAO.\nMake sure you have installed all the dependencies and configured PYTHONPATH system variable correctly\n*see installation guide for detail\nControlling Robot  Turn on the robot. See this guide for detail Start the robot bridge on your computer\n$ roslaunch nao_bringup nao_full_py.launch nao_ip:=\u0026lt;robot_ip\u0026gt; \\ roscore_ip:=\u0026lt;roscore_ip\u0026gt;  This will start the robot\u0026rsquo;s default configuration with the following publisher:\n joint_states tf top camera bottom camera left sonar right sonar microphone  To visualize the robot, open rviz\n$ rosrun rviz rviz   In top bar, go to File-\u0026gt;Open Config navigate to \u0026lt;your catkin workspace\u0026gt;/src/nao_robot/nao_description/config and open the file with .rviz extension  make sure you have nao_meshes installed  you should see something similar to the below screenshot   Controlling the robot\n execute rosnode list to check if /nao_walker node is running To turn on the motors\n$ rosservice call /body_stiffness/enable \u0026quot;{}\u0026quot;  To turn off the motors\n$ rosservice call /body_stiffness/disable \u0026quot;{}\u0026quot;  once the motors are on, use the following command to move the robot in x-direction\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 1.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'  To stop the robot, run:\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 0.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'    Next: NAOqi SDK Guide Reference  Getting started with ROS for Nao, including NAOqi and rviz nao_bringup  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554274800,"objectID":"19be522e21a2bbc9bfc08057e9b4a940","permalink":"/post/nao-tutorial/getting-started/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/getting-started/","section":"post","summary":"This tutorial shows how to work with Physical NAO.","tags":["Tutorial","Robotics","NAO","ROS"],"title":"NAO Tutorial: Getting Started with NAO + ROS","type":"post"},{"authors":["Zijian Hu"],"categories":null,"content":" Installing the SDK  Make sure you have the following installed\n Python 2.7 CMake version 2.8.3 or higher   Download the following from Aldebaran Community website (you need to register an account in order to download the files)\n pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz naoqi-sdk-2.1.2.x-linux64.tar.gz [optional] choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar   Execute the following command and replace 2.1.2.x with the version you downloaded\nUnzip the tar files\n$ mkdir ~/naoqi $ tar xzf \u0026lt;path to NAOqi C++ SDK\u0026gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64 $ tar xzf \u0026lt;path to NAOqi Python SDK\u0026gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64  Check the installation by executing NAOqi\n$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi  You should see output similiar to\nStarting NAOqi version 2.1.2.17 . . . NAOqi is ready...  Press CTRL-C to exit\nNow we need to add NAOqi SDK to system variables. Add the following lines at the end of ~/.bashrc\n$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH $ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64 $ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64  Execute source ~/.bashrc to apply the changes\nVerify in python console\nimport naoqi  if correctly installed, there should be no error\n  Install ROS  See Official ROS Installation Tutorial  Install NAO package for ROS  Install the packages needed\n replace kinetic to your ROS version if needed   $ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \\ ros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \\ ros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \\ ros-kinetic-camera-info-manager-py  Install the main package with sudo apt-get install ros-kinetic-nao-robot\n Install packages for robot control\n$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \\ ros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \\ ros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \\ ros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \\ ros-kinetic-moveit  Install packages for simulation\n  Notice: to install nao_meshes package, you need to agree the policy sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes\nNext: Getting Started Reference  Python SDK Install Guide C++ SDK Installation Installation of ROS for usage with or on a NAO robot  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"d1a155d4c74134a706498e44f00eb8d9","permalink":"/post/nao-tutorial/installation/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/installation/","section":"post","summary":"Installing NAOqi SDK and configure NAO with ROS.","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: Installation","type":"post"},{"authors":["Zijian Hu"],"categories":null,"content":" Before starting  Make sure you have Choregraphe suite installed  See this tutorial for detail   Using Choregraphe  Follow this tutorial  Using Python in Choregraphe  Follow this tutorial  Using Dialog topic in Choregraphe  Follow this tutorial  Using Python  Follow this tutorial  Reference  Choregraphe Suite Installation Hello World 1 - using Choregraphe  ","date":1548230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"26a08ddd1577f0e9da8d89fead5714d0","permalink":"/post/nao-tutorial/nao-sdk/","publishdate":"2019-01-23T00:00:00-08:00","relpermalink":"/post/nao-tutorial/nao-sdk/","section":"post","summary":"This tutorial shows how to use NAOqi SDK","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: NAOqi SDK","type":"post"}]