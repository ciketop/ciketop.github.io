[{"authors":["ram-nevatia"],"categories":null,"content":"The following paragraph is taken from Dr Ram Nevatia\u0026rsquo;s personal website.\nRamakant Nevatia (nevatia at usc.edu) received his Ph.D. degree from Stanford University with specialty in the area of computer vision. He is currently a Professor of Computer Science and Electrical Engineering and Director of the Institute for Robotics and Intelligent Systems at the University of Southern California. Dr. Nevatia has made important contributions to several areas of computer vision including the topics of shape description, object recognition, stereo analysis aerial image analysis, tracking of humans and event recognition. He is author of two books, several book chapters and over 150 referred technical papers. He is a Fellow of the Institute of Electrical and Electronics Engineers (IEEE) and of the American Association for Artificial Intelligence (AAAI).\nDr. Nevatia has been a principal investigator of major Government funded computer vision research programs and has been funded by agencies such as DARPA, IARPA, ONR and DoJ. His work has been cited more than 25,000 times and he has a citation h-index of 76. He has supervised more than 30 Ph.D. dissertations. His former students are employed in top companies such as Google, Facebook, IBM, Amazon, GE Research, Siemens Research, Samsung, Qualcomm and many others\n","date":1624086000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624086000,"objectID":"e9639d05b72b8e8ea8b935dcc6005295","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Ram Nevatia\u0026rsquo;s personal website.\nRamakant Nevatia (nevatia at usc.edu) received his Ph.D. degree from Stanford University with specialty in the area of computer vision.","tags":null,"title":"Ram Nevatia","type":"authors"},{"authors":null,"categories":null,"content":"Zijian Hu is a researcher at the University of Southern California Viterbi School of Engineering. He is interested in building robust machine perception and control systems that allow adaptive, safe, and reliable human-robot interactions. Such systems should operate in noisy environments and can be trained and adapted with minimal supervision.\n  Download my CV and resumé (last updated on May 21, 2021) for more details.\n","date":1624086000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624086000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Zijian Hu is a researcher at the University of Southern California Viterbi School of Engineering. He is interested in building robust machine perception and control systems that allow adaptive, safe, and reliable human-robot interactions.","tags":null,"title":"Zijian Hu","type":"authors"},{"authors":["chris-birmingham"],"categories":null,"content":"The following paragraph is taken from Chris Birmingham\u0026rsquo;s personal website.\nI am a PhD student in Maja Mataric’s Interaction Lab at the University of Southern California. I am driven by the question - how can we use technology to bring people together rather than isolate them? I believe social robots may bring people out of the digital world and back together in the physical world. I currently focus on understanding the dynamics of multi-party interaction between groups of people and a robot. You can read more about this work here.\nAs part of this work I spend most of my time developing deep learning models for understanding and predicting human behavior in order to support more intelligent interactions.\n","date":1590908400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590908400,"objectID":"36e157e86a1548efc035a34affb613cf","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Chris Birmingham\u0026rsquo;s personal website.\nI am a PhD student in Maja Mataric’s Interaction Lab at the University of Southern California. I am driven by the question - how can we use technology to bring people together rather than isolate them?","tags":null,"title":"Chris Birmingham","type":"authors"},{"authors":["maja-mataric"],"categories":null,"content":"The following paragraph is taken from Dr Maja Matarić\u0026rsquo;s personal website.\nMaja Matarić is distinguished professor and Chan Soon-Shiong chair in Computer Science Department, Neuroscience Program, and the Department of Pediatrics and Interim Vice President for Research at the University of Southern California, founding director of the USC Robotics and Autonomous Systems Center (RASC), co-director of the USC Robotics Research Lab, and the lead of the Viterbi K-12 STEM Center. She received her PhD in Computer Science and Artificial Intelligence from MIT in 1994, MS in Computer Science from MIT in 1990, and BS in Computer Science from the University of Kansas in 1987.\n","date":1590908400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1590908400,"objectID":"0f3f111ba48ba5c63190e969f242e69d","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Maja Matarić\u0026rsquo;s personal website.\nMaja Matarić is distinguished professor and Chan Soon-Shiong chair in Computer Science Department, Neuroscience Program, and the Department of Pediatrics and Interim Vice President for Research at the University of Southern California, founding director of the USC Robotics and Autonomous Systems Center (RASC), co-director of the USC Robotics Research Lab, and the lead of the Viterbi K-12 STEM Center.","tags":null,"title":"Maja J. Matarić","type":"authors"},{"authors":["houston-claure"],"categories":null,"content":"AI Researcher | Robotics \u0026amp; Human-Computer Interaction Researcher | PhD Candidate at Cornell University\n","date":1559372400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1559372400,"objectID":"382433cc53e7038d1622efec500432ee","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"AI Researcher | Robotics \u0026amp; Human-Computer Interaction Researcher | PhD Candidate at Cornell University","tags":null,"title":"Houston Claure","type":"authors"},{"authors":["stefanos-nikolaidis"],"categories":null,"content":"The following paragraph is taken from Dr Stefanos Nikolaidis' personal website.\nI am an assistant professor in computer science at the University of Southern California, where I run the Interactive and Collaborative Autonomous Robotic Systems (ICAROS) lab. I graduated with a PhD from the CMU Robotics Institute and with a MS from MIT.\nThe ICAROS lab focuses on the core computational challenges in human-robot interaction: what are good models of human behavior, how to learn such models from noisy samples, and how to robustly generate actions for robotic teammates in large scale real-world applications. Our research spans the whole spectrum of human-robot interaction science: from distilling the fundamental mathematical principles that govern interactive behaviors, to developing approximation algorithms for deployed robotic systems and testing them \u0026ldquo;in the wild\u0026rdquo; with actual end users.\n","date":1559372400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1559372400,"objectID":"30593b4448225b8efe979c6b06f52a76","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"The following paragraph is taken from Dr Stefanos Nikolaidis' personal website.\nI am an assistant professor in computer science at the University of Southern California, where I run the Interactive and Collaborative Autonomous Robotic Systems (ICAROS) lab.","tags":null,"title":"Stefanos Nikolaidis","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://www.zijianhu.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Zijian Hu","Zhengyu Yang","Xuefeng Hu","Ram Nevatia"],"categories":[],"content":"Table of Contents  Introduction  Semi-supervised Learning   Method  Problem Description Augmentation Strategy Pseudo-labeling Loss SimPLE Algorithm      Introduction Deep learning has recently achieved state-of-the-art performance on many computer vision tasks. labeling large datasets is very expensive and often not feasible, especially in domains that require expertise to provide labels. Semi-Supervised Learning (SSL), on the other hand, can take advantage of partially labeled data, which is much more easily available, as shown in Figure 1, for example.\n Illustration of an image set with a limited amount of labeled images among a large number of unlabeled images.  Semi-supervised Learning A critical problem in semi-supervised learning is how to generalize the information learned from limited label data to unlabeled data. Following the continuity assumption that close data have a higher probability of sharing the same label, many approaches have been developed, including the recently proposed Label Propagation.\nAnother critical problem in semi-supervised learning is how to directly learn from the large amount of unlabeled data. Maintaining consistency between differently augmented unlabeled data has been recently studied and proved to be an effective way to learn from unlabeled data in both self-supervised learning.\nThe recently proposed MixMatch combines the above techniques and designed a unified loss term to let the model learn from differently augmented labeled and unlabeled data, together with the mix-up technique, which encourages convex behavior between samples to increase model\u0026rsquo;s generalization ability. ReMixMatch further improves the MixMatch by introducing the distribution alignment and Augmentation Anchoring techniques, which allows the model to accommodate and leverage from the heavily augmented samples. FixMatch simplified its previous works by introducing a confidence threshold into its unsupervised objective function and achieves state-of-the-art performance over the standard benchmarks.\nIn this paper, we propose to take advantage of the relationship between different unlabeled samples. We introduce a novel Pair Loss, which encourages a pair of similar unlabeled samples (in the augmented space) to have similar predictions if at least one of them is of high confidence in its prediction. Combining the techniques developed by the MixMatch family, we propose the SimPLE algorithm. As shown in Figure 2, the SimPLE algorithm generates pseudo labels of unlabeled samples by averaging and sharpening the predictions on multiple weakly augmented variations of the same sample.\n An overview of the proposed SimPLE algorithm.  Then, we use both the labels and pseudo labels to compute the supervised cross-entropy loss and unsupervised $L2$ distance loss. These two terms push the decision boundaries to go through low-density areas and encourage consistency among different variations of the same samples. Finally, with the newly proposed Pair Loss, we harness the relationships among the pseudo labels of different samples by encouraging consistency among different unlabeled samples which share a great similarity.\nMethod To take full advantage of the vast quantity of unlabeled samples in SSL problems, we propose the SimPLE algorithm that focuses on the relationship between unlabeled samples.\nProblem Description We define the semi-supervised image classification problem as following. In a $L$-class classification setting, we have:\n $\\mathcal{X}=\\left(\\left(x_{b}, y_{b}\\right) ; b \\in(1, \\ldots, B)\\right)$: a batch of labeled data $\\mathcal{U}=\\left(u_{b} ; b \\in(1, \\ldots, B)\\right)$: a batch of unlabeled data $\\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid x ; \\theta\\right)$: the model\u0026rsquo;s predicted softmax class probability of input $x$ parameterized by weight $\\theta$  Augmentation Strategy Our algorithm uses Augmentation Anchoring, in which pseudo labels come from weakly augmented samples act as \u0026ldquo;anchor,\u0026rdquo; and we align the strongly augmented samples to the \u0026ldquo;anchor.\u0026rdquo; Our weak augmentation, follows that of MixMatch family, contains a random cropping followed by a random horizontal flip. We use RandAugment or a fixed augmentation strategy that contains difficult transformations such as random affine and color jitter as strong augmentation. For every batch, RandAugment randomly selects a fixed number of augmentations from a predefined pool; the intensity of each transformation is determined by a magnitude parameter.\nPseudo-labeling Our pseudo labeling is based on the label guessing technique used in MixMatch. We first take the average of the model\u0026rsquo;s predictions of several weakly augmented versions of the same unlabeled sample as its pseudo label. As the prediction is averaged from $K$ slight perturbations of the same input instead of $K$ severe perturbation or a single perturbation, the guessed pseudo label should be more stable. Then, we use the sharpening operation defined in MixMatch to increase the temperature of the label\u0026rsquo;s distribution: $$\\operatorname{Sharpen}(p, T):= \\frac{p^{\\frac{1}{T}}}{\\textbf{1}^\\top p^{\\frac{1}{T}}}$$\nAs the peak of the pseudo label\u0026rsquo;s distribution is \u0026ldquo;sharpened,\u0026rdquo; the network will push this sample further away from the decision boundary. Additionally, following the practice of MixMatch, we use the exponential moving average of the model at each time step to guess the labels.\nLoss Our loss consists of three terms, $\\mathcal{L_X}$, $\\mathcal{L_U}$, and $\\mathcal{L_P}$, representing the supervised loss, the unsupervised loss, and the Pair Loss respectively.\n$$ \\begin{align} \\mathcal{L} \u0026amp;= \\mathcal{L_X} + \\lambda_{\\mathcal{U}} \\mathcal{L_U} + \\lambda_{\\mathcal{P}} \\mathcal{L_P} \\\\\n\\mathcal{L_X} \u0026amp;= \\frac{1}{\\left| \\mathcal{X}' \\right|} \\sum_{x,y \\in \\hat{\\mathcal{X}}} H\\left(y, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid x ; \\theta\\right)\\right) \\\\\n\\mathcal{L_U} \u0026amp;= \\frac{ \\sum_{u,q \\in \\hat{\\mathcal{U}}} \\mathbb{1}_{\\left(\\max\\left(q\\right) \u0026gt; \\tau_c\\right)} \\left| q - \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid u ; \\theta\\right) \\right|^{2}_{2} }{L \\left| \\hat{\\mathcal{U}} \\right|} \\end{align} $$\n$\\mathcal{L_X}$ calculates the cross-entropy of weakly augmented labeled samples, and $\\mathcal{L_U}$ represents the $L_2$ distance between strongly augmented samples and their pseudo labels, filtered by high confidence threshold. Notice that $\\mathcal{L_U}$ only enforces the consistency among different perturbations of the same samples but not the consistency among different samples.\nAs we aim to exploit the relationship among unlabeled samples, we hereby introduce a novel loss term, Pair Loss, that allows information to propagate implicitly between different unlabeled samples. In Pair Loss, we use a high confidence pseudo label of an unlabeled point, $p$, as an \u0026ldquo;anchor.\u0026rdquo; All unlabeled samples whose pseudo labels are similar enough to $p$ need to align their predictions under severe perturbation to the \u0026ldquo;anchor.\u0026rdquo;\n Pair Loss Overview.  Figure 3 offers an overview of this selection process. During this process, the similarity threshold \u0026ldquo;extended\u0026rdquo; our confidence threshold in an adaptive manner, as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level. Formally, we defined the Pair Loss as following:\n$$ \\begin{aligned} \\mathcal{L_P} \u0026amp;= \\frac{1}{\\binom{K\u0026rsquo;B}{2}} \\sum_{ \\substack{ i,j \\in \\left[\\left|\\mathcal{U}'\\right|\\right], i \\ne j \\\\\n\\left(v_l, q_l\\right) = \\mathcal{U}'_{i} \\\\\n\\left(v_r, q_r\\right) = \\mathcal{U}'_{j} } } \\varphi_{\\tau_c}\\left(\\max\\left(q_l\\right)\\right) \\\\\n\u0026amp;\\cdot \\varphi_{\\tau_s}\\left(f_{\\operatorname{sim}}\\left(q_l, q_r\\right)\\right) \\\\\n\u0026amp;\\cdot f_{\\operatorname{dist}}\\left(q_l, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid v_r ; \\theta\\right)\\right) \\end{aligned} $$\nHere, $\\tau_c$ and $\\tau_s$ denote the confidence threshold and similarity threshold respectively. $\\varphi_t(x)=\\mathbb{1}_{\\left(x \u0026gt; t\\right)} x$ is a hard threshold function controlled by threshold $t$. $f_{\\operatorname{sim}}\\left(p, q\\right)$ measures the similarity between two probability vectors $p, q$ by Bhattacharyya coefficient. The coefficient is bounded between $[0, 1]$, and represents the size of the overlapping portion of the two discrete distributions:\n$$f_{\\operatorname{sim}}\\left(p, q\\right) = \\sqrt{p} \\cdot \\sqrt{q}$$\n$f_{\\operatorname{dist}}\\left(p, q\\right)$ measures the distance between two probability vectors $p, q$. As $f_{\\operatorname{sim}}\\left(p, q\\right)$ is bounded between $[0, 1]$, we simply choose the distance function to be $f_{\\operatorname{dist}}\\left(p, q\\right) = 1 - f_{\\operatorname{sim}}\\left(p, q\\right)$.\nAlthough based on analysis, we found that $\\cos(\\cos^{-1}(\\sqrt{\\tau_c})+\\cos^{-1}(\\tau_s))^2$ is the minimal confidence a label need to have for it to be selected by both thresholds, such low confidence label are rarely selected in practice. Based on empirical evidence, we believe this is caused by the fact a label $p$ that can pass through the high confidence threshold typically has a near one hot distribution. Thus, for another label $q$ to fall in the similarity threshold of $q$, it must also have relatively high confidence. Due to this property, the Pair Loss is not very sensitive to the choices of hyperparameters $\\tau_s$, $\\tau_c$, which we will show empirically in later section.\nSimPLE Algorithm By putting together all the components introduced in this section, we now present the SimPLE algorithm. During training, for a mini-batch of samples, SimPLE will first augment both labeled and unlabeled samples with both weak and strong augmentations. The pseudo labels of the unlabeled samples are obtained by averaging and then sharpening the models' predictions on the weakly augmented unlabeled samples. Finally, we optimize the three loss terms based on augmented samples and pseudo labels. During testing, SimPLE uses the exponential moving average of the weights of the model to do prediction, as what is done by MixMatch. Figure 2 gives an overview of the algorithm, and the complete training algorithm is described in Alg. 1.\n  SimPLE algorithm.  The experiment section will be updated soon\n","date":1624086000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624086000,"objectID":"cb7c155e9547637654e2d0947afb3b0b","permalink":"https://www.zijianhu.com/publication/hu-2020-simple/","publishdate":"2020-12-05T00:00:00-07:00","relpermalink":"/publication/hu-2020-simple/","section":"publication","summary":"We propose a novel algorithm that focuses on the less studied relationship between the unlabeled data. Our algorithm achieves state-of-the-art performance on standard benchmarks","tags":["Machine Learning","Semi-Supervised Learning","Low-Shot Learning"],"title":"SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":"This is an ongoing project. The details are subject to change.\nThe following project description is taken from DARPA.\n In supervised machine learning (ML), the ML system learns by example to recognize things, such as objects in images or speech. Humans provide these examples to ML systems during their training in the form of labeled data. With enough labeled data, we can generally build accurate pattern recognition models.\nThe problem is that training accurate models currently requires lots of labeled data. For tasks like machine translation, speech recognition or object recognition, deep neural networks (DNNs) have emerged as the state of the art, due to the superior accuracy they can achieve. To gain this advantage over other techniques, however, DNN models need more data, typically requiring 109 or 1010 labeled training examples to achieve good performance.\nThe commercial world has harvested and created large sets of labeled data for training models. These datasets are often created via crowdsourcing: a cheap and efficient way to create labeled data. Unfortunately, crowdsourcing techniques are often not possible for proprietary or sensitive data. Creating data sets for these sorts of problems can result in 100x higher costs and 50x longer time to label.\nTo make matters worse, machine learning models are brittle, in that their performance can degrade severely with small changes in their operating environment. For instance, the performance of computer vision systems degrades when data is collected from a new sensor and new collection viewpoints. Similarly, dialog and text understanding systems are very sensitive to changes in formality and register. As a result, additional labels are needed after initial training to adapt these models to new environments and data collection conditions. For many problems, the labeled data required to adapt models to new environments approaches the amount required to train a new model from scratch.\nThe Learning with Less Labeling (LwLL) program aims to make the process of training machine learning models more efficient by reducing the amount of labeled data required to build a model by six or more orders of magnitude, and by reducing the amount of data needed to adapt models to new environments to tens to hundreds of labeled examples.\n LwLL Evaluation The evaluation has 3 types of tasks: Image Classification, Object Detection and Machine Translation. Our team is focusing on the image classification task. Each task includes a base phase and an adaptation pha where each phase consists of 6 to 8 stages. For image classification, pre-trained models on predefined whitelisted datasets are allowed.\n Number of labels in each stage  The full training set without any label is given at the beginning. At stage 1, can request 1 label per category in the training set (label budget is 1). As the training progress, the label budget is increased.\n LwLL evaluation model pipeline  Our team used a few-shot learning method for the first 2 to 3 checkpoints when the labeled set size is small. Once sufficient number of labeled data samples become available, the training was handed to our semi-supervised algorithm.\nSimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification  SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification Zijian Hu, Zhengyu Yang, Xuefeng Hu, Ram Nevatia  PDF  Cite  Code  Project     SimPLE algorithm  We proposed a novel semi-supervised classification algorithm, SimPLE (Figure 3), that focuses on the less studied relationship between the high confidence unlabeled data that are similar to each other.\n Pair Loss Overview.  As shown in Figure 4, the new proposed Pair Loss minimizes the statistical distance between high confidence pseudo labels with similarity above a certain threshold. The similarity threshold \u0026ldquo;extended\u0026rdquo; our confidence threshold in an adaptive manner, as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level.\nFormally, we defined the Pair Loss as the following:\n$$ \\begin{aligned} \\mathcal{L_P} \u0026amp;= \\frac{1}{\\binom{KB}{2}} \\sum_{\\mathcal{U}'} \\mathbb{1}_{\\max\\left(q_l\\right) \u0026gt; \\tau_c} \\cdot \\mathbb{1}_{f_{\\operatorname{sim}}\\left(q_l,q_r\\right) \u0026gt; \\tau_s} \\\\\n\u0026amp;\\cdot f_{\\operatorname{dist}}\\left(q_l, \\mathrm{p}_{\\text{model}}\\left(\\tilde{y} \\mid v_r ; \\theta\\right)\\right) \\end{aligned} $$\nNotations:\n $K$: number of augmentations $B$: batch size $\\mathcal{U}'$: unlabeled batch $\\tau_c$: confidence threshold $\\tau_s$: similarity threshold $f_{\\operatorname{sim}}\\left(\\cdot,\\cdot\\right)$: similarity function  We use Bhattacharyya coefficient.   $f_{\\operatorname{dist}}\\left(\\cdot,\\cdot\\right)$: distance function  We use $1-$ Bhattacharyya coefficient.    Experimental Results    Dataset Num. Labels Method Backbone Top-1 Accuracy     CIFAR-100 10000 MixMatch WRN 28-8 71.69%   ReMixMatch WRN 28-8 76.97%   FixMatch WRN 28-8 77.40%   SimPLE WRN 28-8 78.11%   Mini-ImageNet 4000 MixMatch WRN 28-2 55.47%   MixMatch Enhanced WRN 28-2 60.50%   SimPLE WRN 28-2 66.55%   ImageNet to DomainNet-Real 3795 MixMatch ResNet-50 35.34%   MixMatch Enhanced ResNet-50 35.16%   SimPLE ResNet-50 50.90%   DomainNet-Real to Mini-ImageNet 4000 MixMatch WRN 28-2 53.39%   MixMatch Enhanced WRN 28-2 55.75%   SimPLE WRN 28-2 58.73%    Our algorithm, SimPLE, achieved state-of-the-art performance on standard SSL benchmarks and achieved the best accuracy on some tasks in LwLL evaluation. We also evaluated our method in the transfer setting where our algorithm outperforms prior works and supervised baseline by a large margin.\nDevelopment Detail  Designed a novel algorithm for semi-supervised classification Evaluated our algorithm on standard benchmarks (CIFAR-10, CIFAR-100, SVHN, Mini-ImageNet) Evaluated our algorithm in the transfer learning setting (on Mini-ImageNet, DomainNet-Real, AID, RESISC45), where models are initialized models pretrained on ImageNet or DomainNet-Real Distributed training with PyTorch Distributed Data Parallel GPU accelerated data augmentation with Kornia  Related Publications  Zijian Hu, Zhengyu Yang, Xuefeng Hu, Ram Nevatia  (2021). SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification. 2021 Conference on Computer Vision and Pattern Recognition (CVPR).  PDF  Cite  Code  Project   ","date":1607151600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607151600,"objectID":"5815644c6c414909f37084eb0375343a","permalink":"https://www.zijianhu.com/project/lwll/","publishdate":"2020-12-05T00:00:00-07:00","relpermalink":"/project/lwll/","section":"project","summary":"We propose a novel algorithm for semi-supervised classification that achieves state-of-the-art performance on standard benchmarks and outperform previous works on transfer setting by a large margin.","tags":["Computer Vision","Machine Learning","Semi-Supervised Learning"],"title":"Learning with Less Labeling (LwLL)","type":"project"},{"authors":["Zijian Hu"],"categories":[],"content":"Introduction This example carefully replicates the behavior of TensorFlow\u0026rsquo;s tf.train.ExponentialMovingAverage.\nNotice that when applying EMA, only the trainable parameters should be changed; for PyTorch, we can get the trainable parameters by model.parameters() or model.named_parameters() where model is a torch.nn.Module.\nSince my implementation creates a copy of the input model (i.e. shadow), the buffers needs to be copied to shadow whenever update() is invoked.\nAlternative Implementation You could implement shadow as a dict, for detail of this version see 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现. One problem with that implementation is that shadow needs to be manually saved since shadow parameters are not stored in state_dict; a simple fix to this problem is to register all shadow parameters by calling register_parameter(\u0026lt;parameter name\u0026gt;).\nImplementations import torch\rfrom torch import nn\rfrom copy import deepcopy\rfrom collections import OrderedDict\rfrom sys import stderr\r# for type hint\rfrom torch import Tensor\rclass EMA(nn.Module):\rdef __init__(self, model: nn.Module, decay: float):\rsuper().__init__()\rself.decay = decay\rself.model = model\rself.shadow = deepcopy(self.model)\rfor param in self.shadow.parameters():\rparam.detach_()\r@torch.no_grad()\rdef update(self):\rif not self.training:\rprint(\u0026quot;EMA update should only be called during training\u0026quot;, file=stderr, flush=True)\rreturn\rmodel_params = OrderedDict(self.model.named_parameters())\rshadow_params = OrderedDict(self.shadow.named_parameters())\r# check if both model contains the same set of keys\rassert model_params.keys() == shadow_params.keys()\rfor name, param in model_params.items():\r# see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\r# shadow_variable -= (1 - decay) * (shadow_variable - variable)\rshadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))\rmodel_buffers = OrderedDict(self.model.named_buffers())\rshadow_buffers = OrderedDict(self.shadow.named_buffers())\r# check if both model contains the same set of keys\rassert model_buffers.keys() == shadow_buffers.keys()\rfor name, buffer in model_buffers.items():\r# buffers are copied\rshadow_buffers[name].copy_(buffer)\rdef forward(self, inputs: Tensor, return_feature: bool = False) -\u0026gt; Tensor:\rif self.training:\rreturn self.model(inputs, return_feature)\relse:\rreturn self.shadow(inputs, return_feature)\r Reference  tf.train.ExponentialMovingAverage 【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现  ","date":1595487600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595487600,"objectID":"ade87a5ed3dd7bd98a96c8ab9d9891c4","permalink":"https://www.zijianhu.com/post/pytorch/ema/","publishdate":"2020-07-23T00:00:00-07:00","relpermalink":"/post/pytorch/ema/","section":"post","summary":"This tutorial shows how to correctly implement EMA for PyTorch","tags":["Machine Learning","PyTorch","Tutorial"],"title":"PyTorch: Exponential Moving Average (EMA) Example","type":"post"},{"authors":["Chris Birmingham","Zijian Hu","Kartik Mahajan","Eli Reber","Maja J. Matarić"],"categories":[],"content":"","date":1590908400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590908400,"objectID":"faec5e510f3f74c79cf99b94ff1c2a4b","permalink":"https://www.zijianhu.com/publication/birmingham-icra-2020/","publishdate":"2020-02-22T00:00:00-07:00","relpermalink":"/publication/birmingham-icra-2020/","section":"publication","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction"],"title":"Can I Trust You? A User Study of Robot Mediation of a Support Group","type":"publication"},{"authors":["Zijian Hu","Chris Birmingham"],"categories":[],"content":"In this project, we designed and evaluated a novel framework for robot mediation of a support group. We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (Wizard-of-Oz). At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings. In a second-author paper at International Conference on Robotics and Automation (ICRA), we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.\nThe following project description is taken from Interaction Lab website.\n Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\n Study Design  Volunteers demonstrating the study setup  In each study session, three participants were seated around the end of a table with a seated NAO robot as shown in Figure 1. The NAO robot, acting as a group moderator, was positioned towards the participants. On the table, a 360-degree microphone and 3 cameras facing directly to the participants' face were placed. Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members. The robot operator was seated behind a one-way mirror hidden from participants.\n   Sensitivity Question Disclosure     Low What do you like about school? When I feel stressed, I think my circuits might overload.\nDoes anyone else feel the same way?   Medium What are some of the hardest parts of school for you? Sometimes I worry I am inadequate for this school.\nDoes anyone else sometimes feel that too?   Hard What will happen if you don’t succeed in school? Sometimes I worry about if I belong here. Does anyone else feel the same way?    During the interaction, the robot can ask questions or make disclosures. A total of 16 questions and 6 disclosures are available. On average, 12 questions and 3 disclosures were made by the robot in each session.\nThe questions and disclosures are grouped into low, medium, and high sensitivity as illustrated in the below table.\n Self-annotation at the end of each session   Self-annotation UI  To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.\nThe detail for the procedure of the study can be found here.\nDevelopment Detail  Control architecture  As shown in Figure 3. The wizard controls the robot through the Wizard-of-Oz web interface.\nFor project development, my contributions includes:\n Developed NAO control program Designed and implemented web-based Wizard of Oz controller Designed and implemented self-annotation website Developed data collection program for one depth camera, three webcams and one 360 degree microphone Data post-processing for data whitening and fast data loading Turn-taking prediction with Temporal Convolutional Networks (TCN) and LSTM for multi-modal input  Related Publications  Chris Birmingham, Zijian Hu, Kartik Mahajan, Eli Reber, Maja J. Matarić  (2020). Can I Trust You? A User Study of Robot Mediation of a Support Group. 2020 International Conference on Robotics and Automation (ICRA).  PDF  Cite  Project  DOI   ","date":1590908400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590908400,"objectID":"52cee06d58cafb39b25827e34b3c439f","permalink":"https://www.zijianhu.com/project/multi_party/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/multi_party/","section":"project","summary":"We designed and evaluated a novel framework for robot mediation of a support group","tags":["Robotics","Human-Robot Interaction","Machine Learning"],"title":"Trust in Multi-Party Human-Robot Interaction","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Marton Demeter","Zijian Hu","Maja J. Matarić"],"categories":[],"content":"Linked material YouTube video for \u0026ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot\u0026rdquo;\n  ","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"1c5583df9fe020a761882594c6f66d02","permalink":"https://www.zijianhu.com/publication/fitter-roman-2019-hardware/","publishdate":"2019-09-13T00:00:00-07:00","relpermalink":"/publication/fitter-roman-2019-hardware/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot","type":"publication"},{"authors":["Zijian Hu"],"categories":[],"content":"In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.\nThe following project description is taken from Interaction Lab website.\n Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses specifically on interactions between one or more robots and multiple people, known as Multi-Party Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI (interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party interactions (within-group turn taking, dyadic dynamics, and group dynamics).\nTo address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems to function in many contexts, including service, support, and mediation. In realistic human contexts, service and support robots need to work with varying numbers of individuals, particularly when working within team structures. In mediation, robotic systems must by definition, be able to work with multiple parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.\nThis project will advance the basic research in trust and influence in MP-HRI contexts. This will involve exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research will examine robot group mediation for group conseling, with extensions to team performance in robot service and support teams.\n Development Detail We used an Ohmni robot equipped with an arm and a Linux PC.\n  For project development, my contributions consist of the following:\n Developed customized web user interface for robot arm control Designed and implemented communication protocol and software between the Linux PC and the Ohmni server Developed user interface with a turning dial for robot arm control  Related Publications   Naomi T. Fitter, Youngseok Joung, Marton Demeter, Zijian Hu, Maja J. Matarić  (2019). Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot. 2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man).  PDF  Cite  Project  Video  DOI    Naomi T. Fitter, Youngseok Joung, Zijian Hu, Marton Demeter, Maja J. Matarić  (2019). User Interface Tradeoffs for Remote Deictic Gesturing. 2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man).  PDF  Cite  Project  DOI   ","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"352b7f240bc94841fab78c090a703aa5","permalink":"https://www.zijianhu.com/project/nri_kids/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/nri_kids/","section":"project","summary":"We developed and evaluated various control methods and interfaces for mobile remote presence robots (we used [Ohmni](https://ohmnilabs.com/products/ohmnirobot/) robot) for remote K-12 education","tags":["Robotics","Human-Robot Interaction"],"title":"Telepresence Robot for K-12 Remote Education","type":"project"},{"authors":["Naomi T. Fitter","Youngseok Joung","Zijian Hu","Marton Demeter","Maja J. Matarić"],"categories":[],"content":"","date":1571036400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571036400,"objectID":"7f10e633ecbf290aab868bbe523130d0","permalink":"https://www.zijianhu.com/publication/fitter-roman-2019-ui/","publishdate":"2019-09-13T00:00:00-07:00","relpermalink":"/publication/fitter-roman-2019-ui/","section":"publication","summary":"Providing different control interface for telepresence robot for K-12 in-school education","tags":["Robotics","Human-Robot Interaction"],"title":"User Interface Tradeoffs for Remote Deictic Gesturing","type":"publication"},{"authors":["Houston Claure","Zijian Hu","Stefanos Nikolaidis"],"categories":[],"content":"Our work looks to understand how robots can be successfully integrated into human teams. Much work in the Human-Robot Interaction space has investigated one on one interactions with one robot and one human. Our work looks to fill this gap of knowledge by providing an algorithm that takes into account the social construct of human fairness and optimization through a Multi Armed Bandit variant algorithm. We apply this algorithm to a robot tasked with distributing resources to different human team members\nStudy Design  Study setup  As shown in Figure 1, In each study session, several participants were seated around the table. Each participant is expected to complete their own task. The robot is tasked to bring the puzzle piece to each participant in an order determined by its resource distribution algorithm. Out goal is to evaluate the effectiveness and fairness of different resource distribution algorithms.\nDevelopment Detail  Control architecture  On the table, a unique ArUco maker is attached in front of each participant, in front of the puzzle pieces, and on top of the robot. A camera mounted on a tripod that oversees the entire table is used for robot localization and navigation.\nAs shown in Figure 2. The main component in the perception system is the ArUco marker detector. The controller is composed of a finite-state machine and resource distribution algorithm. The finite-state machine encodes the state transition while the resource distribution algorithm determines where (which participant) to deliver the current puzzle piece. The perception system runs concurrently with the controller.\nFor project development, my contributions includes:\n Designed and implemented multi-threaded perception and control system for Anki Vector robot Camera calibration with ArUco Markers ArUco Marker detection for navigation and robot localization Path planning using finite state machine  ","date":1559372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559372400,"objectID":"037175b67cedd265f822f63adb37b464","permalink":"https://www.zijianhu.com/project/robot_team/","publishdate":"2019-06-01T00:00:00-07:00","relpermalink":"/project/robot_team/","section":"project","summary":"Our work provides an algorithm that takes into account the social construct of human fairness and optimization through a Multi Armed Bandit variant algorithm","tags":["Robotics","Human-Robot Interaction","Computer Vision"],"title":"Resource Distribution in Human-Robot Teams","type":"project"},{"authors":["Chaoyang He","Tian Xie","Zhengyu Yang","Zijian Hu","Shuai Xia"],"categories":[],"content":"This page contains a copy of the final report of our final project for Spring 2019 CSCI-599: Deep Learning and its Applications at the University of Southern California. The original page was removed\nTable of Contents  Problem formulation  Statistical Challenges System Challenges General Definition of Federated Learning General Framework of Federated Multi-Task Learning Federated Multi-Task Deep Learning Framework   Approach  Decentralized Periodic Averaging SGD Algorithm Advantages of Our Algorithm System Design   Comparison to Previous Works  Federated Multi-Task Learning Stochastic Gradient Decent Optimization   Results Appendix  Appendix I: Convergence Analysis Appendix II: Poster      In the cloud-based environment, distributed deep learning systems such as Tensorflow and PyTorch is widespread in various domains, but they are typically built based on the parameter server, which requires data integrated into one place (a server or a cluster) to train a model[Dean et al.; Li et al., 2014; Cui et al., 2014; Abadi et al., 2016; Paszke et al., 2017], as shown in Figure 1(a). Increasing practical constraints lead this data integration difficult or impossible, including data privacy and confidentiality, intellectual property protection, and law constraints. A promising solution to these problems is called Federated Learning [McMahan et al., 2016]. As shown in Figure 1(b), Federated Learning enables collaboratively training on edge devices or edge data centers through exchanging parameters or gradients without centralizing their scattered data (In this work, we use both worker and node to represent edge devices or edge data centers). Representative examples happen in various domains such as Mobile Internet, health, and finance domains [McMahan and Ramage, 2017, Liu et al., 2018a, Liu et al., 2018b].\n Federated Multi-Task Learning Topology. (a) Cloud-Based Distributed Learning; (b) Centralized Federated Learning; (c) Decentralized Federated Learning; (d) Centralized Communication Topology with Decentralized Parameter Exchanging Topology.  Problem formulation Statistical Challenges  Non-IID: Each worker generates data in a non-i.i.d. (independent and identically distributed) manner with a distinct statistical distribution. Unbalanced Local Data: Workers have different quantity of data sample due to their different behaviors.  These two characteristics bring challenges to learning a high-performance personalized model for each worker.\nSystem Challenges  Larger Worker Number: The worker number is typically larger than cloudbased learning. The larger number will pose a higher communication cost and difficult communication synchronization. Heterogeneous Networks: Each worker may differ in communication capacity due to heterogeneous networks(4G, WiFi, and other IoT protocol). Heterogeneous Computation: Computational capacities of each worker may differ due to variability in hardware(CPU, memory).  These three characteristics make communication cost and low-speed training become a major bottleneck towards real-world deployment.\nIn practice, scattered data owners also demand personalized models rather than a global model for all owners. They hope to not only get help from other owners’ data to train a high accuracy model but also to gain their personalized models which can represent their unique data properties. Thus, to simultaneously address statistical and system challenges is the primary research direction of federated learning.\n Federated Multi-Task Deep Learning Framework  General Definition of Federated Learning Following the first federated learning paper McMahan et al. [2016], we define the objective function for the federated setting as\n$$ \\begin{aligned} F(\\mathbf{w}) \u0026amp;= \\sum_{k=1}^{K} \\frac{n_k}{N} F_{k}(\\mathbf{w}) \\\\\n\u0026amp;= \\sum_{k=1}^{K} \\frac{n_k}{N} \\frac{1}{n_k} \\sum_{i \\in \\mathcal{P}_{k}} l(\\mathbf{x}_{i}, \\mathbf{y}_{i}; \\mathbf{w}) \\end{aligned} $$\nwhere $ l(\\mathbf{x}_{i}, \\mathbf{y}_{i}; \\mathbf{w}) $ is the loss function of the prediction on example $(\\mathbf{x}_{i}, \\mathbf{y}_{i})$ made with model parameters $\\mathbf{w}$, K is the total learning nodes number, $\\mathcal{P}_{k}$ is the set of indexes of data points on node k, $n_k = |P_k|$, and $\\sum_{k=1}^{K} n_{k} = N$. This objective function can capture the different quantity of samples and statistical distribution of K nodes. Here, different nodes learn the global model jointly, which showing as the same loss function $l$ and parameters $\\mathbf{w}$.\nGeneral Framework of Federated Multi-Task Learning As mentioned in the introduction, federated multi-task learning is a framework that can improve the performance by directly capturing the relationships among unbalanced data in multiple devices, which implies that it can address the statistical challenges in federated learning. The general formulation for federated multi-task learning is:\n$$ \\min_{\\mathbf{W}}\\sum_{k=1}^{K}\\frac{1}{n_k}\\sum_{i=1}^{n_k}l_i(\\mathbf{x}_i,\\mathbf{y}_i;\\mathbf{w}_k) + \\mathcal{R}(\\mathbf{W},\\mathbf{\\Omega}). $$\nwhere $\\mathbf{W} = (\\mathbf{w}_1,\\mathbf{w}_2,\u0026hellip;,\\mathbf{w}_K) \\in \\mathbb{R}^{d \\times{m}}$ is the parameters for different tasks and $\\mathcal{R}(\\mathbf{W},\\mathbf{\\Omega})$ is the regularization. Different multi-task framework is mainly different from the regularization term $\\mathcal{R}$.\nThe first term of the objective models the summation of different empirical loss of each node. The second term serves as a task-relationship regularizer with $\\mathbf{\\Omega}\\in\\mathbb{R}^{K\\times{K}}$ being the covariance matrix Zhang and Yeung [2012].\nThe covariance matrix is able to describe positive, negative and unrelated correlation between nodes, which can either known as priori or being measured while learning the models simultaneously.\nEach element $\\mathbf{\\Omega}_{i,j}$ is a value that indicates the similarity between two nodes. Here we use a bio-convex formulation in Zhang and Yeung [2012], which is a general case for other regularization methods,\n$$ \\mathcal{R}(\\mathbf{W},\\mathbf{\\Omega}) = \\lambda_ 1tr(\\mathbf{W}\\mathbf{\\Omega}^{-1}\\mathbf{W}^{T}) + \\lambda_2||\\mathbf{W}||_F^2. $$\nwhere we constrain $\\vec{W}$ with covariance matrix $\\mathbf{\\Omega}^{-1}$ through matrix trace $tr(\\mathbf{W}\\mathbf{\\Omega}^{-1}\\mathbf{W}^{T})$. This means the closer $\\mathbf{w}_i$ and $\\mathbf{w}_j$ is, the larger the $\\mathbf{\\Omega}_{i,j}$ will be. Specifically if $\\mathbf{\\Omega}$ is an identity matrix, then each node is independent to each other. Smith et al. [2017] proposed MOCHA based on the above multi-task learning framework. However, MOCHA can only handle convex functions in federated multi-task learning settings, which can not be generated to non-convex deep learning models. Our work generates federated multi-task learning framework to the non-convex DNN setting.\nFederated Multi-Task Deep Learning Framework DNNs are able to extract deep features from raw data. However, to the best of our knowledge, DNNs has not been applied to federated multi-task problems. We thus consider DNNs as our feature transformation function and make prediction based on the hidden features. Formally speaking, the formulation can be defined as:\n$$ \\begin{aligned} \u0026amp; \\begin{aligned} \\min_{\\mathbf{\\theta}, \\vec{U},\\vec{W},\\mathbf{\\Omega}} \u0026amp;\\sum_{k=1}^{K}\\frac{1}{n_k}\\bigg[\\bigg.\\sum_{i=1}^{n_k}l(f(\\vec{x}_i^{k},\\mathbf{\\theta}_k,\\vec{U}_k,\\vec{w}_k),\\vec{y}_i^k) \\\\\n\u0026amp;+ \\frac{1}{2}\\lambda_ 1tr(\\vec{W}_k\\mathbf{\\Omega}_k^{-1}\\vec{W}_k^{T})\\bigg]\\bigg. + \\frac{1}{2}\\lambda_2||\\vec{W}||_F^2\\\\\n\u0026amp;+\\frac{1}{2}\\lambda_3||\\mathbf{\\theta}||_F^2 + \\frac{1}{2}\\lambda_4||\\vec{U}||_F^2, \\end{aligned} \\\\\n\u0026amp; \\quad \\text{s.t.} \\quad \\mathbf{\\Omega}_k \\ge 0 \\quad \\text{and} \\quad tr(\\mathbf{\\Omega}_k) = 1, \\quad k = 1, 2, \u0026hellip; ,K. \\end{aligned} $$\nwhere $f(\\cdot)$ represents DNNs feature mapping as shown in Figure 2(b). $\\mathbf{\\theta}_k$ is the feature transformation network. $\\mathbf{U}_k$ and $\\vec{w}_k$ are output layer (e.g. softmax). The first constraint holds due to the fact that $\\mathbf{\\Omega}$ is defined as a task covariance matrix. The second constraint is used to restrict its complexity.\nIn federated learning situation, training should be conducted on each node respectively. One intuitive thought is the centralized network topology in McMahan et al. [2016], where one center node synchronously takes a weighted average parameters of every clients at each time step (Figure 2(a)). However, this model faces the problems that in DNNs situation, far more parameters need to be calculated and transferred. Each node has heterogeneous computing performance and network bandwidth (Figure (b)). Setting one center node to synchronously collect all the parameters will induce high communication cost and low convergence speed. In order to overcome these problems, we design a decentralized topology, where each node only needs to share their parameters with neighbored nodes as shown in Figure 2(c)), where there is no communication between worker one and worker 4. Abandoning the central node induces the problem that parameters cannot be exchanged and synchronized amongst every nodes, which means that the centralized optimization method can not be achieved on this topology. To this end, we propose a Decentralized Periodic Averaging SGD (DPA-SGD) to tackle the optimization problem in decentralized topology.\n Approach Decentralized Periodic Averaging SGD As for decentralized topology, due to the disappearing of central node, same central averaging method can not be applied. In order to overcome this problem, we come up with a novel optimization method, Decentralized Periodic Averaging SGD (DPA-SGD). The main idea of DPA-SGD is that during the communication period $\\tau$, local SGD is applied on each node respectively, and synchronizing all the parameters at every $\\tau$ iterations amongst its connected neighbors. Due to this decentralized diverse connection, one global $\\mathbf{\\Omega}$ can not represent the individual correlation. So we propose to use a distinct covariance matrix $\\mathbf{\\Omega}_k$ to represent their own mutual relationship. We also come up with an effective way to update the different $\\mathbf{\\Omega}_k$. To be specific, consider one particular node $m$ and its neighbor connected nodes as set $\\mathcal{M}$.\nThe new objective function can be defined as:\n$$ \\begin{aligned} \u0026amp; \\begin{aligned} \\min_{\\mathbf{\\theta}, \\vec{U},\\vec{W},\\mathbf{\\Omega}} \u0026amp;\\sum_{k=1}^{K}\\frac{1}{n_k}\\bigg[\\bigg.\\sum_{i=1}^{n_k}l(f(\\vec{x}_i^{k},\\mathbf{\\theta}_k,\\vec{U}_k,\\vec{w}_k),\\vec{y}_i^k) \\\\\n\u0026amp;+ \\frac{1}{2}\\lambda_ 1tr(\\vec{W}_k\\mathbf{\\Omega}_k^{-1}\\vec{W}_k^{T})\\bigg]\\bigg. + \\frac{1}{2}\\lambda_2||\\vec{W}||_F^2\\\\\n\u0026amp;+\\frac{1}{2}\\lambda_3||\\mathbf{\\theta}||_F^2 + \\frac{1}{2}\\lambda_4||\\vec{U}||_F^2, \\end{aligned} \\\\\n\u0026amp; \\quad \\text{s.t.} \\quad \\mathbf{\\Omega}_k \\ge 0 \\quad \\text{and} \\quad tr(\\mathbf{\\Omega}_k) = 1, \\quad k = 1, 2, \u0026hellip; ,K. \\end{aligned} $$\nwhere $\\vec{W}_k = (\\vec{w}_1,\\vec{w}_2,\u0026hellip;,\\vec{w}_m,\u0026hellip;\\vec{w}_{|\\mathcal{M}|})\\in\\mathbb{R}^{d\\times|\\mathcal{M}|}$ is the parameters for $m$ and its neighbor tasks. The matrix $\\mathbf{\\Omega}_k\\in\\mathbb{R}^{|\\mathcal{M}|\\times{|\\mathcal{M}|}}$ represents the correlation amongst nodes in set $\\mathcal{M}$. Here in order to record the entire nodes connection in the network, we introduce a node connection matrix $\\vec{M}\\in\\mathbb{R}^{K\\times{K}}$ represents the neighbor relationships for each nodes, where $M_{i, j}$ is a value that indicates node $i$ and $j$ are connected as shown in Figure 3, where worker one is only connected with worker two and four. Note that, if $\\vec{M} = \\vec{I}$ (Identity matrix), then every nodes are independent and update the parameters respectively. If $\\vec{M} = \\vec{J}$ (one for each element), the model is degenerated into centralized model. We study the model performance under sparse matrix $\\vec{M}$ and find that similar results can be achieved as a ring network topology, which each node is only connected with its nearby two nodes, as illustrated in Figure 3.\n Decentralized Periodic Averaging SGD  To solve this non-convex problem, we apply the alternating optimization method Zhang and Yeung [2012], where alternately updating parameters $\\vec{X} = (\\vec{W, U},\\mathbf{\\theta})$ and $\\mathbf{\\Omega}$.\nOptimizing $\\mathbf{\\theta}, \\vec{W}$ and $\\vec{U}$: For simplicity, we define set $\\Xi = (\\mathbf{\\Omega}_1, \\mathbf{\\Omega}_2,\u0026hellip;,\\mathbf{\\Omega_K})$ to represent the correlation matrix for every nodes. Fixing $\\Xi$, we can use SGD method to update $\\mathbf{\\theta}, \\vec{W}$ and $\\vec{U}$ jointly. Our problem can then be reformulated as:\n$$ \\begin{aligned} G(\\vec{W}, \\vec{U}, \\mathbf{\\theta}|\\Xi) \u0026amp;= \\sum_{k=1}^{K}\\frac{1}{n_k} \\bigg[\\bigg. \\sum_{i=1}^{n_k}l(f(\\textbf{x}_i^{k},\\mathbf{\\theta}_k,\\textbf{U}_k,\\textbf{w}_k),\\textbf{y}_i^k) \\\\\n\u0026amp; + \\frac{1}{2} \\lambda_1tr(\\textbf{W}_k\\mathbf{\\Omega}_k^{-1}\\textbf{W}_k^{T}) \\bigg]\\bigg. + \\frac{1}{2}\\lambda_2||\\textbf{W}||_F^2 \\\\\n\u0026amp;+ \\frac{1}{2}\\lambda_3||\\mathbf{\\theta}||_F^2 + \\frac{1}{2}\\lambda_4||\\textbf{U}||_F^2 \\end{aligned} $$\nWe can calculate the gradient of $\\vec{W}$, $\\vec{U}$ and $\\mathbf{\\theta}$ respectively. Let $L = \\sum_{k=1}^{K}\\frac{1}{n_k}\\sum_{i=1}^{n_k}l(f(\\mathbf{x}_i^{k}, \\mathbf{\\theta}_k,\\mathbf{u}_k,\\mathbf{w}_k),\\mathbf{y}_i^k)$. Then the gradient formulations for each node are:\n$$ \\frac{\\partial{G(\\vec{W}, \\vec{U}, \\mathbf{\\theta}|\\Xi)}}{\\partial{\\vec{w}_k}} = \\frac{\\partial{L}}{\\partial{\\vec{w}_k}} + \\lambda_1\\sum_{i=1}^{\\mathcal{|M|}} \\frac{1}{n_i}{\\vec{w}_k}\\mathbf{\\Omega}_i^{-1} + \\lambda_2\\vec{w}_k $$\nwhere the summation is amongst all the nodes connected to node $k$,\n$$ \\frac{\\partial{G(\\vec{W}, \\vec{U}, \\mathbf{\\theta}|\\Xi)}}{\\partial \\mathbf{\\theta}_k} = \\frac{\\partial{L}}{\\partial\\mathbf{\\theta}_k} + \\lambda_3\\mathbf{\\theta}_k, $$\n$$ \\frac{\\partial{G(\\vec{W}, \\vec{U}, \\mathbf{\\theta}|\\Xi)}}{\\partial{\\vec{u}_k}} = \\frac{\\partial{L}}{\\partial{\\vec{u}_k}} + \\lambda_4\\vec{u}_k $$\nOptimizing $\\Xi$: In paper Zhang and Yeung [2012], an analytical solution form is given for $\\mathbf{\\Omega}$:\n$$ \\mathbf{\\Omega} = \\frac{(\\vec{W}^T\\vec{W})^\\frac{1}{2}}{tr((\\vec{W}^T\\vec{W})^{\\frac{1}{2}})} $$\nApparently, if $\\mathbf{w}_i$ and $\\mathbf{w}_j$ are close to each other, $\\mathbf{\\Omega}$ will be large. However, the missing central node forbidding to average parameters globally. So here we propose a novel way to update each $\\mathbf{\\Omega}_k\\in\\Xi$:\n$$ \\mathbf{\\Omega}_{t+1}^{(k)} \\leftarrow \\eta\\frac{1}{|\\mathcal{M}|}({\\sum_{i=1}^{|\\mathcal{M}|}}\\frac{1}{n_i}\\mathbf{\\Omega}_{t}^{(i)} + \\frac{(\\vec{W}_k^T\\vec{W}_k)^\\frac{1}{2}}{tr((\\vec{W}_k^T\\vec{W}_k)^{\\frac{1}{2}})}) $$\nThe first averaging term can incorporate the nearby nodes correlation into its own and the second term captures the new correlation between its neighbors as shown in Figure 3.\nAlgorithm In general, the algorithm of DPA-SGD can be summarized as: while in local update period, each node calculates the gradient $g(\\vec{X}_t^{(i)})$ based on one mini-batch of data and then update $\\vec{X}^{(i)}$; For every synchronization per $\\tau$ update, the novel update way of $\\mathbf{\\Omega}$ is conducted.\n  Algorithm 1: Federated Multi-Task Deep Learning Framework with Decentralized Averaging SGD  Advantages of Our Algorithm Here we illustrate system-wise advantages of DPA-SGD:\n Illustration of training time reducing due to the mechanism of DPA-SGD  Faster convergence speed. Figure 4 illustrates three reasons that DHA-SGD can speed up convergence.\n Periodic averaging can alleviate the communication delay by reducing times of synchronization which only happen periodically. As we can see in Figure 4, the yellow blocks (communication) will largely be deleted due to the periodic averaging mechanism. This idle time can also be significantly reduced through periodic averaging as shown in Figure 4. In the decentralized topology, because a worker only needs to exchange gradients with its neighbors, another worker with slow computation and communication will not interrupt its iteration. For example, worker 2 in the above figure can synchronize earlier without waiting for worker 4. Thus, DHA-SGD can largely reduce convergence time.  Fewer communication rounds. The periodic averaging strategy can reduce the number of communication rounds. Although our work focuses on optimizing the ratio of computation and communication rather than improving the communication cost for each iteration, gradient compression method (Deep Gradient Compression [Lin et al., 2017]) for each iteration can directly extendable to to our algorithm in a practical system.\nSystem Design  Federated Learning System Design and Protocol  We have developed a real-world decentralized federated learning system. Our system is currently deployed in a cluster environment, consisting of 24 physical machines, each supporting 48 CPU cores. Suppose we run one process per CPU core, this system can run up to 1152 workers for parallel training. Our federated learning system enables centralized and decentralization training while supporting the Federated multi-task learning framework. It also supports user and training management in the Federated Learning scenario. As shown in Figure 5 (Upper), it includes training process management such as topology generation, user selection, configuration dispatching, on-device training, and statistics collection. In the topological generation step, we can select the appropriate eigenvalues for the topology matrix according to our the convergence property analysis, which determines the sparseness of the network topology to balance the model accuracy and the training speed. The user selection process can configure different strategies to select distinct users. Each user only requires training once for a model. In our experiment, users are selected according to the LEAF data set. This process can be further optimized in the future to choose users who are more meaningful to the model.\nThe software architecture of each worker is shown in Figure 5 (Lower). Each work can be an IoT device, smartphone or the edge server. It is an abstract architecture design for the on-device software system which unifies the system architecture for different on-device operating systems such as Android or iOS for the smartphone and the linux edge server. For the edge server, we implement this system design in a Python environment. For the smartphone, we develop the training worker system based on Android and iOS. The low-level communication is an abstract layer. In the edge server environment, we use the MPI protocol for exchanging gradient or other necessary information. As we can see, the communication sending and receiving threads are independent of the training threads. The collaborate through message queues. Currently, we only disclose the implementation of the MPI communication protocol in the source code. In the future, we consider open source our code to support more on-device platforms.\nTo put it simply, another contribution of this paper is that we publish a practical federated learning system that can promote further research on distributed learning and especially federated learning.\nComparison to Previous Works Federated Multi-Task Learning Early examples of research into federated learning. To address both statistical and system challenges, [Smith et al., 2017] and [Caldas et al. 2018] propose a multi-task learning framework for federated learning and its related optimization algorithm, which extends early works from distributed machine learning, including SDCAShalev-Shwartz and Zhang [2013]; Yang [2013]; Yang et al. [2013] and COCOAJaggi et al. [2014]; Ma et al. [2015]; Smith et al. [2016]. The main limitation of Smith et al. [2017] and Caldas et al. [2018], however, is that strong duality is only guaranteed when the objective function is convex, which can not be generalized to the non-convex setting, especially deep neural networks.\nAnother line of work related to federated multi-task learning is the cloud-based distributed multi-task learning [Ahmed et al., 2014; Jin et al., 2015; Mateos-Núñez et al., 2015; Wang et al., 2016; Baytas et al., 2016; Liu et al., 2017]. However, their assumption that the same amount of work is done locally on each node is prohibitive in federated settings, and none of these works take into account the systems challenges that arise in the federated setting.\nIn our work, we focus on training the deep learning model in the federated setting. To be more specific, in this work we further extend previous works to a generic multi-task deep learning framework and a more efficient optimization method. Different from previous works, we propose a decentralized approach for federated learning.\nStochastic Gradient Decent Optimization In large scale distributed deep learning, to address the communication bottleneck, synchronized mini-batch SGD, which increase the computation to communication ratio, is widely used in the parameter server framework [Dean et al.; Li et al., 2014; Cui et al., 2014] and popular deep learning systems such as Tensorflow [Abadi et al., 2016] and PyTorch [Paszke et al., 2017].\nCompared to this synchronized mini-batch SGD, Federated Averaging(FedAvg) SGD [Koneˇcný et al., 2016] in the federated setting empirically shows it has less communication rounds and it is also robust to non-IID data, which is now the state-of-the-art SGD algorithm for federated learning.\nDecentralized SGD, another approach to reducing communication, was successfully applied to deep learning [Jin et al., 2016; Jiang et al., 2017; Lian et al., 2017]. Instead of synchronizing with all workers, a worker in the decentralized SGD framework only needs to exchange gradient with its neighbors. Therefore, this sparse-connected topology can reduce the overall communication per iteration.\nOur work aims to design a novel SGD algorithm for our multi-task deep learning framework, which can leverage the advantages of periodic averaging SGD and decentralized SGD. We called it as Decentralized Periodic Averaging SGD. Although recent work [Wang and Joshi, 2018] has this idea preliminarily, it does not provide adequate theoretical analysis and empirical evaluation in the federated setting.\nResults DatasetNumber of devicesTotal samplesSample per devicemeanstdevFEMNIST3,550805,263226.8388.94Sent140660,1201, 600, 4982.424.71Shakespeare2,288106, 12646.3891.71\rThis table is a brief statistic of the dataset (LEAF) that we are using in the experiments. To be specific, FEMNIST is a handwritten digit database, where image classification problem can be conducted based on this dataset. Sent140 is a text dataset, which allows you to discover the sentiment of a brand or product. Shakespeare is also a text dataset, however the objective here is try to learn the pattern of the sentences and generate similar text style.\n  Accuracy for FedAvg Single-task and Ring DPA-SGD Multi-task vs Round number  We compare the performance between FedAvg, the current state-of-the-art Federated Learning algorithm, and our DPA-SGD on Multi-Task Learning Framework (DPA-SGD-MTL) with a ring topology network. From the Figure, we clearly see that the training speed is around 20% faster than the FedAvg. The Ring DPA-SGD-MTL’s accuracy is also comparable to the FedAvg.\n           Accuracy for FedAvg-MTL and DPA-SGD MTL vs walk clock time    Accuracy for FedAvg-MTL and DPA-SGD MTL vs round number     We also compared the performance between FedAvg on Multi-Task Learning Framework (FedAvg-MTL) and our DPA-SGD on Multi-Task Learning Framework (Ring DPA-SGD-MTL) with a ring topology network. From the Figure, we clearly see that the training speed is around 20% faster than the FedAvg-MTL. The Ring DPA-SGD-MTL’s accuracy is slightly lower but comparable than the FedAvg-MTL . This proves that our convergence analysis that when decentralized the topology of the gradient exchanging, due to the sparsity of the topology has a negative impact on the convergence of the model. We have to balance the performance of the model and the training speed in the federated learning setting.\nWe will conduct more experiment on model performance and training speed on communication and computation heterogeneous setting (delay = 0ms, 300ms, 1s, 3s), different number of works (4, 8, 16, 32, 64, 128, 256, 512), and different topologies (0.3, 0.6, 0.9). Hopefully, we can finish these experiments before the NeurIPS deadline.\nAppendix Appendix I: Convergence Analysis Convergence of DPA-SGD: If the communication period $\\tau$ satisfies:\n$$ T \\geq K^{3} \\left( \\frac{1+\\zeta^{2}}{1-\\zeta^{2}} \\tau - 1 \\right)^{2} $$\nthen with learning rate $\\eta = \\frac{K}{LK} \\sqrt{\\frac{K}{T}}$ , the average-squared gradient norm after K iterations is bounded by\n$$ \\mathbb{E} \\Bigg[ \\frac{1}{T} \\sum\\limits_{t=1}^T {\\left\\lVert \\nabla F(\\vec u_{t}) \\right\\rVert}^{2} \\Bigg] \\leq \\frac{2[F(\\vec{x}_{1})-F_{inf}]}{\\eta T} + \\frac{2\\sigma^{2}}{\\sqrt{KT}} \\ = \\mathcal{O}(\\frac{1}{\\sqrt{KT}}) $$\nHow to choose $\\tau$, $\\zeta$: for a small number of well-connected workers, larger $\\tau$ is more preferable; for a large number of workers, using a sparse mixing matrix and small $\\tau$ gives better convergence.\nEffect of Extreme Large $K$: the iteration number T will be extreme large. To guaran-tee the convergence, try to reduce the worker number through system-wised optimization:\n Streaming training upload on-device data to the edge data center.  Appendix II: Poster Poster (click to see the pdf)\r","date":1556866800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556866800,"objectID":"0db6e08d4cded809a3c1c898b263c15f","permalink":"https://www.zijianhu.com/project/dpa_sgd/","publishdate":"2019-05-03T00:00:00-07:00","relpermalink":"/project/dpa_sgd/","section":"project","summary":"We designed a decentralized multi-task learning framework and a novel optimization algorithm to collaboratively train models over distributed devices by only sharing their gradient periodically","tags":["Machine Learning","Federated Learning","Multi-task Learning"],"title":"Decentralized Federated Multi-Task Learning and System Design","type":"project"},{"authors":["Zijian Hu","Lauren Klein"],"categories":[],"content":"The following project description is taken from Interaction Lab website.\n Infants engage in motor babbling that allows them to explore their space and learn what movements produce desired outcomes. Less motor babbling from infants can lead to developmental delays. Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice.\nTowards this end, we are collaborating with physical therapists to create approaches to predict the developmental status of infants using wearable sensors; running user studies that explore various robot rewards for contingent activities for the infant, as well as measuring the infant\u0026rsquo;s ability to mimic the robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to increase the infant\u0026rsquo;s engagement with the task.\n Development Detail   For project development, my contributions includes:\n Detecting and Tracking two Sphero SPRK+ robots with a wall-mounted camera Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset Visual Tracking:  With SiamRPN: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold With CSRT tracker: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location    ","date":1553583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553583600,"objectID":"4d1e0e9c7b1d701a2c5554b22de01a26","permalink":"https://www.zijianhu.com/project/baby/","publishdate":"2019-03-26T00:00:00-07:00","relpermalink":"/project/baby/","section":"project","summary":"Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide contingent positive feedback to increase exploration and expand early movement practice","tags":["Computer Vision","Robotics","Human-Robot Interaction"],"title":"Infant-Robot Interaction as an Early Intervention Strategy","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.zijianhu.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Zijian Hu"],"categories":[],"content":"Make sure you have installed all the dependencies and configured PYTHONPATH system variable correctly\n*see installation guide for detail\nControlling Robot   Turn on the robot. See this guide for detail\n  Start the robot bridge on your computer\n$ roslaunch nao_bringup nao_full_py.launch nao_ip:=\u0026lt;robot_ip\u0026gt; \\ roscore_ip:=\u0026lt;roscore_ip\u0026gt;  This will start the robot\u0026rsquo;s default configuration with the following publisher:\n joint_states tf top camera bottom camera left sonar right sonar microphone    To visualize the robot, open rviz\n$ rosrun rviz rviz   In top bar, go to File-\u0026gt;Open Config navigate to \u0026lt;your catkin workspace\u0026gt;/src/nao_robot/nao_description/config and open the file with .rviz extension  make sure you have nao_meshes installed   you should see something similar to the below screenshot     Controlling the robot\n  execute rosnode list to check if /nao_walker node is running\n  To turn on the motors\n$ rosservice call /body_stiffness/enable \u0026quot;{}\u0026quot;  To turn off the motors\n$ rosservice call /body_stiffness/disable \u0026quot;{}\u0026quot;    once the motors are on, use the following command to move the robot in x-direction\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 1.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'  To stop the robot, run:\n$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \\ '{linear: {x: 0.0, y: 0.0, z: 0.0}, \\ angular: {x: 0.0, y: 0.0, z: 0.0}}'      Next: NAOqi SDK Guide Reference  Getting started with ROS for Nao, including NAOqi and rviz nao_bringup  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554274800,"objectID":"19be522e21a2bbc9bfc08057e9b4a940","permalink":"https://www.zijianhu.com/post/nao-tutorial/getting-started/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/getting-started/","section":"post","summary":"This tutorial shows how to work with Physical NAO Robot","tags":["Tutorial","Robotics","NAO","ROS"],"title":"NAO Tutorial: Getting Started with NAO + ROS","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Installing the SDK   Make sure you have the following installed\n Python 2.7 CMake version 2.8.3 or higher    Download the following from Aldebaran Community website (you need to register an account in order to download the files)\n pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz naoqi-sdk-2.1.2.x-linux64.tar.gz [optional] choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar    Execute the following command and replace 2.1.2.x with the version you downloaded\nUnzip the tar files\n$ mkdir ~/naoqi\r$ tar xzf \u0026lt;path to NAOqi C++ SDK\u0026gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64\r$ tar xzf \u0026lt;path to NAOqi Python SDK\u0026gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64\r Check the installation by executing NAOqi\n$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi\r You should see output similiar to\nStarting NAOqi version 2.1.2.17\r.\r.\r.\rNAOqi is ready...\r Press CTRL-C to exit\nNow we need to add NAOqi SDK to system variables. Add the following lines at the end of ~/.bashrc\n$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH\r$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64\r$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64\r Execute source ~/.bashrc to apply the changes\nVerify in python console\nimport naoqi\r if correctly installed, there should be no error\n  Install ROS  See Official ROS Installation Tutorial  Install NAO package for ROS   Install the packages needed\n  replace kinetic to your ROS version if needed\n$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \\\rros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \\\rros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \\\rros-kinetic-camera-info-manager-py\r     Install the main package with sudo apt-get install ros-kinetic-nao-robot\n  Install packages for robot control\n$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \\\rros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \\\rros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \\\rros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \\\rros-kinetic-moveit\r   Install packages for simulation\nNotice: to install nao_meshes package, you need to agree the policy sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes\n  Next: Getting Started Reference  Python SDK Install Guide C++ SDK Installation Installation of ROS for usage with or on a NAO robot  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"d1a155d4c74134a706498e44f00eb8d9","permalink":"https://www.zijianhu.com/post/nao-tutorial/installation/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/installation/","section":"post","summary":"Installing NAOqi SDK and configure NAO with ROS","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: Installation","type":"post"},{"authors":["Zijian Hu"],"categories":[],"content":"Before starting  Make sure you have Choregraphe suite installed  See this tutorial for detail    Using Choregraphe  Follow this tutorial  Using Python in Choregraphe  Follow this tutorial  Using Dialog topic in Choregraphe  Follow this tutorial  Using Python  Follow this tutorial  Reference  Choregraphe Suite Installation Hello World 1 - using Choregraphe  ","date":1548226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554188400,"objectID":"26a08ddd1577f0e9da8d89fead5714d0","permalink":"https://www.zijianhu.com/post/nao-tutorial/nao-sdk/","publishdate":"2019-01-23T00:00:00-07:00","relpermalink":"/post/nao-tutorial/nao-sdk/","section":"post","summary":"This tutorial shows how to use NAOqi SDK","tags":["Tutorial","Robotics","NAO"],"title":"NAO Tutorial: NAOqi SDK","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://www.zijianhu.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]