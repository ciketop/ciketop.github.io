<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Human-Robot Interaction | Zijian Hu</title>
    <link>https://www.zijianhu.com/tag/human-robot-interaction/</link>
      <atom:link href="https://www.zijianhu.com/tag/human-robot-interaction/index.xml" rel="self" type="application/rss+xml" />
    <description>Human-Robot Interaction</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 31 May 2020 00:00:00 -0700</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/media/icon_hu175ac60b1877c266d8732aac3c921a86_8742_512x512_fill_lanczos_center_2.png</url>
      <title>Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/tag/human-robot-interaction/</link>
    </image>
    
    <item>
      <title>Can I Trust You? A User Study of Robot Mediation of a Support Group</title>
      <link>https://www.zijianhu.com/publication/birmingham-icra-2020/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/birmingham-icra-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust in Multi-Party Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/project/multi_party/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/multi_party/</guid>
      <description>&lt;p&gt;In this project, we designed and evaluated a novel framework for robot mediation of a support group.
We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (&lt;a href=&#34;https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wizard-of-Oz&lt;/a&gt;).
At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings.
In a &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;second-author paper&lt;/a&gt; at International Conference on Robotics and Automation (ICRA),
we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=multiparty_support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;














&lt;figure  id=&#34;figure-volunteers-demonstrating-the-study-setup&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Volunteers demonstrating the study setup&#34; srcset=&#34;
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_7f42a7f4bbd37dd6ebeadffc500591fc.jpg 400w,
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_74074270f216fcf6a73d06effbe6a739.jpg 760w,
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_7f42a7f4bbd37dd6ebeadffc500591fc.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Volunteers demonstrating the study setup
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In each study session, three participants were seated around the end of a table with a seated
&lt;a href=&#34;https://www.softbankrobotics.com/emea/index.php/en/nao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAO&lt;/a&gt; robot as shown in
Figure &lt;a href=&#34;#figure-volunteers-demonstrating-the-study-setup&#34;&gt;1&lt;/a&gt;.
The NAO robot, acting as a group moderator, was positioned towards the participants.
On the table, a 360-degree microphone and 3 cameras facing directly to the participants&#39; face were placed.
Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members.
The robot operator was seated behind a one-way mirror hidden from participants.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensitivity&lt;/th&gt;
&lt;th&gt;Question&lt;/th&gt;
&lt;th&gt;Disclosure&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;What do you like about school?&lt;/td&gt;
&lt;td&gt;When I feel stressed, I think my circuits might overload.&lt;br&gt;Does anyone else feel the same way?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;What are some of the hardest parts of school for you?&lt;/td&gt;
&lt;td&gt;Sometimes I worry I am inadequate for this school.&lt;br&gt;Does anyone else sometimes feel that too?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;td&gt;What will happen if you donâ€™t succeed in school?&lt;/td&gt;
&lt;td&gt;Sometimes I worry about if I belong here. &lt;br&gt;Does anyone else feel the same way?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;During the interaction, the robot can ask questions or make disclosures.
A total of 16 questions and 6 disclosures are available.
On average, 12 questions and 3 disclosures were made by the robot in each session.&lt;/p&gt;
&lt;p&gt;The questions and disclosures are grouped into low, medium, and high sensitivity as illustrated in the below table.&lt;/p&gt;














&lt;figure  id=&#34;figure-self-annotation-at-the-end-of-each-session&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Self-annotation at the end of each session&#34; srcset=&#34;
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_f185a0b114e2a2f4b71d1dc670ef2d1e.jpg 400w,
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_18a3e6c8642b06fa0a6b1be14de65482.jpg 760w,
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_f185a0b114e2a2f4b71d1dc670ef2d1e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Self-annotation at the end of each session
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-self-annotation-ui&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Self-annotation UI&#34; srcset=&#34;
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_0eaed455e52637024578e82b913c3f0d.jpeg 400w,
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_47efa0fb9bb1d5086cd36d0a59ab7fc3.jpeg 760w,
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_1200x1200_fit_q75_lanczos.jpeg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_0eaed455e52637024578e82b913c3f0d.jpeg&#34;
               width=&#34;760&#34;
               height=&#34;374&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Self-annotation UI
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.&lt;/p&gt;
&lt;p&gt;The detail for the procedure of the study can be found &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;














&lt;figure  id=&#34;figure-control-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Control architecture&#34; srcset=&#34;
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_ea79ef16c416ef42845a6e21c1a777b3.png 400w,
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_a7aa5f0207f9c10c827af96ade1ef0b2.png 760w,
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_ea79ef16c416ef42845a6e21c1a777b3.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Control architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-control-architecture&#34;&gt;3&lt;/a&gt;.
The wizard controls the robot through the Wizard-of-Oz web interface.&lt;/p&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed NAO control program&lt;/li&gt;
&lt;li&gt;Designed and implemented web-based Wizard of Oz controller&lt;/li&gt;
&lt;li&gt;Designed and implemented self-annotation website&lt;/li&gt;
&lt;li&gt;Developed data collection program for one depth camera, three webcams and one 360 degree microphone&lt;/li&gt;
&lt;li&gt;Data post-processing for data whitening and fast data loading&lt;/li&gt;
&lt;li&gt;Turn-taking prediction with &lt;a href=&#34;http://zpascal.net/cvpr2017/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporal Convolutional Networks (TCN)&lt;/a&gt; and LSTM for multi-modal input&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Chris Birmingham&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Kartik Mahajan&lt;/span&gt;, &lt;span &gt;
      Eli Reber&lt;/span&gt;, &lt;span &gt;
      Maja J. MatariÄ‡&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;Can I Trust You? A User Study of Robot Mediation of a Support Group&lt;/a&gt;.
  &lt;em&gt;2020 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.04671&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/birmingham-icra-2020/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/multi_party/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/ICRA40945.2020.9196875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</guid>
      <description>&lt;h2 id=&#34;linked-material&#34;&gt;&lt;strong&gt;Linked material&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;YouTube video for &amp;ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&amp;rdquo;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Telepresence Robot for K-12 Remote Education</title>
      <link>https://www.zijianhu.com/project/nri_kids/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/nri_kids/</guid>
      <description>&lt;p&gt;In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=nrikids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;We used an &lt;a href=&#34;https://ohmnilabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohmni&lt;/a&gt; robot equipped with an arm and a Linux PC.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions consist of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed customized web user interface for robot arm control&lt;/li&gt;
&lt;li&gt;Designed and implemented communication protocol and software between the Linux PC and the Ohmni server&lt;/li&gt;
&lt;li&gt;Developed user interface with a turning dial for robot arm control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;
&lt;p&gt;






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Naomi T. Fitter&lt;/span&gt;, &lt;span &gt;
      Youngseok Joung&lt;/span&gt;, &lt;span &gt;
      Marton Demeter&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Maja J. MatariÄ‡&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-hardware/&#34;&gt;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://robotics.usc.edu/publications/media/uploads/pubs/pubdb_1048_87866f1437ff45e99c4fc89e3f9f45be.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-hardware/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  







  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://youtu.be/Ft8XCAIbslE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  









  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Naomi T. Fitter&lt;/span&gt;, &lt;span &gt;
      Youngseok Joung&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Marton Demeter&lt;/span&gt;, &lt;span &gt;
      Maja J. MatariÄ‡&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-ui/&#34;&gt;User Interface Tradeoffs for Remote Deictic Gesturing&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://robotics.usc.edu/publications/media/uploads/pubs/pubdb_1049_35632024bfb044c7ba2d4e9cf53e560e.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-ui/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>User Interface Tradeoffs for Remote Deictic Gesturing</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
