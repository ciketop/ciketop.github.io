<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Zijian Hu</title>
    <link>https://www.zijianhu.com/tag/computer-vision/</link>
      <atom:link href="https://www.zijianhu.com/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 05 Dec 2020 00:00:00 -0700</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/media/icon_hub8aaec6c2c2e1082c2b24756ea87d425_171065_512x512_fill_lanczos_center_2.png</url>
      <title>Computer Vision</title>
      <link>https://www.zijianhu.com/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>Learning with Less Labeling (LwLL)</title>
      <link>https://www.zijianhu.com/project/lwll/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/lwll/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;This is an ongoing project. The details are subject to change.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://www.darpa.mil/program/learning-with-less-labeling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DARPA&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In supervised machine learning (ML), the ML system learns by example to recognize things, such as objects in images or speech. Humans provide these examples to ML systems during their training in the form of labeled data. With enough labeled data, we can generally build accurate pattern recognition models.&lt;/p&gt;
&lt;p&gt;The problem is that training accurate models currently requires lots of labeled data. For tasks like machine translation, speech recognition or object recognition, deep neural networks (DNNs) have emerged as the state of the art, due to the superior accuracy they can achieve. To gain this advantage over other techniques, however, DNN models need more data, typically requiring 109 or 1010 labeled training examples to achieve good performance.&lt;/p&gt;
&lt;p&gt;The commercial world has harvested and created large sets of labeled data for training models. These datasets are often created via crowdsourcing: a cheap and efficient way to create labeled data. Unfortunately, crowdsourcing techniques are often not possible for proprietary or sensitive data. Creating data sets for these sorts of problems can result in 100x higher costs and 50x longer time to label.&lt;/p&gt;
&lt;p&gt;To make matters worse, machine learning models are brittle, in that their performance can degrade severely with small changes in their operating environment. For instance, the performance of computer vision systems degrades when data is collected from a new sensor and new collection viewpoints. Similarly, dialog and text understanding systems are very sensitive to changes in formality and register. As a result, additional labels are needed after initial training to adapt these models to new environments and data collection conditions. For many problems, the labeled data required to adapt models to new environments approaches the amount required to train a new model from scratch.&lt;/p&gt;
&lt;p&gt;The Learning with Less Labeling (LwLL) program aims to make the process of training machine learning models more efficient by reducing the amount of labeled data required to build a model by six or more orders of magnitude, and by reducing the amount of data needed to adapt models to new environments to tens to hundreds of labeled examples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;lwll-evaluation&#34;&gt;LwLL Evaluation&lt;/h2&gt;
&lt;p&gt;The evaluation has 3 types of tasks: Image Classification, Object Detection and Machine Translation.
Our team is focusing on the image classification task.
Each task includes a base phase and an adaptation pha where each phase consists of 6 to 8 stages.
For image classification, pre-trained models on predefined whitelisted datasets are allowed.&lt;/p&gt;














&lt;figure  id=&#34;figure-number-of-labels-in-each-stage&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Number of labels in each stage&#34; srcset=&#34;
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_ebe883e776b8bd89780073fbc18e8976.png 400w,
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_4763e4ead4909379d09f8ebd07279ec2.png 760w,
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_ebe883e776b8bd89780073fbc18e8976.png&#34;
               width=&#34;760&#34;
               height=&#34;457&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Number of labels in each stage
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The full training set without any label is given at the beginning.
At stage 1, can request 1 label per category in the training set (label budget is 1).
As the training progress, the label budget is increased.&lt;/p&gt;














&lt;figure  id=&#34;figure-lwll-evaluation-model-pipeline&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;LwLL evaluation model pipeline&#34; srcset=&#34;
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_b1b3cf849fba56be982376556309c68c.png 400w,
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_c90e3d93cf702df6a0d4165d4077fecd.png 760w,
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_b1b3cf849fba56be982376556309c68c.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      LwLL evaluation model pipeline
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Our team used a few-shot learning method for the first 2 to 3 checkpoints when the labeled set size is small.
Once sufficient number of labeled data samples become available, the training was handed to our semi-supervised algorithm.&lt;/p&gt;
&lt;h2 id=&#34;simple-similar-pseudo-label-exploitation-for-semi-supervised-classification&#34;&gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/h2&gt;







  
    









  




&lt;div class=&#34;view-list-item&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;a href=&#34;https://www.zijianhu.com/publication/hu-2020-simple/&#34; &gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/a&gt;

  

  
  &lt;div class=&#34;article-metadata&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Zhengyu Yang&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Xuefeng Hu&lt;/span&gt;, &lt;span &gt;
      Ram Nevatia&lt;/span&gt;
  &lt;/div&gt;
  

  
  &lt;div class=&#34;btn-links&#34;&gt;
    








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2103.16725&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/hu-2020-simple/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/zijian-hu/SimPLE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;




  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/lwll/&#34;&gt;
    Project
  &lt;/a&gt;
  



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1XUTD1MABhzNa92lbbX5jeT9WupgRdgX4/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Poster
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1e1v0tUvjuN5A8YOntQn1hbxGPLV6Yp5I/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Slides
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1SiXiXSRTSSlUn6_JT64QJajnXrmMpYga/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;





  &lt;/div&gt;
  

&lt;/div&gt;

  
















&lt;figure  id=&#34;figure-simple-algorithm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;SimPLE algorithm&#34; srcset=&#34;
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_0605d2c51801b670c0e6f8f0c76155a7.png 400w,
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_fd8bb7a4688eb35996a473879abb02fe.png 760w,
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_0605d2c51801b670c0e6f8f0c76155a7.png&#34;
               width=&#34;760&#34;
               height=&#34;265&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      SimPLE algorithm
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We proposed a novel semi-supervised classification algorithm, SimPLE (Figure &lt;a href=&#34;#figure-simple-algorithm&#34;&gt;3&lt;/a&gt;),
that focuses on the less studied relationship between the high confidence unlabeled data that are similar to each other.&lt;/p&gt;














&lt;figure  id=&#34;figure-pair-loss-overview&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Pair Loss Overview.&#34; srcset=&#34;
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_94280aeecc749839c06355a9e4d70c27.png 400w,
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_27c79b729d9183d7453a356cbc09af89.png 760w,
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_94280aeecc749839c06355a9e4d70c27.png&#34;
               width=&#34;760&#34;
               height=&#34;591&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Pair Loss Overview.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-pair-loss-overview&#34;&gt;4&lt;/a&gt;, the new proposed Pair Loss minimizes the statistical distance between high confidence pseudo labels with similarity above a certain threshold.
The similarity threshold &amp;ldquo;extended&amp;rdquo; our confidence threshold in an adaptive manner,
as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level.&lt;/p&gt;
&lt;p&gt;Formally, we defined the Pair Loss as the following:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathcal{L_P} &amp;amp;= \frac{1}{\binom{KB}{2}}
\sum_{\mathcal{U}&#39;}
\mathbb{1}_{\max\left(q_l\right) &amp;gt; \tau_c} \cdot
\mathbb{1}_{f_{\operatorname{sim}}\left(q_l,q_r\right) &amp;gt; \tau_s} \\&lt;br&gt;
&amp;amp;\cdot f_{\operatorname{dist}}\left(q_l, \mathrm{p}_{\text{model}}\left(\tilde{y} \mid v_r ; \theta\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K$: number of augmentations&lt;/li&gt;
&lt;li&gt;$B$: batch size&lt;/li&gt;
&lt;li&gt;$\mathcal{U}&#39;$: unlabeled batch&lt;/li&gt;
&lt;li&gt;$\tau_c$: confidence threshold&lt;/li&gt;
&lt;li&gt;$\tau_s$: similarity threshold&lt;/li&gt;
&lt;li&gt;$f_{\operatorname{sim}}\left(\cdot,\cdot\right)$: similarity function
&lt;ul&gt;
&lt;li&gt;We use &lt;a href=&#34;http://www.jstor.org/stable/25047882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya coefficient&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$f_{\operatorname{dist}}\left(\cdot,\cdot\right)$: distance function
&lt;ul&gt;
&lt;li&gt;We use $1-$ &lt;a href=&#34;http://www.jstor.org/stable/25047882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya coefficient&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Dataset&lt;/th&gt;
    &lt;th&gt;Num. Labels&lt;/th&gt;
    &lt;th&gt;Method&lt;/th&gt;
    &lt;th&gt;Backbone&lt;/th&gt;
    &lt;th&gt;Top-1 Accuracy&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;4&#34;&gt;CIFAR-100&lt;/td&gt;
    &lt;td rowspan=&#34;4&#34;&gt;10000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;71.69%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;ReMixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;76.97%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;FixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;77.40%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-8&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;78.11%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;Mini-ImageNet&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;4000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;55.47%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;60.50%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-2&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;66.55%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;ImageNet to DomainNet-Real&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;3795&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;ResNet-50&lt;/td&gt;
    &lt;td&gt;35.34%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;ResNet-50&lt;/td&gt;
    &lt;td&gt;35.16%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;ResNet-50&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;50.90%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;DomainNet-Real to Mini-ImageNet&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;4000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;53.39%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;55.75%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-2&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;58.73%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our algorithm, SimPLE, achieved state-of-the-art performance on standard SSL benchmarks and achieved the best accuracy on some tasks in LwLL evaluation. We also evaluated our method in the transfer setting where our algorithm outperforms prior works and supervised baseline by a large margin.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Designed a novel algorithm for semi-supervised classification&lt;/li&gt;
&lt;li&gt;Evaluated our algorithm on standard benchmarks (CIFAR-10, CIFAR-100, SVHN, Mini-ImageNet)&lt;/li&gt;
&lt;li&gt;Evaluated our algorithm in the transfer learning setting (on Mini-ImageNet, DomainNet-Real, AID, RESISC45), where models are initialized models pretrained on ImageNet or DomainNet-Real&lt;/li&gt;
&lt;li&gt;Distributed training with PyTorch Distributed Data Parallel&lt;/li&gt;
&lt;li&gt;GPU accelerated data augmentation with &lt;a href=&#34;https://kornia.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kornia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Zhengyu Yang&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Xuefeng Hu&lt;/span&gt;, &lt;span &gt;
      Ram Nevatia&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://www.zijianhu.com/publication/hu-2020-simple/&#34;&gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/a&gt;.
  &lt;em&gt;2021 Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2103.16725&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/hu-2020-simple/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/zijian-hu/SimPLE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;




  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/lwll/&#34;&gt;
    Project
  &lt;/a&gt;
  



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1XUTD1MABhzNa92lbbX5jeT9WupgRdgX4/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Poster
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1e1v0tUvjuN5A8YOntQn1hbxGPLV6Yp5I/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Slides
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1SiXiXSRTSSlUn6_JT64QJajnXrmMpYga/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;




&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Resource Distribution in Human-Robot Teams</title>
      <link>https://www.zijianhu.com/project/robot_team/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/robot_team/</guid>
      <description>&lt;p&gt;Our work looks to understand how robots can be successfully integrated into human teams. Much work in the Human-Robot Interaction space has investigated one on one interactions with one robot and one human. Our work looks to fill this gap of knowledge by providing an algorithm that takes into account the social construct of human fairness and optimization through a Multi Armed Bandit variant algorithm. We apply this algorithm to a robot tasked with distributing resources to different human team members&lt;/p&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;














&lt;figure  id=&#34;figure-study-setup&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Study setup&#34; srcset=&#34;
               /project/robot_team/images/studySetup_hu76b3e537e205736635d7ca0202a3cf5b_777755_b22b18d47bd8291ec8f62716d097d558.png 400w,
               /project/robot_team/images/studySetup_hu76b3e537e205736635d7ca0202a3cf5b_777755_a3c92289d4ca76572d5bf0e1c9423ba5.png 760w,
               /project/robot_team/images/studySetup_hu76b3e537e205736635d7ca0202a3cf5b_777755_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/robot_team/images/studySetup_hu76b3e537e205736635d7ca0202a3cf5b_777755_b22b18d47bd8291ec8f62716d097d558.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Study setup
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-study-setup&#34;&gt;1&lt;/a&gt;, In each study session, several participants were seated around the table.
Each participant is expected to complete their own task.
The robot is tasked to bring the puzzle piece to each participant in an order determined by its resource distribution algorithm.
Out goal is to evaluate the effectiveness and fairness of different resource distribution algorithms.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;














&lt;figure  id=&#34;figure-control-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Control architecture&#34; srcset=&#34;
               /project/robot_team/images/controlArchitecture_hu6303699076812cc32636c1b3c5e734bb_935427_139424e08ede7dc3c51bb00cd1801959.png 400w,
               /project/robot_team/images/controlArchitecture_hu6303699076812cc32636c1b3c5e734bb_935427_e17bfe3bc8a349a268823c9b1658b7f3.png 760w,
               /project/robot_team/images/controlArchitecture_hu6303699076812cc32636c1b3c5e734bb_935427_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/robot_team/images/controlArchitecture_hu6303699076812cc32636c1b3c5e734bb_935427_139424e08ede7dc3c51bb00cd1801959.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Control architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;On the table, a unique ArUco maker is attached in front of each participant, in front of the puzzle pieces, and on top of the robot.
A camera mounted on a tripod that oversees the entire table is used for robot localization and navigation.&lt;/p&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-control-architecture&#34;&gt;2&lt;/a&gt;.
The main component in the perception system is the ArUco marker detector.
The controller is composed of a finite-state machine and resource distribution algorithm.
The finite-state machine encodes the state transition while the resource distribution algorithm determines where (which participant) to deliver the current puzzle piece.
The perception system runs concurrently with the controller.&lt;/p&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designed and implemented multi-threaded perception and control system for &lt;a href=&#34;https://anki.com/en-ca/vector.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anki Vector&lt;/a&gt; robot&lt;/li&gt;
&lt;li&gt;Camera calibration with ArUco Markers&lt;/li&gt;
&lt;li&gt;ArUco Marker detection for navigation and robot localization&lt;/li&gt;
&lt;li&gt;Path planning using finite state machine&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
