<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics | Zijian Hu</title>
    <link>https://www.zijianhu.com/tag/robotics/</link>
      <atom:link href="https://www.zijianhu.com/tag/robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Robotics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 31 May 2020 00:00:00 -0700</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/images/icon_hub8aaec6c2c2e1082c2b24756ea87d425_171065_512x512_fill_lanczos_center_2.png</url>
      <title>Robotics</title>
      <link>https://www.zijianhu.com/tag/robotics/</link>
    </image>
    
    <item>
      <title>Can I Trust You? A User Study of Robot Mediation of a Support Group</title>
      <link>https://www.zijianhu.com/publication/birmingham-icra-2020/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/birmingham-icra-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust in Multi-Party Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/project/multi_party/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/multi_party/</guid>
      <description>&lt;p&gt;In this project, we designed and evaluated a novel framework for robot mediation of a support group.
We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (&lt;a href=&#34;https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wizard-of-Oz&lt;/a&gt;).
At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings.
In a &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;second-author paper&lt;/a&gt; at International Conference on Robotics and Automation (ICRA),
we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=multiparty_support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;






  



  
  











&lt;figure id=&#34;figure-volunteers-demonstrating-the-study-setup&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; data-caption=&#34;Volunteers demonstrating the study setup&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;4032&#34; height=&#34;3024&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Volunteers demonstrating the study setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In each study session, three participants were seated around the end of a table with a seated &lt;a href=&#34;https://www.softbankrobotics.com/emea/index.php/en/nao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAO&lt;/a&gt; robot as shown in Figure 1.
The NAO robot, acting as a group moderator, was positioned towards the participants.
On the table, a 360-degree microphone and 3 cameras facing directly to the participants&#39; face were placed.
Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members.
The robot operator was seated behind a one-way mirror hidden from participants.&lt;/p&gt;
&lt;p&gt;To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.&lt;/p&gt;
&lt;p&gt;The detail for the procedure of the study can be found &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed NAO control program&lt;/li&gt;
&lt;li&gt;Designed and implemented web-based Wizard of Oz controller&lt;/li&gt;
&lt;li&gt;Designed and implemented self-annotation website&lt;/li&gt;
&lt;li&gt;Developed data collection program for one depth camera, three webcams and one 360 degree microphone&lt;/li&gt;
&lt;li&gt;Data post-processing for data whitening and fast data loading&lt;/li&gt;
&lt;li&gt;Turn-taking prediction with &lt;a href=&#34;http://zpascal.net/cvpr2017/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporal Convolutional Networks (TCN)&lt;/a&gt; and LSTM for multi-modal input&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/chris-birmingham/&#34;&gt;Chris Birmingham&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/kartik-mahajan/&#34;&gt;Kartik Mahajan&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/eli-reber/&#34;&gt;Eli Reber&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. MatariÄ‡&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;Can I Trust You? A User Study of Robot Mediation of a Support Group&lt;/a&gt;.
  &lt;em&gt;2020 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.04671&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/birmingham-icra-2020/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/multi_party/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/ICRA40945.2020.9196875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</guid>
      <description>&lt;h2 id=&#34;linked-material&#34;&gt;&lt;strong&gt;Linked material&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;YouTube video for &amp;ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&amp;rdquo;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Telepresence Robot for K-12 Remote Education</title>
      <link>https://www.zijianhu.com/project/nri_kids/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/nri_kids/</guid>
      <description>&lt;p&gt;In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=nrikids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;We used an &lt;a href=&#34;https://ohmnilabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohmni&lt;/a&gt; robot equipped with an arm and a Linux PC.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions consist of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed customized web user interface for robot arm control&lt;/li&gt;
&lt;li&gt;Designed and implemented communication protocol and software between the Linux PC and the Ohmni server&lt;/li&gt;
&lt;li&gt;Developed user interface with a turning dial for robot arm control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;
&lt;p&gt;






  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. MatariÄ‡&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-hardware/&#34;&gt;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1048/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-hardware/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  







  
  
    
  
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://youtu.be/Ft8XCAIbslE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  









  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. MatariÄ‡&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-ui/&#34;&gt;User Interface Tradeoffs for Remote Deictic Gesturing&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1049/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-ui/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>User Interface Tradeoffs for Remote Deictic Gesturing</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resource Distribution in Human-Robot Teams</title>
      <link>https://www.zijianhu.com/project/robot_team/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/robot_team/</guid>
      <description>&lt;p&gt;Our work looks to understand how robots can be successfully integrated into human teams. Much work in the Human-Robot Interaction space has investigated one on one interactions with one robot and one human. Our work looks to fill this gap of knowledge by providing an algorithm that takes into account the social construct of human fairness and optimization through a Multi Armed Bandit variant algorithm. We apply this algorithm to a robot tasked with distributing resources to different human team members&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designed and implemented multi-threaded perception and control system for &lt;a href=&#34;https://anki.com/en-ca/vector.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anki Vector&lt;/a&gt; robot&lt;/li&gt;
&lt;li&gt;Camera calibration with ArUco Markers&lt;/li&gt;
&lt;li&gt;ArUco Marker detection for navigation and robot localization&lt;/li&gt;
&lt;li&gt;Path planning using finite state machine&lt;/li&gt;
&lt;li&gt;Resource distribution with Multi Armed Bandit variant algorithm&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Getting Started with NAO &#43; ROS</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/getting-started/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/getting-started/</guid>
      <description>&lt;p&gt;Make sure you have installed all the dependencies and configured &lt;strong&gt;PYTHONPATH&lt;/strong&gt; system variable correctly&lt;/p&gt;
&lt;p&gt;*&lt;em&gt;see &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/installation&#34;&gt;installation guide&lt;/a&gt; for detail&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;controlling-robot&#34;&gt;Controlling Robot&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Turn on the robot. See &lt;a href=&#34;http://doc.aldebaran.com/2-1/nao/getting_out_of_the_box.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt; for detail&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the robot bridge on your computer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ roslaunch nao_bringup nao_full_py.launch nao_ip:=&amp;lt;robot_ip&amp;gt; \
roscore_ip:=&amp;lt;roscore_ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start the robot&amp;rsquo;s default configuration with the following publisher:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;joint_states&lt;/li&gt;
&lt;li&gt;tf&lt;/li&gt;
&lt;li&gt;top camera&lt;/li&gt;
&lt;li&gt;bottom camera&lt;/li&gt;
&lt;li&gt;left sonar&lt;/li&gt;
&lt;li&gt;right sonar&lt;/li&gt;
&lt;li&gt;microphone&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To visualize the robot, open &lt;strong&gt;rviz&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosrun rviz rviz
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;In top bar, go to &lt;code&gt;File-&amp;gt;Open Config&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;navigate to &lt;code&gt;&amp;lt;your catkin workspace&amp;gt;/src/nao_robot/nao_description/config&lt;/code&gt; and open the file with &lt;strong&gt;.rviz&lt;/strong&gt; extension
&lt;ul&gt;
&lt;li&gt;make sure you have &lt;a href=&#34;http://wiki.ros.org/nao_meshes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_meshes&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you should see something similar to the below screenshot
&lt;img src=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started?action=AttachFile&amp;amp;do=get&amp;amp;target=NaoRviz.png&#34; alt=&#34;NAO rviz&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Controlling the robot&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;execute &lt;code&gt;rosnode list&lt;/code&gt; to check if &lt;strong&gt;/nao_walker&lt;/strong&gt; node is running&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To turn on the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/enable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To turn off the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/disable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;once the motors are on, use the following command to move the robot in x-direction&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 1.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To stop the robot, run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 0.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;next-naoqi-sdk-guidepostnao-tutorialnao-sdk&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/nao-sdk&#34;&gt;NAOqi SDK Guide&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting started with ROS for Nao, including NAOqi and rviz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao_bringup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_bringup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Installation</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/installation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/installation/</guid>
      <description>&lt;h2 id=&#34;installing-the-sdk&#34;&gt;Installing the SDK&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make sure you have the following installed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/download/releases/2.7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cmake.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMake&lt;/a&gt; version 2.8.3 or higher&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the following from &lt;a href=&#34;https://community.aldebaran.com/en/resources/software&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aldebaran Community&lt;/a&gt; website (&lt;em&gt;you need to register an account in order to download the files&lt;/em&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;naoqi-sdk-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;[optional]&lt;/em&gt;&lt;/strong&gt; choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command and replace &lt;strong&gt;2.1.2.x&lt;/strong&gt; with the version you downloaded&lt;/p&gt;
&lt;p&gt;Unzip the tar files&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir ~/naoqi
$ tar xzf &amp;lt;path to NAOqi C++ SDK&amp;gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64
$ tar xzf &amp;lt;path to NAOqi Python SDK&amp;gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the installation by executing NAOqi&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see output similiar to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Starting NAOqi version 2.1.2.17
.
.
.
NAOqi is ready...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;strong&gt;CTRL-C&lt;/strong&gt; to exit&lt;/p&gt;
&lt;p&gt;Now we need to add &lt;strong&gt;NAOqi SDK&lt;/strong&gt; to system variables. Add the following lines at the end of &lt;strong&gt;~/.bashrc&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH
$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64
$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Execute &lt;code&gt;source ~/.bashrc&lt;/code&gt; to apply the changes&lt;/p&gt;
&lt;p&gt;Verify &lt;em&gt;in python console&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if correctly installed, there should be no error&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-ros&#34;&gt;Install ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://wiki.ros.org/kinetic/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Official ROS Installation Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-nao-package-for-ros&#34;&gt;Install &lt;strong&gt;NAO package&lt;/strong&gt; for ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install the packages needed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;replace kinetic to your ROS version if needed&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \
ros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \
ros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \
ros-kinetic-camera-info-manager-py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the main package with &lt;code&gt;sudo apt-get install ros-kinetic-nao-robot&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for robot control&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \
ros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \
ros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \
ros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \
ros-kinetic-moveit
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for simulation&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Notice: to install nao_meshes package, you need to agree the policy&lt;/em&gt;
&lt;code&gt;sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;next-getting-startedpostnao-tutorialgetting-started&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/getting-started/&#34;&gt;Getting Started&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/python/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python SDK Install Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/cpp/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C++ SDK Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Installation of ROS for usage with or on a NAO robot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: NAOqi SDK</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</guid>
      <description>&lt;h2 id=&#34;before-starting&#34;&gt;Before starting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you have &lt;strong&gt;Choregraphe suite&lt;/strong&gt; installed
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt; for detail&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--## Hello World Example--&gt;
&lt;h3 id=&#34;using-choregraphe&#34;&gt;Using Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python-in-choregraphe&#34;&gt;Using Python in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_script.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-dialog-topic-in-choregraphe&#34;&gt;Using Dialog topic in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_dialog.htmll&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python&#34;&gt;Using Python&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_python.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choregraphe Suite Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hello World 1 - using Choregraphe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
