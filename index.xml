<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zijian Hu</title>
    <link>https://www.zijianhu.com/</link>
      <atom:link href="https://www.zijianhu.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Zijian Hu</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/media/icon_hud1ad56724d07d7afb4be08de8db336b3_28000_512x512_fill_lanczos_center_2.png</url>
      <title>Zijian Hu</title>
      <link>https://www.zijianhu.com/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://www.zijianhu.com/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://www.zijianhu.com/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FedLLM: Build Your Own Large Language Models on Proprietary Data using the FedML Platform</title>
      <link>https://www.zijianhu.com/project/fedllm/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/fedllm/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;This is an ongoing project. The details are subject to change.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification</title>
      <link>https://www.zijianhu.com/publication/hu-2020-simple/</link>
      <pubDate>Sat, 19 Jun 2021 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/hu-2020-simple/</guid>
      <description>&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#semi-supervised-learning&#34;&gt;Semi-supervised Learning&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#method&#34;&gt;Method&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#problem-description&#34;&gt;Problem Description&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#augmentation-strategy&#34;&gt;Augmentation Strategy&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#pseudo-labeling&#34;&gt;Pseudo-labeling&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#loss&#34;&gt;Loss&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#simple-algorithm&#34;&gt;SimPLE Algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Deep learning has recently achieved state-of-the-art performance on many computer vision tasks.
labeling large datasets is very expensive and often not feasible, especially in domains that require expertise to provide labels.
Semi-Supervised Learning (SSL), on the other hand, can take advantage of partially labeled data,
which is much more easily available,
as shown in Figure &lt;a href=&#34;#figure-illustration-of-an-image-set-with-a-limited-amount-of-labeled-images-among-a-large-number-of-unlabeled-images&#34;&gt;1&lt;/a&gt;, for example.&lt;/p&gt;














&lt;figure  id=&#34;figure-illustration-of-an-image-set-with-a-limited-amount-of-labeled-images-among-a-large-number-of-unlabeled-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Illustration of an image set with a limited amount of labeled images among a large number of unlabeled images.&#34; srcset=&#34;
               /publication/hu-2020-simple/images/problem_hue2b165cb1d98fede9488ffa362868af2_2811778_0f338128b15e47acc02cb32e5fe382ff.png 400w,
               /publication/hu-2020-simple/images/problem_hue2b165cb1d98fede9488ffa362868af2_2811778_ac3c1424891747c7e5fff2fbe36407f9.png 760w,
               /publication/hu-2020-simple/images/problem_hue2b165cb1d98fede9488ffa362868af2_2811778_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/publication/hu-2020-simple/images/problem_hue2b165cb1d98fede9488ffa362868af2_2811778_0f338128b15e47acc02cb32e5fe382ff.png&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Illustration of an image set with a limited amount of labeled images among a large number of unlabeled images.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;semi-supervised-learning&#34;&gt;Semi-supervised Learning&lt;/h3&gt;
&lt;p&gt;A critical problem in semi-supervised learning is how to generalize the information learned from limited label data to unlabeled data.
Following the &lt;em&gt;continuity assumption&lt;/em&gt; that close data have a higher probability of sharing the same label, many approaches have been developed, including the recently proposed &lt;a href=&#34;https://arxiv.org/abs/1904.04717&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Label Propagation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another critical problem in semi-supervised learning is how to directly learn from the large amount of unlabeled data.
Maintaining consistency between differently augmented unlabeled data has been recently studied and proved to be an effective way to learn from unlabeled data in both self-supervised learning.&lt;/p&gt;
&lt;p&gt;The recently proposed &lt;a href=&#34;https://arxiv.org/abs/1905.02249&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MixMatch&lt;/a&gt; combines the above techniques and designed a unified loss term to let the model learn from differently augmented labeled and unlabeled data, together with the &lt;a href=&#34;https://openreview.net/forum?id=ByxtC2VtPB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mix-up&lt;/a&gt; technique, which encourages convex behavior between samples to increase model&amp;rsquo;s generalization ability.
&lt;a href=&#34;https://openreview.net/forum?id=HklkeR4KPB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ReMixMatch&lt;/a&gt; further improves the MixMatch by introducing the distribution alignment and Augmentation Anchoring techniques, which allows the model to accommodate and leverage from the heavily augmented samples.
&lt;a href=&#34;https://arxiv.org/abs/2001.07685&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FixMatch&lt;/a&gt; simplified its previous works by introducing a confidence threshold into its unsupervised objective function and achieves state-of-the-art performance over the standard benchmarks.&lt;/p&gt;
&lt;p&gt;In this paper, we propose to take advantage of the relationship between different unlabeled samples.
We introduce a novel Pair Loss, which encourages a pair of similar unlabeled samples (in the augmented space) to have similar predictions if at least one of them is of high confidence in its prediction.
Combining the techniques developed by the MixMatch family, we propose the SimPLE algorithm.
As shown in Figure &lt;a href=&#34;#figure-an-overview-of-the-proposed-simple-algorithm&#34;&gt;2&lt;/a&gt;, the SimPLE algorithm generates pseudo labels of unlabeled samples by averaging and sharpening the predictions on multiple weakly augmented variations of the same sample.&lt;/p&gt;














&lt;figure  id=&#34;figure-an-overview-of-the-proposed-simple-algorithm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;An overview of the proposed SimPLE algorithm.&#34; srcset=&#34;
               /publication/hu-2020-simple/images/overview_hu7e6c5a87fea892f23597c7d6d500ed86_413333_6c500e3411e5ed516bb4a69ee32cb91c.png 400w,
               /publication/hu-2020-simple/images/overview_hu7e6c5a87fea892f23597c7d6d500ed86_413333_9d4b0d149d455640abe5b158f0c3e6e8.png 760w,
               /publication/hu-2020-simple/images/overview_hu7e6c5a87fea892f23597c7d6d500ed86_413333_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/publication/hu-2020-simple/images/overview_hu7e6c5a87fea892f23597c7d6d500ed86_413333_6c500e3411e5ed516bb4a69ee32cb91c.png&#34;
               width=&#34;760&#34;
               height=&#34;265&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      An overview of the proposed SimPLE algorithm.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Then, we use both the labels and pseudo labels to compute the supervised cross-entropy loss and unsupervised $L2$ distance loss.
These two terms push the decision boundaries to go through low-density areas and encourage consistency among different variations of the same samples.
Finally, with the newly proposed Pair Loss, we harness the relationships among the pseudo labels of different samples by encouraging consistency among different unlabeled samples which share a great similarity.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;To take full advantage of the vast quantity of unlabeled samples in SSL problems, we propose the SimPLE algorithm that focuses on the relationship between unlabeled samples.&lt;/p&gt;
&lt;h3 id=&#34;problem-description&#34;&gt;Problem Description&lt;/h3&gt;
&lt;p&gt;We define the semi-supervised image classification problem as following. In a $L$-class classification setting, we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{X}=\left(\left(x_{b}, y_{b}\right) ; b \in(1, \ldots, B)\right)$: a batch of labeled data&lt;/li&gt;
&lt;li&gt;$\mathcal{U}=\left(u_{b} ; b \in(1, \ldots, B)\right)$: a batch of unlabeled data&lt;/li&gt;
&lt;li&gt;$\mathrm{p}_{\text{model}}\left(\tilde{y} \mid x ; \theta\right)$: the model&amp;rsquo;s predicted softmax class probability of input $x$ parameterized by weight $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;augmentation-strategy&#34;&gt;Augmentation Strategy&lt;/h3&gt;
&lt;p&gt;Our algorithm uses Augmentation Anchoring, in which pseudo labels come from weakly augmented samples act as &amp;ldquo;anchor,&amp;rdquo; and we align the strongly augmented samples to the &amp;ldquo;anchor.&amp;rdquo;
Our weak augmentation, follows that of MixMatch family, contains a random cropping followed by a random horizontal flip.
We use &lt;a href=&#34;https://arxiv.org/abs/1909.13719&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RandAugment&lt;/a&gt; or a fixed augmentation strategy that contains difficult transformations such as random affine and color jitter as strong augmentation.
For every batch, RandAugment randomly selects a fixed number of augmentations from a predefined pool; the intensity of each transformation is determined by a magnitude parameter.&lt;/p&gt;
&lt;h3 id=&#34;pseudo-labeling&#34;&gt;Pseudo-labeling&lt;/h3&gt;
&lt;p&gt;Our pseudo labeling is based on the label guessing technique used in MixMatch. We first take the average of the model&amp;rsquo;s predictions of several weakly augmented versions of the same unlabeled sample as its pseudo label.
As the prediction is averaged from $K$ slight perturbations of the same input instead of $K$ severe perturbation or a single perturbation, the guessed pseudo label should be more stable.
Then, we use the sharpening operation defined in MixMatch to increase the temperature of the label&amp;rsquo;s distribution:
$$\operatorname{Sharpen}(p, T):= \frac{p^{\frac{1}{T}}}{\textbf{1}^\top p^{\frac{1}{T}}}$$&lt;/p&gt;
&lt;p&gt;As the peak of the pseudo label&amp;rsquo;s distribution is &amp;ldquo;sharpened,&amp;rdquo; the network will push this sample further away from the decision boundary.
Additionally, following the practice of MixMatch, we use the exponential moving average of the model at each time step to guess the labels.&lt;/p&gt;
&lt;h3 id=&#34;loss&#34;&gt;Loss&lt;/h3&gt;
&lt;p&gt;Our loss consists of three terms, $\mathcal{L_X}$, $\mathcal{L_U}$, and $\mathcal{L_P}$, representing the supervised loss, the unsupervised loss, and the Pair Loss respectively.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\mathcal{L} &amp;amp;= \mathcal{L_X} + \lambda_{\mathcal{U}} \mathcal{L_U} + \lambda_{\mathcal{P}} \mathcal{L_P} \\&lt;br&gt;
\mathcal{L_X} &amp;amp;= \frac{1}{\left| \mathcal{X}&#39; \right|} \sum_{x,y \in \hat{\mathcal{X}}} H\left(y, \mathrm{p}_{\text{model}}\left(\tilde{y} \mid x ; \theta\right)\right) \\&lt;br&gt;
\mathcal{L_U} &amp;amp;= \frac{
\sum_{u,q \in \hat{\mathcal{U}}}
\mathbb{1}_{\left(\max\left(q\right) &amp;gt; \tau_c\right)}
\left| q - \mathrm{p}_{\text{model}}\left(\tilde{y} \mid u ; \theta\right) \right|^{2}_{2}
}{L \left| \hat{\mathcal{U}} \right|}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;$\mathcal{L_X}$ calculates the cross-entropy of weakly augmented labeled samples, and $\mathcal{L_U}$ represents the $L_2$ distance between strongly augmented samples and their pseudo labels, filtered by high confidence threshold.
Notice that $\mathcal{L_U}$ only enforces the consistency among different perturbations of the same samples but not the consistency among different samples.&lt;/p&gt;
&lt;p&gt;As we aim to exploit the relationship among unlabeled samples, we hereby introduce a novel loss term, Pair Loss, that allows information to propagate implicitly between different unlabeled samples.
In Pair Loss, we use a high confidence pseudo label of an unlabeled point, $p$, as an &amp;ldquo;anchor.&amp;rdquo;
All unlabeled samples whose pseudo labels are similar enough to $p$ need to align their predictions under severe perturbation to the &amp;ldquo;anchor.&amp;rdquo;&lt;/p&gt;














&lt;figure  id=&#34;figure-pair-loss-overview&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Pair Loss Overview.&#34; srcset=&#34;
               /publication/hu-2020-simple/featured_hu4ae5f06fd090dbf90a6b38dbfa89063a_483332_53b1a67d5262e8a228a01d773272c442.png 400w,
               /publication/hu-2020-simple/featured_hu4ae5f06fd090dbf90a6b38dbfa89063a_483332_1eefaad082ad5a03d6424f8da23dde2f.png 760w,
               /publication/hu-2020-simple/featured_hu4ae5f06fd090dbf90a6b38dbfa89063a_483332_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/publication/hu-2020-simple/featured_hu4ae5f06fd090dbf90a6b38dbfa89063a_483332_53b1a67d5262e8a228a01d773272c442.png&#34;
               width=&#34;760&#34;
               height=&#34;590&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Pair Loss Overview.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Figure &lt;a href=&#34;#figure-pair-loss-overview&#34;&gt;3&lt;/a&gt; offers an overview of this selection process. During this process, the similarity threshold &amp;ldquo;extended&amp;rdquo; our confidence threshold in an adaptive manner, as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level.
Formally, we defined the Pair Loss as following:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathcal{L_P} &amp;amp;= \frac{1}{\binom{K&amp;rsquo;B}{2}}
\sum_{
\substack{
i,j \in \left[\left|\mathcal{U}&#39;\right|\right], i \ne j \\&lt;br&gt;
\left(v_l, q_l\right) = \mathcal{U}&#39;_{i} \\&lt;br&gt;
\left(v_r, q_r\right) = \mathcal{U}&#39;_{j}
}
}
\varphi_{\tau_c}\left(\max\left(q_l\right)\right) \\&lt;br&gt;
&amp;amp;\cdot \varphi_{\tau_s}\left(f_{\operatorname{sim}}\left(q_l, q_r\right)\right) \\&lt;br&gt;
&amp;amp;\cdot f_{\operatorname{dist}}\left(q_l, \mathrm{p}_{\text{model}}\left(\tilde{y} \mid v_r ; \theta\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Here, $\tau_c$ and $\tau_s$ denote the confidence threshold and similarity threshold respectively.
$\varphi_t(x)=\mathbb{1}_{\left(x &amp;gt; t\right)} x$ is a hard threshold function controlled by threshold $t$.
$f_{\operatorname{sim}}\left(p, q\right)$ measures the similarity between two probability vectors $p, q$ by &lt;a href=&#34;http://www.jstor.org/stable/25047882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya coefficient&lt;/a&gt;.
The coefficient is bounded between $[0, 1]$, and represents the size of the overlapping portion of the two discrete distributions:&lt;/p&gt;
&lt;p&gt;$$f_{\operatorname{sim}}\left(p, q\right) = \sqrt{p} \cdot \sqrt{q}$$&lt;/p&gt;
&lt;p&gt;$f_{\operatorname{dist}}\left(p, q\right)$ measures the distance between two probability vectors $p, q$.
As $f_{\operatorname{sim}}\left(p, q\right)$ is bounded between $[0, 1]$, we simply choose the distance function to be $f_{\operatorname{dist}}\left(p, q\right) = 1 - f_{\operatorname{sim}}\left(p, q\right)$.&lt;/p&gt;
&lt;p&gt;Although based on analysis, we found that $\cos(\cos^{-1}(\sqrt{\tau_c})+\cos^{-1}(\tau_s))^2$ is the minimal confidence a label need to have for it to be selected by both thresholds, such low confidence label are rarely selected in practice.
Based on empirical evidence, we believe this is caused by the fact a label $p$ that can pass through the high confidence threshold typically has a near one hot distribution.
Thus, for another label $q$ to fall in the similarity threshold of $q$, it must also have relatively high confidence.
Due to this property, the Pair Loss is not very sensitive to the choices of hyperparameters $\tau_s$, $\tau_c$, which we will show empirically in later section.&lt;/p&gt;
&lt;h3 id=&#34;simple-algorithm&#34;&gt;SimPLE Algorithm&lt;/h3&gt;
&lt;p&gt;By putting together all the components introduced in this section, we now present the SimPLE algorithm.
During training, for a mini-batch of samples, SimPLE will first augment both labeled and unlabeled samples with both weak and strong augmentations.
The pseudo labels of the unlabeled samples are obtained by averaging and then sharpening the models&#39; predictions on the weakly augmented unlabeled samples.
Finally, we optimize the three loss terms based on augmented samples and pseudo labels.
During testing, SimPLE uses the exponential moving average of the weights of the model to do prediction, as what is done by MixMatch.
Figure &lt;a href=&#34;#figure-an-overview-of-the-proposed-simple-algorithm&#34;&gt;2&lt;/a&gt; gives an overview of the algorithm, and the complete training algorithm is described in Alg. &lt;a href=&#34;#figure-simple-algorithm&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;














&lt;figure  id=&#34;figure-simple-algorithm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;SimPLE algorithm.&#34; srcset=&#34;
               /publication/hu-2020-simple/images/algorithm1_hu53e17220f872f8c8eb187beb5c84338c_350636_3309cce8a38cf231de7e2708d153799e.png 400w,
               /publication/hu-2020-simple/images/algorithm1_hu53e17220f872f8c8eb187beb5c84338c_350636_44cf32d47947121dc1679bf65e753e49.png 760w,
               /publication/hu-2020-simple/images/algorithm1_hu53e17220f872f8c8eb187beb5c84338c_350636_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/publication/hu-2020-simple/images/algorithm1_hu53e17220f872f8c8eb187beb5c84338c_350636_3309cce8a38cf231de7e2708d153799e.png&#34;
               width=&#34;760&#34;
               height=&#34;430&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SimPLE algorithm.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;The experiment section will be updated soon&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning with Less Labeling (LwLL)</title>
      <link>https://www.zijianhu.com/project/lwll/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/lwll/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;This is an ongoing project. The details are subject to change.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://www.darpa.mil/program/learning-with-less-labeling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DARPA&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In supervised machine learning (ML), the ML system learns by example to recognize things, such as objects in images or speech. Humans provide these examples to ML systems during their training in the form of labeled data. With enough labeled data, we can generally build accurate pattern recognition models.&lt;/p&gt;
&lt;p&gt;The problem is that training accurate models currently requires lots of labeled data. For tasks like machine translation, speech recognition or object recognition, deep neural networks (DNNs) have emerged as the state of the art, due to the superior accuracy they can achieve. To gain this advantage over other techniques, however, DNN models need more data, typically requiring 109 or 1010 labeled training examples to achieve good performance.&lt;/p&gt;
&lt;p&gt;The commercial world has harvested and created large sets of labeled data for training models. These datasets are often created via crowdsourcing: a cheap and efficient way to create labeled data. Unfortunately, crowdsourcing techniques are often not possible for proprietary or sensitive data. Creating data sets for these sorts of problems can result in 100x higher costs and 50x longer time to label.&lt;/p&gt;
&lt;p&gt;To make matters worse, machine learning models are brittle, in that their performance can degrade severely with small changes in their operating environment. For instance, the performance of computer vision systems degrades when data is collected from a new sensor and new collection viewpoints. Similarly, dialog and text understanding systems are very sensitive to changes in formality and register. As a result, additional labels are needed after initial training to adapt these models to new environments and data collection conditions. For many problems, the labeled data required to adapt models to new environments approaches the amount required to train a new model from scratch.&lt;/p&gt;
&lt;p&gt;The Learning with Less Labeling (LwLL) program aims to make the process of training machine learning models more efficient by reducing the amount of labeled data required to build a model by six or more orders of magnitude, and by reducing the amount of data needed to adapt models to new environments to tens to hundreds of labeled examples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;lwll-evaluation&#34;&gt;LwLL Evaluation&lt;/h2&gt;
&lt;p&gt;The evaluation has 3 types of tasks: Image Classification, Object Detection and Machine Translation.
Our team is focusing on the image classification task.
Each task includes a base phase and an adaptation pha where each phase consists of 6 to 8 stages.
For image classification, pre-trained models on predefined whitelisted datasets are allowed.&lt;/p&gt;














&lt;figure  id=&#34;figure-number-of-labels-in-each-stage&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Number of labels in each stage&#34; srcset=&#34;
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_ebe883e776b8bd89780073fbc18e8976.png 400w,
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_4763e4ead4909379d09f8ebd07279ec2.png 760w,
               /project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/labelSizes_hubb74a262980ba22c8035cac02b706536_14330_ebe883e776b8bd89780073fbc18e8976.png&#34;
               width=&#34;760&#34;
               height=&#34;457&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Number of labels in each stage
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The full training set without any label is given at the beginning.
At stage 1, can request 1 label per category in the training set (label budget is 1).
As the training progress, the label budget is increased.&lt;/p&gt;














&lt;figure  id=&#34;figure-lwll-evaluation-model-pipeline&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;LwLL evaluation model pipeline&#34; srcset=&#34;
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_b1b3cf849fba56be982376556309c68c.png 400w,
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_c90e3d93cf702df6a0d4165d4077fecd.png 760w,
               /project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/pipeline_hu9924dad4871f9743a57a641474e56b32_2536040_b1b3cf849fba56be982376556309c68c.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      LwLL evaluation model pipeline
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Our team used a few-shot learning method for the first 2 to 3 checkpoints when the labeled set size is small.
Once sufficient number of labeled data samples become available, the training was handed to our semi-supervised algorithm.&lt;/p&gt;
&lt;h2 id=&#34;simple-similar-pseudo-label-exploitation-for-semi-supervised-classification&#34;&gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/h2&gt;







  
    









  




&lt;div class=&#34;view-list-item&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
  &lt;a href=&#34;https://www.zijianhu.com/publication/hu-2020-simple/&#34; &gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/a&gt;

  

  
  &lt;div class=&#34;article-metadata&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Zhengyu Yang&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Xuefeng Hu&lt;/span&gt;, &lt;span &gt;
      Ram Nevatia&lt;/span&gt;
  &lt;/div&gt;
  

  
  &lt;div class=&#34;btn-links&#34;&gt;
    








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2103.16725&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/hu-2020-simple/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/zijian-hu/SimPLE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;




  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/lwll/&#34;&gt;
    Project
  &lt;/a&gt;
  



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1XUTD1MABhzNa92lbbX5jeT9WupgRdgX4/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Poster
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1e1v0tUvjuN5A8YOntQn1hbxGPLV6Yp5I/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Slides
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1SiXiXSRTSSlUn6_JT64QJajnXrmMpYga/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/CVPR46437.2021.01485&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;



  &lt;/div&gt;
  

&lt;/div&gt;

  
















&lt;figure  id=&#34;figure-simple-algorithm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;SimPLE algorithm&#34; srcset=&#34;
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_0605d2c51801b670c0e6f8f0c76155a7.png 400w,
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_fd8bb7a4688eb35996a473879abb02fe.png 760w,
               /project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/SimPLE_hu7e6c5a87fea892f23597c7d6d500ed86_413333_0605d2c51801b670c0e6f8f0c76155a7.png&#34;
               width=&#34;760&#34;
               height=&#34;265&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      SimPLE algorithm
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We proposed a novel semi-supervised classification algorithm, SimPLE (Figure &lt;a href=&#34;#figure-simple-algorithm&#34;&gt;3&lt;/a&gt;),
that focuses on the less studied relationship between the high confidence unlabeled data that are similar to each other.&lt;/p&gt;














&lt;figure  id=&#34;figure-pair-loss-overview&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Pair Loss Overview.&#34; srcset=&#34;
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_94280aeecc749839c06355a9e4d70c27.png 400w,
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_27c79b729d9183d7453a356cbc09af89.png 760w,
               /project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/lwll/images/pairloss_huef2b7e22861dfbe63d7e9199d7f52e18_879311_94280aeecc749839c06355a9e4d70c27.png&#34;
               width=&#34;760&#34;
               height=&#34;591&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Pair Loss Overview.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-pair-loss-overview&#34;&gt;4&lt;/a&gt;, the new proposed Pair Loss minimizes the statistical distance between high confidence pseudo labels with similarity above a certain threshold.
The similarity threshold &amp;ldquo;extended&amp;rdquo; our confidence threshold in an adaptive manner,
as a sample whose pseudo label confidence is below the threshold can still be selected by the loss and be pushed to a higher confidence level.&lt;/p&gt;
&lt;p&gt;Formally, we defined the Pair Loss as the following:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathcal{L_P} &amp;amp;= \frac{1}{\binom{KB}{2}}
\sum_{\mathcal{U}&#39;}
\mathbb{1}_{\max\left(q_l\right) &amp;gt; \tau_c} \cdot
\mathbb{1}_{f_{\operatorname{sim}}\left(q_l,q_r\right) &amp;gt; \tau_s} \\&lt;br&gt;
&amp;amp;\cdot f_{\operatorname{dist}}\left(q_l, \mathrm{p}_{\text{model}}\left(\tilde{y} \mid v_r ; \theta\right)\right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$K$: number of augmentations&lt;/li&gt;
&lt;li&gt;$B$: batch size&lt;/li&gt;
&lt;li&gt;$\mathcal{U}&#39;$: unlabeled batch&lt;/li&gt;
&lt;li&gt;$\tau_c$: confidence threshold&lt;/li&gt;
&lt;li&gt;$\tau_s$: similarity threshold&lt;/li&gt;
&lt;li&gt;$f_{\operatorname{sim}}\left(\cdot,\cdot\right)$: similarity function
&lt;ul&gt;
&lt;li&gt;We use &lt;a href=&#34;http://www.jstor.org/stable/25047882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya coefficient&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$f_{\operatorname{dist}}\left(\cdot,\cdot\right)$: distance function
&lt;ul&gt;
&lt;li&gt;We use $1-$ &lt;a href=&#34;http://www.jstor.org/stable/25047882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bhattacharyya coefficient&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Dataset&lt;/th&gt;
    &lt;th&gt;Num. Labels&lt;/th&gt;
    &lt;th&gt;Method&lt;/th&gt;
    &lt;th&gt;Backbone&lt;/th&gt;
    &lt;th&gt;Top-1 Accuracy&lt;/th&gt;
  &lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;4&#34;&gt;CIFAR-100&lt;/td&gt;
    &lt;td rowspan=&#34;4&#34;&gt;10000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;71.69%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;ReMixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;76.97%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;FixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-8&lt;/td&gt;
    &lt;td&gt;77.40%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-8&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;78.11%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;Mini-ImageNet&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;4000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;55.47%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;60.50%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-2&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;66.55%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;ImageNet to DomainNet-Real&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;3795&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;ResNet-50&lt;/td&gt;
    &lt;td&gt;35.34%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;ResNet-50&lt;/td&gt;
    &lt;td&gt;35.16%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;ResNet-50&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;50.90%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=&#34;3&#34;&gt;DomainNet-Real to Mini-ImageNet&lt;/td&gt;
    &lt;td rowspan=&#34;3&#34;&gt;4000&lt;/td&gt;
    &lt;td&gt;MixMatch&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;53.39%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MixMatch Enhanced&lt;/td&gt;
    &lt;td&gt;WRN 28-2&lt;/td&gt;
    &lt;td&gt;55.75%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;SimPLE&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;WRN 28-2&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;b&gt;58.73%&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our algorithm, SimPLE, achieved state-of-the-art performance on standard SSL benchmarks and achieved the best accuracy on some tasks in LwLL evaluation. We also evaluated our method in the transfer setting where our algorithm outperforms prior works and supervised baseline by a large margin.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Designed a novel algorithm for semi-supervised classification&lt;/li&gt;
&lt;li&gt;Evaluated our algorithm on standard benchmarks (CIFAR-10, CIFAR-100, SVHN, Mini-ImageNet)&lt;/li&gt;
&lt;li&gt;Evaluated our algorithm in the transfer learning setting (on Mini-ImageNet, DomainNet-Real, AID, RESISC45), where models are initialized models pretrained on ImageNet or DomainNet-Real&lt;/li&gt;
&lt;li&gt;Distributed training with PyTorch Distributed Data Parallel&lt;/li&gt;
&lt;li&gt;GPU accelerated data augmentation with &lt;a href=&#34;https://kornia.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kornia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Zhengyu Yang&lt;/span&gt;&lt;i class=&#34;author-notes fas fa-info-circle&#34; data-toggle=&#34;tooltip&#34; title=&#34;Equal contribution&#34;&gt;&lt;/i&gt;, &lt;span &gt;
      Xuefeng Hu&lt;/span&gt;, &lt;span &gt;
      Ram Nevatia&lt;/span&gt;
  &lt;/span&gt;
  (2021).
  &lt;a href=&#34;https://www.zijianhu.com/publication/hu-2020-simple/&#34;&gt;SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification&lt;/a&gt;.
  &lt;em&gt;2021 Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2103.16725&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/hu-2020-simple/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/zijian-hu/SimPLE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;




  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/lwll/&#34;&gt;
    Project
  &lt;/a&gt;
  



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1XUTD1MABhzNa92lbbX5jeT9WupgRdgX4/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Poster
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1e1v0tUvjuN5A8YOntQn1hbxGPLV6Yp5I/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Slides
&lt;/a&gt;



  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://drive.google.com/file/d/1SiXiXSRTSSlUn6_JT64QJajnXrmMpYga/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/CVPR46437.2021.01485&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>PyTorch: Exponential Moving Average (EMA) Example</title>
      <link>https://www.zijianhu.com/post/pytorch/ema/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/pytorch/ema/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This example carefully replicates the behavior of TensorFlow&amp;rsquo;s &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf.train.ExponentialMovingAverage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Notice that when applying EMA, only the trainable parameters should be changed; for PyTorch, we can get the trainable parameters by &lt;code&gt;model.parameters()&lt;/code&gt; or &lt;code&gt;model.named_parameters()&lt;/code&gt; where &lt;code&gt;model&lt;/code&gt; is a &lt;code&gt;torch.nn.Module&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since my implementation creates a copy of the input model (i.e. &lt;code&gt;shadow&lt;/code&gt;), the buffers needs to be copied to &lt;code&gt;shadow&lt;/code&gt; whenever &lt;code&gt;update()&lt;/code&gt; is invoked.&lt;/p&gt;
&lt;h3 id=&#34;alternative-implementation&#34;&gt;Alternative Implementation&lt;/h3&gt;
&lt;p&gt;You could implement &lt;code&gt;shadow&lt;/code&gt; as a &lt;code&gt;dict&lt;/code&gt;, for detail of this version see &lt;a href=&#34;https://fyubang.com/2019/06/01/ema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现&lt;/a&gt;. One problem with that implementation is that shadow needs to be manually saved since shadow parameters are not stored in &lt;code&gt;state_dict&lt;/code&gt;; a simple fix to this problem is to register all shadow parameters by calling &lt;code&gt;register_parameter(&amp;lt;parameter name&amp;gt;)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
from torch import nn

from copy import deepcopy
from collections import OrderedDict
from sys import stderr

# for type hint
from torch import Tensor


class EMA(nn.Module):
    def __init__(self, model: nn.Module, decay: float):
        super().__init__()
        self.decay = decay

        self.model = model
        self.shadow = deepcopy(self.model)

        for param in self.shadow.parameters():
            param.detach_()

    @torch.no_grad()
    def update(self):
        if not self.training:
            print(&amp;quot;EMA update should only be called during training&amp;quot;, file=stderr, flush=True)
            return

        model_params = OrderedDict(self.model.named_parameters())
        shadow_params = OrderedDict(self.shadow.named_parameters())

        # check if both model contains the same set of keys
        assert model_params.keys() == shadow_params.keys()

        for name, param in model_params.items():
            # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
            # shadow_variable -= (1 - decay) * (shadow_variable - variable)
            shadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))

        model_buffers = OrderedDict(self.model.named_buffers())
        shadow_buffers = OrderedDict(self.shadow.named_buffers())

        # check if both model contains the same set of keys
        assert model_buffers.keys() == shadow_buffers.keys()

        for name, buffer in model_buffers.items():
            # buffers are copied
            shadow_buffers[name].copy_(buffer)

    def forward(self, inputs: Tensor, return_feature: bool = False) -&amp;gt; Tensor:
        if self.training:
            return self.model(inputs, return_feature)
        else:
            return self.shadow(inputs, return_feature)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf.train.ExponentialMovingAverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fyubang.com/2019/06/01/ema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Can I Trust You? A User Study of Robot Mediation of a Support Group</title>
      <link>https://www.zijianhu.com/publication/birmingham-icra-2020/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/birmingham-icra-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust in Multi-Party Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/project/multi_party/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/multi_party/</guid>
      <description>&lt;p&gt;In this project, we designed and evaluated a novel framework for robot mediation of a support group.
We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (&lt;a href=&#34;https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wizard-of-Oz&lt;/a&gt;).
At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings.
In a &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;second-author paper&lt;/a&gt; at International Conference on Robotics and Automation (ICRA),
we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=multiparty_support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;














&lt;figure  id=&#34;figure-volunteers-demonstrating-the-study-setup&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Volunteers demonstrating the study setup&#34; srcset=&#34;
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_7f42a7f4bbd37dd6ebeadffc500591fc.jpg 400w,
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_74074270f216fcf6a73d06effbe6a739.jpg 760w,
               /project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_7f42a7f4bbd37dd6ebeadffc500591fc.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Volunteers demonstrating the study setup
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In each study session, three participants were seated around the end of a table with a seated
&lt;a href=&#34;https://www.softbankrobotics.com/emea/index.php/en/nao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAO&lt;/a&gt; robot as shown in
Figure &lt;a href=&#34;#figure-volunteers-demonstrating-the-study-setup&#34;&gt;1&lt;/a&gt;.
The NAO robot, acting as a group moderator, was positioned towards the participants.
On the table, a 360-degree microphone and 3 cameras facing directly to the participants&#39; face were placed.
Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members.
The robot operator was seated behind a one-way mirror hidden from participants.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensitivity&lt;/th&gt;
&lt;th&gt;Question&lt;/th&gt;
&lt;th&gt;Disclosure&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;td&gt;What do you like about school?&lt;/td&gt;
&lt;td&gt;When I feel stressed, I think my circuits might overload.&lt;br&gt;Does anyone else feel the same way?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;What are some of the hardest parts of school for you?&lt;/td&gt;
&lt;td&gt;Sometimes I worry I am inadequate for this school.&lt;br&gt;Does anyone else sometimes feel that too?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hard&lt;/td&gt;
&lt;td&gt;What will happen if you don’t succeed in school?&lt;/td&gt;
&lt;td&gt;Sometimes I worry about if I belong here. &lt;br&gt;Does anyone else feel the same way?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;During the interaction, the robot can ask questions or make disclosures.
A total of 16 questions and 6 disclosures are available.
On average, 12 questions and 3 disclosures were made by the robot in each session.&lt;/p&gt;
&lt;p&gt;The questions and disclosures are grouped into low, medium, and high sensitivity as illustrated in the below table.&lt;/p&gt;














&lt;figure  id=&#34;figure-self-annotation-at-the-end-of-each-session&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Self-annotation at the end of each session&#34; srcset=&#34;
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_f185a0b114e2a2f4b71d1dc670ef2d1e.jpg 400w,
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_18a3e6c8642b06fa0a6b1be14de65482.jpg 760w,
               /project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/selfAnnotation_hu018bd18db52de5e1bf3466d495c63534_3322063_f185a0b114e2a2f4b71d1dc670ef2d1e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Self-annotation at the end of each session
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-self-annotation-ui&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Self-annotation UI&#34; srcset=&#34;
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_0eaed455e52637024578e82b913c3f0d.jpeg 400w,
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_47efa0fb9bb1d5086cd36d0a59ab7fc3.jpeg 760w,
               /project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_1200x1200_fit_q75_lanczos.jpeg 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/selfAnnotationInterface_hu01a33997c9fd23d1255e85aa1fdfd3e0_226197_0eaed455e52637024578e82b913c3f0d.jpeg&#34;
               width=&#34;760&#34;
               height=&#34;374&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Self-annotation UI
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.&lt;/p&gt;
&lt;p&gt;The detail for the procedure of the study can be found &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;














&lt;figure  id=&#34;figure-control-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Control architecture&#34; srcset=&#34;
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_ea79ef16c416ef42845a6e21c1a777b3.png 400w,
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_a7aa5f0207f9c10c827af96ade1ef0b2.png 760w,
               /project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://www.zijianhu.com/project/multi_party/images/controlArchitecture_hub5ada478ae36be9d27c03f6260a933bc_467110_ea79ef16c416ef42845a6e21c1a777b3.png&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Control architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As shown in Figure &lt;a href=&#34;#figure-control-architecture&#34;&gt;3&lt;/a&gt;.
The wizard controls the robot through the Wizard-of-Oz web interface.&lt;/p&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed NAO control program&lt;/li&gt;
&lt;li&gt;Designed and implemented web-based Wizard of Oz controller&lt;/li&gt;
&lt;li&gt;Designed and implemented self-annotation website&lt;/li&gt;
&lt;li&gt;Developed data collection program for one depth camera, three webcams and one 360 degree microphone&lt;/li&gt;
&lt;li&gt;Data post-processing for data whitening and fast data loading&lt;/li&gt;
&lt;li&gt;Turn-taking prediction with &lt;a href=&#34;http://zpascal.net/cvpr2017/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporal Convolutional Networks (TCN)&lt;/a&gt; and LSTM for multi-modal input&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Chris Birmingham&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Kartik Mahajan&lt;/span&gt;, &lt;span &gt;
      Eli Reber&lt;/span&gt;, &lt;span &gt;
      Maja J. Matarić&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;Can I Trust You? A User Study of Robot Mediation of a Support Group&lt;/a&gt;.
  &lt;em&gt;2020 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.04671&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/birmingham-icra-2020/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/multi_party/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/ICRA40945.2020.9196875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</guid>
      <description>&lt;h2 id=&#34;linked-material&#34;&gt;&lt;strong&gt;Linked material&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;YouTube video for &amp;ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&amp;rdquo;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Telepresence Robot for K-12 Remote Education</title>
      <link>https://www.zijianhu.com/project/nri_kids/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/nri_kids/</guid>
      <description>&lt;p&gt;In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=nrikids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;We used an &lt;a href=&#34;https://ohmnilabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohmni&lt;/a&gt; robot equipped with an arm and a Linux PC.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions consist of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed customized web user interface for robot arm control&lt;/li&gt;
&lt;li&gt;Designed and implemented communication protocol and software between the Linux PC and the Ohmni server&lt;/li&gt;
&lt;li&gt;Developed user interface with a turning dial for robot arm control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;
&lt;p&gt;






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Naomi T. Fitter&lt;/span&gt;, &lt;span &gt;
      Youngseok Joung&lt;/span&gt;, &lt;span &gt;
      Marton Demeter&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Maja J. Matarić&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-hardware/&#34;&gt;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://robotics.usc.edu/publications/media/uploads/pubs/pubdb_1048_87866f1437ff45e99c4fc89e3f9f45be.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-hardware/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  







  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://youtu.be/Ft8XCAIbslE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  









  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      Naomi T. Fitter&lt;/span&gt;, &lt;span &gt;
      Youngseok Joung&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;
      Zijian Hu&lt;/span&gt;, &lt;span &gt;
      Marton Demeter&lt;/span&gt;, &lt;span &gt;
      Maja J. Matarić&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-ui/&#34;&gt;User Interface Tradeoffs for Remote Deictic Gesturing&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://robotics.usc.edu/publications/media/uploads/pubs/pubdb_1049_35632024bfb044c7ba2d4e9cf53e560e.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-ui/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;





  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>User Interface Tradeoffs for Remote Deictic Gesturing</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;https://uscinteractionlab.web.app/project?id=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://www.zijianhu.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://www.zijianhu.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Getting Started with NAO &#43; ROS</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/getting-started/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/getting-started/</guid>
      <description>&lt;p&gt;Make sure you have installed all the dependencies and configured &lt;strong&gt;PYTHONPATH&lt;/strong&gt; system variable correctly&lt;/p&gt;
&lt;p&gt;*&lt;em&gt;see &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/installation&#34;&gt;installation guide&lt;/a&gt; for detail&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;controlling-robot&#34;&gt;Controlling Robot&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Turn on the robot. See &lt;a href=&#34;http://doc.aldebaran.com/2-1/nao/getting_out_of_the_box.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt; for detail&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the robot bridge on your computer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ roslaunch nao_bringup nao_full_py.launch nao_ip:=&amp;lt;robot_ip&amp;gt; \
roscore_ip:=&amp;lt;roscore_ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start the robot&amp;rsquo;s default configuration with the following publisher:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;joint_states&lt;/li&gt;
&lt;li&gt;tf&lt;/li&gt;
&lt;li&gt;top camera&lt;/li&gt;
&lt;li&gt;bottom camera&lt;/li&gt;
&lt;li&gt;left sonar&lt;/li&gt;
&lt;li&gt;right sonar&lt;/li&gt;
&lt;li&gt;microphone&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To visualize the robot, open &lt;strong&gt;rviz&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosrun rviz rviz
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;In top bar, go to &lt;code&gt;File-&amp;gt;Open Config&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;navigate to &lt;code&gt;&amp;lt;your catkin workspace&amp;gt;/src/nao_robot/nao_description/config&lt;/code&gt; and open the file with &lt;strong&gt;.rviz&lt;/strong&gt; extension
&lt;ul&gt;
&lt;li&gt;make sure you have &lt;a href=&#34;http://wiki.ros.org/nao_meshes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_meshes&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you should see something similar to the below screenshot
&lt;img src=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started?action=AttachFile&amp;amp;do=get&amp;amp;target=NaoRviz.png&#34; alt=&#34;NAO rviz&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Controlling the robot&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;execute &lt;code&gt;rosnode list&lt;/code&gt; to check if &lt;strong&gt;/nao_walker&lt;/strong&gt; node is running&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To turn on the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/enable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To turn off the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/disable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;once the motors are on, use the following command to move the robot in x-direction&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 1.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To stop the robot, run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 0.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;next-naoqi-sdk-guidepostnao-tutorialnao-sdk&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/nao-sdk&#34;&gt;NAOqi SDK Guide&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting started with ROS for Nao, including NAOqi and rviz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao_bringup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_bringup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Installation</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/installation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/installation/</guid>
      <description>&lt;h2 id=&#34;installing-the-sdk&#34;&gt;Installing the SDK&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make sure you have the following installed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/download/releases/2.7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cmake.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMake&lt;/a&gt; version 2.8.3 or higher&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the following from &lt;a href=&#34;https://community.aldebaran.com/en/resources/software&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aldebaran Community&lt;/a&gt; website (&lt;em&gt;you need to register an account in order to download the files&lt;/em&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;naoqi-sdk-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;[optional]&lt;/em&gt;&lt;/strong&gt; choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command and replace &lt;strong&gt;2.1.2.x&lt;/strong&gt; with the version you downloaded&lt;/p&gt;
&lt;p&gt;Unzip the tar files&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir ~/naoqi
$ tar xzf &amp;lt;path to NAOqi C++ SDK&amp;gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64
$ tar xzf &amp;lt;path to NAOqi Python SDK&amp;gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the installation by executing NAOqi&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see output similiar to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Starting NAOqi version 2.1.2.17
.
.
.
NAOqi is ready...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;strong&gt;CTRL-C&lt;/strong&gt; to exit&lt;/p&gt;
&lt;p&gt;Now we need to add &lt;strong&gt;NAOqi SDK&lt;/strong&gt; to system variables. Add the following lines at the end of &lt;strong&gt;~/.bashrc&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH
$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64
$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Execute &lt;code&gt;source ~/.bashrc&lt;/code&gt; to apply the changes&lt;/p&gt;
&lt;p&gt;Verify &lt;em&gt;in python console&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if correctly installed, there should be no error&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-ros&#34;&gt;Install ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://wiki.ros.org/kinetic/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Official ROS Installation Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-nao-package-for-ros&#34;&gt;Install &lt;strong&gt;NAO package&lt;/strong&gt; for ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install the packages needed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;replace kinetic to your ROS version if needed&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \
ros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \
ros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \
ros-kinetic-camera-info-manager-py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the main package with &lt;code&gt;sudo apt-get install ros-kinetic-nao-robot&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for robot control&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \
ros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \
ros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \
ros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \
ros-kinetic-moveit
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for simulation&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Notice: to install nao_meshes package, you need to agree the policy&lt;/em&gt;
&lt;code&gt;sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;next-getting-startedpostnao-tutorialgetting-started&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/getting-started/&#34;&gt;Getting Started&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/python/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python SDK Install Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/cpp/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C++ SDK Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Installation of ROS for usage with or on a NAO robot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: NAOqi SDK</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0700</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</guid>
      <description>&lt;h2 id=&#34;before-starting&#34;&gt;Before starting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you have &lt;strong&gt;Choregraphe suite&lt;/strong&gt; installed
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt; for detail&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-choregraphe&#34;&gt;Using Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python-in-choregraphe&#34;&gt;Using Python in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_script.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-dialog-topic-in-choregraphe&#34;&gt;Using Dialog topic in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_dialog.htmll&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python&#34;&gt;Using Python&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_python.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choregraphe Suite Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hello World 1 - using Choregraphe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.zijianhu.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.zijianhu.com/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
