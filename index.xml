<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zijian Hu</title>
    <link>https://www.zijianhu.com/</link>
      <atom:link href="https://www.zijianhu.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Zijian Hu</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 23 Jul 2020 00:00:00 -0800</lastBuildDate>
    <image>
      <url>https://www.zijianhu.com/images/icon_hub8aaec6c2c2e1082c2b24756ea87d425_171065_512x512_fill_lanczos_center_2.png</url>
      <title>Zijian Hu</title>
      <link>https://www.zijianhu.com/</link>
    </image>
    
    <item>
      <title>PyTorch: Exponential Moving Average (EMA) Example</title>
      <link>https://www.zijianhu.com/post/pytorch/ema/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/post/pytorch/ema/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This example carefully replicates the behavior of TensorFlow&amp;rsquo;s &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf.train.ExponentialMovingAverage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Notice that when applying EMA, only the trainable parameters should be changed; for PyTorch, we can get the trainable parameters by &lt;code&gt;model.parameters()&lt;/code&gt; or &lt;code&gt;model.named_parameters()&lt;/code&gt; where &lt;code&gt;model&lt;/code&gt; is a &lt;code&gt;torch.nn.Module&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since my implementation creates a copy of the input model (i.e. &lt;code&gt;shadow&lt;/code&gt;), the buffers needs to be copied to &lt;code&gt;shadow&lt;/code&gt; whenever &lt;code&gt;update()&lt;/code&gt; is invoked.&lt;/p&gt;
&lt;h3 id=&#34;alternative-implementation&#34;&gt;Alternative Implementation&lt;/h3&gt;
&lt;p&gt;You could implement &lt;code&gt;shadow&lt;/code&gt; as a &lt;code&gt;dict&lt;/code&gt;, for detail of this version see &lt;a href=&#34;https://fyubang.com/2019/06/01/ema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现&lt;/a&gt;. One problem with that implementation is that shadow needs to be manually saved since shadow parameters are not stored in &lt;code&gt;state_dict&lt;/code&gt;; a simple fix to this problem is to register all shadow parameters by calling &lt;code&gt;register_parameter(&amp;lt;parameter name&amp;gt;)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
from torch import nn

from copy import deepcopy
from collections import OrderedDict
from sys import stderr

# for type hint
from torch import Tensor


class EMA(nn.Module):
    def __init__(self, model: nn.Module, decay: float):
        super().__init__()
        self.decay = decay

        self.model = model
        self.shadow = deepcopy(self.model)

        for param in self.shadow.parameters():
            param.detach_()

    @torch.no_grad()
    def update(self):
        if not self.training:
            print(&amp;quot;EMA update should only be called during training&amp;quot;, file=stderr, flush=True)
            return

        model_params = OrderedDict(self.model.named_parameters())
        shadow_params = OrderedDict(self.shadow.named_parameters())

        # check if both model contains the same set of keys
        assert model_params.keys() == shadow_params.keys()

        for name, param in model_params.items():
            # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
            # shadow_variable -= (1 - decay) * (shadow_variable - variable)
            shadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))

        model_buffers = OrderedDict(self.model.named_buffers())
        shadow_buffers = OrderedDict(self.shadow.named_buffers())

        # check if both model contains the same set of keys
        assert model_buffers.keys() == shadow_buffers.keys()

        for name, buffer in model_buffers.items():
            # buffers are copied
            shadow_buffers[name].copy_(buffer)

    def forward(self, inputs: Tensor, return_feature: bool = False) -&amp;gt; Tensor:
        if self.training:
            return self.model(inputs, return_feature)
        else:
            return self.shadow(inputs, return_feature)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf.train.ExponentialMovingAverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fyubang.com/2019/06/01/ema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Can I Trust You? A User Study of Robot Mediation of a Support Group</title>
      <link>https://www.zijianhu.com/publication/birmingham-icra-2020/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/birmingham-icra-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trust in Multi-Party Human-Robot Interaction</title>
      <link>https://www.zijianhu.com/project/multi_party/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/multi_party/</guid>
      <description>&lt;p&gt;In this project, we designed and evaluated a novel framework for robot mediation of a support group.
We conducted a user study using an NAO robot mediator controlled by a human operator that is unseen by the participants (&lt;a href=&#34;https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wizard-of-Oz&lt;/a&gt;).
At the end of each study, the participants are asked to annotate their trust towards other participants in the study session recordings.
In a &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;second-author paper&lt;/a&gt; at International Conference on Robotics and Automation (ICRA),
we showed that using a robot could significantly increase the average interpersonal trust after the group interaction session.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;http://robotics.usc.edu/interaction/projects/desc2.php?name=multiparty_support&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;study-design&#34;&gt;Study Design&lt;/h2&gt;






  



  
  











&lt;figure id=&#34;figure-volunteers-demonstrating-the-study-setup&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; data-caption=&#34;Volunteers demonstrating the study setup&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/multi_party/images/groupSession_huae0fa70effc1edf77d903f67a0da2f5d_4231037_2000x2000_fit_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;4032&#34; height=&#34;3024&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Volunteers demonstrating the study setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In each study session, three participants were seated around the end of a table with a seated &lt;a href=&#34;https://www.softbankrobotics.com/emea/index.php/en/nao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAO&lt;/a&gt; robot as shown in Figure 1.
The NAO robot, acting as a group moderator, was positioned towards the participants.
On the table, a 360-degree microphone and 3 cameras facing directly to the participants&#39; face were placed.
Behind the robot, an RGB-D camera was mounted on a tripod to record the interactions between the group members.
The robot operator was seated behind a one-way mirror hidden from participants.&lt;/p&gt;
&lt;p&gt;To measure how the level of trust changes overtime, the participants were asked to report their trust towards other participants against the recordings of the current session after the group interaction.&lt;/p&gt;
&lt;p&gt;The detail for the procedure of the study can be found &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed NAO control program&lt;/li&gt;
&lt;li&gt;Designed and implemented web-based Wizard of Oz controller&lt;/li&gt;
&lt;li&gt;Designed and implemented self-annotation website&lt;/li&gt;
&lt;li&gt;Developed data collection program for one depth camera, three webcams and one 360 degree microphone&lt;/li&gt;
&lt;li&gt;Data post-processing for data whitening and fast data loading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;







  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/chris-birmingham/&#34;&gt;Chris Birmingham&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/kartik-mahajan/&#34;&gt;Kartik Mahajan&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/eli-reber/&#34;&gt;Eli Reber&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://www.zijianhu.com/publication/birmingham-icra-2020/&#34;&gt;Can I Trust You? A User Study of Robot Mediation of a Support Group&lt;/a&gt;.
  &lt;em&gt;2020 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.04671&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/birmingham-icra-2020/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/multi_party/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/ICRA40945.2020.9196875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


</description>
    </item>
    
    <item>
      <title>Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-hardware/</guid>
      <description>&lt;h2 id=&#34;linked-material&#34;&gt;&lt;strong&gt;Linked material&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;YouTube video for &amp;ldquo;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&amp;rdquo;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Socially Aware, Expressive, and Personalized Mobile Remote Presence: Co-Robots as Gateways to Access to K-12 In-School Education</title>
      <link>https://www.zijianhu.com/project/nri_kids/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/nri_kids/</guid>
      <description>&lt;p&gt;In this project, we developed and evaluated various control methods and interfaces for mobile remote presence robots (MRP) for remote K-12 education. In the two papers published at the International Symposium on Robot and Human Interactive Communication (RO-MAN), we conducted a user study and evaluated our system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;http://robotics.usc.edu/interaction/sponsors/desc.php?name=nrikids&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Within the field of Human-Robot Interaction (HRI), a growing subfield is forming that focuses
specifically on interactions between one or more robots and multiple people, known as Multi-Party
Human-Robot Interaction (MP-HRI). MP-HRI encompasses the challenges of single-user HRI
(interaction dynamics, human perception, etc.) and extends them to the challenges of multi-party
interactions (within-group turn taking, dyadic dynamics, and group dynamics).&lt;/p&gt;
&lt;p&gt;To address these, MP-HRI requires new methods and approaches. Effective MP-HRI enables robotic systems
to function in many contexts, including service, support, and mediation. In realistic human contexts,
service and support robots need to work with varying numbers of individuals, particularly when working
within team structures. In mediation, robotic systems must by definition, be able to work with multiple
parties. These contexts often overlap, and algorithms that work in one context can benifit work in another.&lt;/p&gt;
&lt;p&gt;This project will advance the basic research in trust and influence in MP-HRI contexts. This will involve
exploring how robots and people establish, maintain, and repair trust in MP-HRI. Specifically, this research
will examine robot group mediation for group conseling, with extensions to team performance in robot
service and support teams.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;
&lt;p&gt;We used an &lt;a href=&#34;https://ohmnilabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohmni&lt;/a&gt; robot equipped with an arm and a Linux PC.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ft8XCAIbslE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions consist of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developed customized web user interface for robot arm control&lt;/li&gt;
&lt;li&gt;Designed and implemented communication protocol and software between the Linux PC and the Ohmni server&lt;/li&gt;
&lt;li&gt;Developed user interface with a turning dial for robot arm control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-publications&#34;&gt;Related Publications&lt;/h2&gt;
&lt;p&gt;






  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-hardware/&#34;&gt;Design and Evaluation of Expressive Turn-Taking Hardware for a Telepresence Robot&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1048/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-hardware/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  







  
  
    
  
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://youtu.be/Ft8XCAIbslE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Video
&lt;/a&gt;



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956413&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  









  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/naomi-t.-fitter/&#34;&gt;Naomi T. Fitter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/youngseok-joung/&#34;&gt;Youngseok Joung&lt;/a&gt;&lt;/span&gt;, &lt;span class=&#34;author-highlighted&#34;&gt;&lt;a href=&#34;https://www.zijianhu.com/author/zijian-hu/&#34;&gt;Zijian Hu&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/marton-demeter/&#34;&gt;Marton Demeter&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://www.zijianhu.com/author/maja-j.-mataric/&#34;&gt;Maja J. Matarić&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://www.zijianhu.com/publication/fitter-roman-2019-ui/&#34;&gt;User Interface Tradeoffs for Remote Deictic Gesturing&lt;/a&gt;.
  &lt;em&gt;2019 IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;http://robotics.usc.edu/publications/1049/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fitter-roman-2019-ui/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;





  
  &lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.zijianhu.com/project/nri_kids/&#34;&gt;
    Project
  &lt;/a&gt;
  









&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1109/RO-MAN46459.2019.8956354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>User Interface Tradeoffs for Remote Deictic Gesturing</title>
      <link>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/publication/fitter-roman-2019-ui/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decentralized Federated Multi-Task Learning and System Design</title>
      <link>https://www.zijianhu.com/project/dpa_sgd/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/dpa_sgd/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;This page contains a copy of the final report of our final project for Spring 2019 CSCI-599: Deep Learning and its Applications at the University of Southern California. The &lt;a href=&#34;http://fl.chaoyanghe.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original page&lt;/a&gt; was removed&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#problem-formulation&#34;&gt;Problem formulation&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#statistical-challenges&#34;&gt;Statistical Challenges&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#system-challenges&#34;&gt;System Challenges&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#general-definition-of-federated-learning&#34;&gt;General Definition of Federated Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#general-framework-of-federated-multi-task-learning&#34;&gt;General Framework of Federated Multi-Task Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#federated-multi-task-deep-learning-framework&#34;&gt;Federated Multi-Task Deep Learning Framework&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#approach&#34;&gt;Approach&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#decentralized-periodic-averaging-sgd&#34;&gt;Decentralized Periodic Averaging SGD&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#algorithm&#34;&gt;Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#advantages-of-our-algorithm&#34;&gt;Advantages of Our Algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#system-design&#34;&gt;System Design&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comparison-to-previous-works&#34;&gt;Comparison to Previous Works&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#federated-multi-task-learning&#34;&gt;Federated Multi-Task Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#stochastic-gradient-decent-optimization&#34;&gt;Stochastic Gradient Decent Optimization&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#appendix-i-convergence-analysis&#34;&gt;Appendix I: Convergence Analysis&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#appendix-ii-poster&#34;&gt;Appendix II: Poster&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;p&gt;In the cloud-based environment, distributed deep learning systems such as Tensorflow and PyTorch is widespread in various domains, but they are typically built based on the parameter server, which
requires data integrated into one place (a server or a cluster) to train a model[Dean &lt;em&gt;et al.&lt;/em&gt;; Li &lt;em&gt;et al.&lt;/em&gt;, 2014; Cui &lt;em&gt;et al.&lt;/em&gt;, 2014; Abadi &lt;em&gt;et al.&lt;/em&gt;, 2016; Paszke &lt;em&gt;et al.&lt;/em&gt;, 2017], as shown in Figure 1(a). Increasing
practical constraints lead this data integration difficult or impossible, including data privacy and confidentiality, intellectual property protection, and law constraints. A promising solution to these
problems is called Federated Learning [McMahan &lt;em&gt;et al.&lt;/em&gt;, 2016]. As shown in Figure 1(b), Federated Learning enables collaboratively training on edge devices or edge data centers through exchanging parameters or gradients without centralizing their scattered data (In this work, we use both worker and node to represent edge devices or edge data centers). Representative examples happen in various domains such as Mobile Internet, health, and finance domains [McMahan and Ramage, 2017, Liu &lt;em&gt;et al.&lt;/em&gt;, 2018a, Liu &lt;em&gt;et al.&lt;/em&gt;, 2018b].&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-federated-multi-task-learning-topology-a-cloud-based-distributed-learning-b-centralized-federated-learning-c-decentralized-federated-learning-d-centralized-communication-topology-with-decentralized-parameter-exchanging-topology&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/introduction_hu3ada94327916f25607af6cbc5bb02ef8_300845_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Federated Multi-Task Learning Topology. (a) Cloud-Based Distributed Learning; (b) Centralized Federated Learning; (c) Decentralized Federated Learning; (d) Centralized Communication Topology with Decentralized Parameter Exchanging Topology.&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/introduction_hu3ada94327916f25607af6cbc5bb02ef8_300845_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1775&#34; height=&#34;1623&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Federated Multi-Task Learning Topology. (a) Cloud-Based Distributed Learning; (b) Centralized Federated Learning; (c) Decentralized Federated Learning; (d) Centralized Communication Topology with Decentralized Parameter Exchanging Topology.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;problem-formulation&#34;&gt;Problem formulation&lt;/h2&gt;
&lt;h3 id=&#34;statistical-challenges&#34;&gt;Statistical Challenges&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Non-IID&lt;/strong&gt;: Each worker generates data in a non-i.i.d. (independent and identically distributed) manner with a distinct statistical distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unbalanced Local Data&lt;/strong&gt;: Workers have different quantity of data sample due to their different behaviors.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two characteristics bring challenges to learning a high-performance personalized model for each worker.&lt;/p&gt;
&lt;h3 id=&#34;system-challenges&#34;&gt;System Challenges&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Larger Worker Number&lt;/strong&gt;: The worker number is typically larger than cloudbased learning. The larger number will pose a higher communication cost and difficult communication
synchronization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heterogeneous Networks&lt;/strong&gt;: Each worker may differ in communication capacity due to heterogeneous networks(4G, WiFi, and other IoT protocol).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heterogeneous Computation&lt;/strong&gt;: Computational
capacities of each worker may differ due to variability in hardware(CPU, memory).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three characteristics make communication cost and low-speed training become a major bottleneck towards real-world deployment.&lt;/p&gt;
&lt;p&gt;In practice, scattered data owners also demand personalized models rather than a global model for all owners. They hope to not only get help from other owners’ data to train a high accuracy model but also to gain their personalized models which can represent their unique data properties. Thus, to simultaneously address statistical and system challenges is the primary research direction of federated learning.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-figure-2-federated-multi-task-deep-learning-framework&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/decentralizedFramework_hu58784681c8646995d497d2752388e320_131151_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Figure 2: Federated Multi-Task Deep Learning Framework&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/decentralizedFramework_hu58784681c8646995d497d2752388e320_131151_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;3006&#34; height=&#34;1017&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Figure 2: Federated Multi-Task Deep Learning Framework
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;general-definition-of-federated-learning&#34;&gt;General Definition of Federated Learning&lt;/h3&gt;
&lt;p&gt;Following the first federated learning paper McMahan &lt;em&gt;et al.&lt;/em&gt; [2016], we define the objective function for the federated setting as&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
F(\mathbf{w}) &amp;amp;= \sum_{k=1}^{K} \frac{n_k}{N} F_{k}(\mathbf{w}) \\&lt;br&gt;
&amp;amp;= \sum_{k=1}^{K} \frac{n_k}{N} \frac{1}{n_k} \sum_{i \in \mathcal{P}_{k}} l(\mathbf{x}_{i}, \mathbf{y}_{i}; \mathbf{w})
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $ l(\mathbf{x}_{i}, \mathbf{y}_{i}; \mathbf{w}) $ is the loss function of the prediction on example $(\mathbf{x}_{i}, \mathbf{y}_{i})$ made with model parameters $\mathbf{w}$, K is the total learning nodes number, $\mathcal{P}_{k}$ is the set of indexes of data points on node k, $n_k = |P_k|$, and $\sum_{k=1}^{K} n_{k} = N$. This objective function can capture the different quantity of samples and statistical distribution of K nodes. Here, different nodes learn the global model jointly, which showing as the same loss function $l$ and parameters $\mathbf{w}$.&lt;/p&gt;
&lt;h3 id=&#34;general-framework-of-federated-multi-task-learning&#34;&gt;General Framework of Federated Multi-Task Learning&lt;/h3&gt;
&lt;p&gt;As mentioned in the introduction, federated multi-task learning is a framework that can improve the performance by directly capturing the relationships among unbalanced data in multiple devices, which implies that it can address the statistical challenges in federated learning. The general formulation for federated multi-task learning is:&lt;/p&gt;
&lt;p&gt;$$
\min_{\mathbf{W}}\sum_{k=1}^{K}\frac{1}{n_k}\sum_{i=1}^{n_k}l_i(\mathbf{x}_i,\mathbf{y}_i;\mathbf{w}_k) + \mathcal{R}(\mathbf{W},\mathbf{\Omega}).
$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{W} = (\mathbf{w}_1,\mathbf{w}_2,&amp;hellip;,\mathbf{w}_K) \in \mathbb{R}^{d \times{m}}$ is the parameters for different tasks and $\mathcal{R}(\mathbf{W},\mathbf{\Omega})$ is the regularization. Different multi-task framework is mainly different from the regularization term $\mathcal{R}$.&lt;/p&gt;
&lt;p&gt;The first term of the objective models the summation of different empirical loss of each node. The second term serves as a task-relationship regularizer with $\mathbf{\Omega}\in\mathbb{R}^{K\times{K}}$ being the covariance matrix Zhang and Yeung [2012].&lt;/p&gt;
&lt;p&gt;The covariance matrix is able to describe positive, negative and unrelated correlation between nodes, which can either known as priori or being measured while learning the models simultaneously.&lt;/p&gt;
&lt;p&gt;Each element $\mathbf{\Omega}_{i,j}$ is a value that indicates the similarity between two nodes. Here we use a bio-convex formulation in Zhang and Yeung [2012], which is a general case for other regularization methods,&lt;/p&gt;
&lt;p&gt;$$
\mathcal{R}(\mathbf{W},\mathbf{\Omega}) = \lambda_ 1tr(\mathbf{W}\mathbf{\Omega}^{-1}\mathbf{W}^{T}) + \lambda_2||\mathbf{W}||_F^2.
$$&lt;/p&gt;
&lt;p&gt;where we constrain $\vec{W}$ with covariance matrix $\mathbf{\Omega}^{-1}$ through matrix trace $tr(\mathbf{W}\mathbf{\Omega}^{-1}\mathbf{W}^{T})$. This means the closer $\mathbf{w}_i$ and $\mathbf{w}_j$ is, the larger the $\mathbf{\Omega}_{i,j}$ will be. Specifically if $\mathbf{\Omega}$ is an identity matrix, then each node is independent to each other.
Smith &lt;em&gt;et al.&lt;/em&gt; [2017] proposed MOCHA based on the above multi-task learning framework. However, MOCHA can only handle convex functions in federated multi-task learning settings, which can not be generated to non-convex deep learning models. Our work generates federated multi-task learning framework to the non-convex DNN setting.&lt;/p&gt;
&lt;h3 id=&#34;federated-multi-task-deep-learning-framework&#34;&gt;Federated Multi-Task Deep Learning Framework&lt;/h3&gt;
&lt;p&gt;DNNs are able to extract deep features from raw data. However, to the best of our knowledge, DNNs has not been applied to federated multi-task problems. We thus consider DNNs as our feature transformation function and make prediction based on the hidden features. Formally speaking, the formulation can be defined as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \begin{aligned} \min_{\mathbf{\theta}, \vec{U},\vec{W},\mathbf{\Omega}}
&amp;amp;\sum_{k=1}^{K}\frac{1}{n_k}\bigg[\bigg.\sum_{i=1}^{n_k}l(f(\vec{x}_i^{k},\mathbf{\theta}_k,\vec{U}_k,\vec{w}_k),\vec{y}_i^k) \\&lt;br&gt;
&amp;amp;+ \frac{1}{2}\lambda_ 1tr(\vec{W}_k\mathbf{\Omega}_k^{-1}\vec{W}_k^{T})\bigg]\bigg. + \frac{1}{2}\lambda_2||\vec{W}||_F^2\\&lt;br&gt;
&amp;amp;+\frac{1}{2}\lambda_3||\mathbf{\theta}||_F^2 + \frac{1}{2}\lambda_4||\vec{U}||_F^2, \end{aligned} \\&lt;br&gt;
&amp;amp; \quad \text{s.t.} \quad  \mathbf{\Omega}_k \ge 0 \quad \text{and} \quad tr(\mathbf{\Omega}_k) = 1, \quad k = 1, 2, &amp;hellip; ,K.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $f(\cdot)$ represents DNNs feature mapping as shown in Figure 2(b). $\mathbf{\theta}_k$ is the feature transformation network. $\mathbf{U}_k$ and $\vec{w}_k$ are output layer (e.g. softmax). The first constraint holds due to the fact that $\mathbf{\Omega}$ is defined as a task covariance matrix. The second constraint is used to restrict its complexity.&lt;/p&gt;
&lt;p&gt;In federated learning situation, training should be conducted on each node respectively. One intuitive thought is the centralized network topology in McMahan &lt;em&gt;et al.&lt;/em&gt; [2016], where one center node synchronously takes a weighted average parameters of every clients at each time step (Figure 2(a)). However,  this model faces the problems that in DNNs situation, far more parameters need to be calculated and transferred. Each node has heterogeneous computing performance and network bandwidth (Figure (b)). Setting one center node to synchronously collect all the parameters will induce high communication cost and low convergence speed. In order to overcome these problems, we design a decentralized topology, where each node only needs to share their parameters with neighbored nodes as shown in Figure 2(c)), where there is no communication between worker one and worker 4. Abandoning the central node induces the problem that parameters cannot be exchanged and synchronized amongst every nodes, which means that the centralized optimization method can not be achieved on this topology. To this end, we propose a Decentralized Periodic Averaging SGD (DPA-SGD) to tackle the optimization problem in decentralized topology.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&#34;decentralized-periodic-averaging-sgd&#34;&gt;Decentralized Periodic Averaging SGD&lt;/h3&gt;
&lt;p&gt;As for decentralized topology, due to the disappearing of central node, same central averaging method can not be applied. In order to overcome this problem, we come up with a novel optimization method, Decentralized Periodic Averaging SGD (DPA-SGD). The main idea of DPA-SGD is that during the communication period $\tau$, local SGD is applied on each node respectively, and synchronizing all the parameters at every $\tau$ iterations amongst its connected neighbors. Due to this decentralized diverse connection, one global $\mathbf{\Omega}$ can not represent the individual correlation. So we propose to use a distinct covariance matrix $\mathbf{\Omega}_k$ to represent their own mutual relationship. We also come up with an effective way to update the different $\mathbf{\Omega}_k$. To be specific, consider one particular node $m$ and its neighbor connected nodes as set $\mathcal{M}$.&lt;/p&gt;
&lt;p&gt;The new objective function can be defined as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp; \begin{aligned} \min_{\mathbf{\theta}, \vec{U},\vec{W},\mathbf{\Omega}}
&amp;amp;\sum_{k=1}^{K}\frac{1}{n_k}\bigg[\bigg.\sum_{i=1}^{n_k}l(f(\vec{x}_i^{k},\mathbf{\theta}_k,\vec{U}_k,\vec{w}_k),\vec{y}_i^k) \\&lt;br&gt;
&amp;amp;+ \frac{1}{2}\lambda_ 1tr(\vec{W}_k\mathbf{\Omega}_k^{-1}\vec{W}_k^{T})\bigg]\bigg. + \frac{1}{2}\lambda_2||\vec{W}||_F^2\\&lt;br&gt;
&amp;amp;+\frac{1}{2}\lambda_3||\mathbf{\theta}||_F^2 + \frac{1}{2}\lambda_4||\vec{U}||_F^2, \end{aligned} \\&lt;br&gt;
&amp;amp; \quad \text{s.t.} \quad  \mathbf{\Omega}_k \ge 0 \quad \text{and} \quad tr(\mathbf{\Omega}_k) = 1, \quad k = 1, 2, &amp;hellip; ,K.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $\vec{W}_k = (\vec{w}_1,\vec{w}_2,&amp;hellip;,\vec{w}_m,&amp;hellip;\vec{w}_{|\mathcal{M}|})\in\mathbb{R}^{d\times|\mathcal{M}|}$ is the parameters for $m$ and its neighbor tasks. The matrix $\mathbf{\Omega}_k\in\mathbb{R}^{|\mathcal{M}|\times{|\mathcal{M}|}}$ represents the correlation amongst nodes in set $\mathcal{M}$. Here in order to record the entire nodes connection in the network, we introduce a &lt;em&gt;node connection matrix&lt;/em&gt; $\vec{M}\in\mathbb{R}^{K\times{K}}$ represents the neighbor relationships for each nodes, where $M_{i, j}$ is a value that indicates node $i$ and $j$ are connected as shown in Figure 3, where worker one is only connected with worker two and four. Note that, if $\vec{M} = \vec{I}$ (Identity matrix), then every nodes are independent and update the parameters respectively. If $\vec{M} = \vec{J}$ (one for each element), the model is degenerated into centralized model. We study the model performance under sparse matrix $\vec{M}$ and find that similar results can be achieved as a ring network topology, which each node is only connected with its nearby two nodes, as illustrated in Figure 3.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-decentralized-periodic-averaging-sgd&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/decentralizedSGD_huba6fab2315ef5e6a14b9cf31294d583a_115107_2000x2000_fit_q75_lanczos.jpg&#34; data-caption=&#34;Decentralized Periodic Averaging SGD&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/decentralizedSGD_huba6fab2315ef5e6a14b9cf31294d583a_115107_2000x2000_fit_q75_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;961&#34; height=&#34;454&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Decentralized Periodic Averaging SGD
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;To solve this non-convex problem, we apply the alternating optimization method  Zhang and Yeung [2012], where alternately updating parameters $\vec{X} = (\vec{W, U},\mathbf{\theta})$ and $\mathbf{\Omega}$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Optimizing&lt;/em&gt; $\mathbf{\theta}, \vec{W}$ and $\vec{U}$: For simplicity, we define set $\Xi = (\mathbf{\Omega}_1, \mathbf{\Omega}_2,&amp;hellip;,\mathbf{\Omega_K})$ to represent the correlation matrix for every nodes. Fixing $\Xi$, we can use SGD method to update $\mathbf{\theta}, \vec{W}$ and $\vec{U}$ jointly. Our problem can then be reformulated as:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
G(\vec{W}, \vec{U}, \mathbf{\theta}|\Xi) &amp;amp;= \sum_{k=1}^{K}\frac{1}{n_k} \bigg[\bigg.
\sum_{i=1}^{n_k}l(f(\textbf{x}_i^{k},\mathbf{\theta}_k,\textbf{U}_k,\textbf{w}_k),\textbf{y}_i^k) \\&lt;br&gt;
&amp;amp; + \frac{1}{2} \lambda_1tr(\textbf{W}_k\mathbf{\Omega}_k^{-1}\textbf{W}_k^{T})
\bigg]\bigg. + \frac{1}{2}\lambda_2||\textbf{W}||_F^2 \\&lt;br&gt;
&amp;amp;+ \frac{1}{2}\lambda_3||\mathbf{\theta}||_F^2 + \frac{1}{2}\lambda_4||\textbf{U}||_F^2
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;We can calculate the gradient of $\vec{W}$, $\vec{U}$ and $\mathbf{\theta}$ respectively. Let $L = \sum_{k=1}^{K}\frac{1}{n_k}\sum_{i=1}^{n_k}l(f(\mathbf{x}_i^{k}, \mathbf{\theta}_k,\mathbf{u}_k,\mathbf{w}_k),\mathbf{y}_i^k)$. Then the gradient formulations for each node are:&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial{G(\vec{W}, \vec{U}, \mathbf{\theta}|\Xi)}}{\partial{\vec{w}_k}} =
\frac{\partial{L}}{\partial{\vec{w}_k}} +
\lambda_1\sum_{i=1}^{\mathcal{|M|}} \frac{1}{n_i}{\vec{w}_k}\mathbf{\Omega}_i^{-1} +
\lambda_2\vec{w}_k
$$&lt;/p&gt;
&lt;p&gt;where the summation is amongst all the nodes connected to node $k$,&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial{G(\vec{W}, \vec{U}, \mathbf{\theta}|\Xi)}}{\partial \mathbf{\theta}_k} = \frac{\partial{L}}{\partial\mathbf{\theta}_k} + \lambda_3\mathbf{\theta}_k,
$$&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial{G(\vec{W}, \vec{U}, \mathbf{\theta}|\Xi)}}{\partial{\vec{u}_k}} = \frac{\partial{L}}{\partial{\vec{u}_k}} + \lambda_4\vec{u}_k
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Optimizing&lt;/em&gt; $\Xi$: In paper Zhang and Yeung [2012], an analytical solution form is given for $\mathbf{\Omega}$:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{\Omega} = \frac{(\vec{W}^T\vec{W})^\frac{1}{2}}{tr((\vec{W}^T\vec{W})^{\frac{1}{2}})}
$$&lt;/p&gt;
&lt;p&gt;Apparently, if $\mathbf{w}_i$ and $\mathbf{w}_j$ are close to each other, $\mathbf{\Omega}$ will be large. However, the missing central node forbidding to average parameters globally. So here we propose a novel way to update each $\mathbf{\Omega}_k\in\Xi$:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{\Omega}_{t+1}^{(k)} \leftarrow \eta\frac{1}{|\mathcal{M}|}({\sum_{i=1}^{|\mathcal{M}|}}\frac{1}{n_i}\mathbf{\Omega}_{t}^{(i)} + \frac{(\vec{W}_k^T\vec{W}_k)^\frac{1}{2}}{tr((\vec{W}_k^T\vec{W}_k)^{\frac{1}{2}})})
$$&lt;/p&gt;
&lt;p&gt;The first averaging term can incorporate the nearby nodes correlation into its own and the second term captures the new correlation between its neighbors as shown in Figure 3.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;In general, the algorithm of DPA-SGD can be summarized as: while in local update period, each node calculates the gradient $g(\vec{X}_t^{(i)})$ based on one mini-batch of data and then update $\vec{X}^{(i)}$; For every synchronization per $\tau$ update, the novel update way of $\mathbf{\Omega}$ is conducted.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-algorithm-1-federated-multi-task-deep-learning-framework-with-decentralized-averaging-sgd&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/algorithm1_huebfc1ec7eb44c05c128ffe140125722d_379297_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;&amp;lt;strong&amp;gt;Algorithm 1&amp;lt;/strong&amp;gt;: Federated Multi-Task Deep Learning Framework with Decentralized Averaging SGD&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/algorithm1_huebfc1ec7eb44c05c128ffe140125722d_379297_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2508&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Algorithm 1&lt;/strong&gt;: Federated Multi-Task Deep Learning Framework with Decentralized Averaging SGD
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;advantages-of-our-algorithm&#34;&gt;Advantages of Our Algorithm&lt;/h3&gt;
&lt;p&gt;Here we illustrate system-wise advantages of DPA-SGD:&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-illustration-of-training-time-reducing-due-to-the-mechanism-of-dpa-sgd&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/advantage_huca52a4d98cc084a2f136be1565963362_287468_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Illustration of training time reducing due to the mechanism of DPA-SGD&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/advantage_huca52a4d98cc084a2f136be1565963362_287468_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2184&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Illustration of training time reducing due to the mechanism of DPA-SGD
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Faster convergence speed&lt;/strong&gt;. Figure 4 illustrates three reasons that DHA-SGD can speed up convergence.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Periodic averaging&lt;/em&gt; can alleviate the communication delay by reducing times of synchronization which only happen periodically. As we can see in Figure 4, the yellow blocks (communication) will largely be deleted due to the periodic averaging mechanism.&lt;/li&gt;
&lt;li&gt;This idle time can also be significantly reduced through periodic averaging as shown in Figure 4.&lt;/li&gt;
&lt;li&gt;In the decentralized topology, because a worker only needs to exchange gradients with its neighbors, another worker with slow computation and communication will not interrupt its iteration. For example, worker 2 in the above figure can synchronize earlier without waiting for worker 4. Thus, DHA-SGD can largely reduce convergence time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Fewer communication rounds&lt;/strong&gt;. The periodic averaging strategy can reduce the number of communication rounds. Although our work focuses on optimizing the ratio of computation and communication rather than improving the communication cost for each iteration, gradient compression method (Deep Gradient Compression [Lin &lt;em&gt;et al.&lt;/em&gt;, 2017]) for each iteration can directly extendable to to our algorithm in a practical system.&lt;/p&gt;
&lt;h3 id=&#34;system-design&#34;&gt;System Design&lt;/h3&gt;






  



  
  











&lt;figure id=&#34;figure-federated-learning-system-design-and-protocol&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/system1_hu83de1cf0503563fdaf62e3d336a68002_546718_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Federated Learning System Design and Protocol&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/system1_hu83de1cf0503563fdaf62e3d336a68002_546718_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1536&#34; height=&#34;1380&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
    Federated Learning System Design and Protocol
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We have developed a real-world decentralized federated learning system. Our system is currently deployed in a cluster environment, consisting of 24 physical machines, each supporting 48 CPU cores. Suppose we run one process per CPU core, this system can run up to 1152 workers for parallel training. Our federated learning system enables centralized and decentralization training while supporting the Federated multi-task learning framework. It also supports user and training management in the Federated Learning scenario. As shown in Figure 5 (Upper), it includes training process management such as topology generation, user selection, configuration dispatching, on-device training, and statistics collection. In the topological generation step, we can select the appropriate eigenvalues for the topology matrix according to our the convergence property analysis, which determines the sparseness of the network topology to balance the model accuracy and the training speed. The user selection process can configure different strategies to select distinct users. Each user only requires training once for a model. In our experiment, users are selected according to the LEAF data set. This process can be further optimized in the future to choose users who are more meaningful to the model.&lt;/p&gt;
&lt;p&gt;The software architecture of each worker is shown in Figure 5 (Lower). Each work can be an IoT device, smartphone or the edge server.  It is an abstract architecture design for the on-device software system which unifies the system architecture for different on-device operating systems such as Android or iOS for the smartphone and the linux edge server. For the edge server, we implement this system design in a Python environment. For the smartphone, we develop the training worker system based on Android and iOS. The low-level communication is an abstract layer. In the edge server environment, we use the MPI protocol for exchanging gradient or other necessary information. As we can see, the communication sending and receiving threads are independent of the training threads. The collaborate through message queues. Currently, we only disclose the implementation of the MPI communication protocol in the source code. In the future, we consider open source our code to support more on-device platforms.&lt;/p&gt;
&lt;p&gt;To put it simply, another contribution of this paper is that we publish a practical federated learning system that can promote further research on distributed learning and especially federated learning.&lt;/p&gt;
&lt;h2 id=&#34;comparison-to-previous-works&#34;&gt;Comparison to Previous Works&lt;/h2&gt;
&lt;h3 id=&#34;federated-multi-task-learning&#34;&gt;Federated Multi-Task Learning&lt;/h3&gt;
&lt;p&gt;Early examples of research into federated learning. To address both statistical and system challenges, [Smith &lt;em&gt;et al.&lt;/em&gt;, 2017] and [Caldas &lt;em&gt;et al&lt;/em&gt;. 2018] propose a multi-task learning framework for federated learning and its related optimization algorithm, which extends early works from distributed machine learning, including SDCAShalev-Shwartz and Zhang [2013]; Yang [2013]; Yang &lt;em&gt;et al.&lt;/em&gt; [2013] and COCOAJaggi &lt;em&gt;et al.&lt;/em&gt; [2014]; Ma &lt;em&gt;et al.&lt;/em&gt; [2015]; Smith &lt;em&gt;et al.&lt;/em&gt; [2016]. The main limitation of Smith &lt;em&gt;et al.&lt;/em&gt; [2017] and Caldas &lt;em&gt;et al.&lt;/em&gt; [2018], however, is that &lt;strong&gt;strong duality is only guaranteed when the objective function is convex, which can not be generalized to the non-convex setting, especially deep neural networks&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another line of work related to federated multi-task learning is the cloud-based distributed multi-task learning [Ahmed &lt;em&gt;et al.&lt;/em&gt;, 2014; Jin &lt;em&gt;et al.&lt;/em&gt;, 2015; Mateos-Núñez &lt;em&gt;et al.&lt;/em&gt;, 2015; Wang &lt;em&gt;et al.&lt;/em&gt;, 2016; Baytas &lt;em&gt;et al.&lt;/em&gt;, 2016; Liu &lt;em&gt;et al.&lt;/em&gt;, 2017]. However, &lt;strong&gt;their assumption that the same amount of work is done locally on each node is prohibitive in federated settings&lt;/strong&gt;, and none of these works take into account the systems challenges that arise in the federated setting.&lt;/p&gt;
&lt;p&gt;In our work, we focus on training the deep learning model in the federated setting. To be more specific, &lt;strong&gt;in this work we further extend previous works to a generic multi-task deep learning framework and a more efficient optimization method. Different from previous works, we propose a decentralized approach for federated learning&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;stochastic-gradient-decent-optimization&#34;&gt;Stochastic Gradient Decent Optimization&lt;/h3&gt;
&lt;p&gt;In large scale distributed deep learning, to address the communication bottleneck, synchronized mini-batch SGD, which increase the computation to communication ratio, is widely used in the parameter server framework [Dean &lt;em&gt;et al.&lt;/em&gt;; Li &lt;em&gt;et al.&lt;/em&gt;, 2014; Cui &lt;em&gt;et al.&lt;/em&gt;, 2014] and popular deep learning systems such as Tensorflow [Abadi &lt;em&gt;et al.&lt;/em&gt;, 2016] and PyTorch [Paszke &lt;em&gt;et al.&lt;/em&gt;, 2017].&lt;/p&gt;
&lt;p&gt;Compared to this synchronized mini-batch SGD, Federated Averaging(FedAvg) SGD [Koneˇcný &lt;em&gt;et al.&lt;/em&gt;, 2016] in the federated setting empirically shows it has less communication rounds and it is also robust to non-IID data, which is now the state-of-the-art SGD algorithm for federated learning.&lt;/p&gt;
&lt;p&gt;Decentralized SGD, another approach to reducing communication, was successfully applied to deep learning [Jin &lt;em&gt;et al.&lt;/em&gt;, 2016; Jiang &lt;em&gt;et al.&lt;/em&gt;, 2017; Lian &lt;em&gt;et al.&lt;/em&gt;, 2017]. Instead of synchronizing with all workers, a worker in the decentralized SGD framework only needs to exchange gradient with its neighbors. Therefore, this sparse-connected topology can reduce the overall communication per iteration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our work aims to design a novel SGD algorithm for our multi-task deep learning framework&lt;/strong&gt;, which &lt;strong&gt;can leverage the advantages of periodic averaging SGD and decentralized SGD&lt;/strong&gt;. We called it as Decentralized Periodic Averaging SGD. Although recent work [Wang and Joshi, 2018] has this idea preliminarily, it does not provide adequate theoretical analysis and empirical evaluation in the federated setting.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;div style=&#34;overflow: auto;&#34;&gt;&lt;table id=&#34;leafdata-table&#34;&gt;&lt;tr&gt;&lt;th&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Dataset&lt;/span&gt;&lt;/th&gt;&lt;th&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Number of devices&lt;/span&gt;&lt;/th&gt;&lt;th&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Total samples&lt;/span&gt;&lt;/th&gt;&lt;th colspan=&#34;2&#34;&gt;&lt;span style=&#34;font-weight:bold&#34;&gt;Sample per device&lt;/span&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;mean&lt;/td&gt;&lt;td&gt;stdev&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FEMNIST&lt;/td&gt;&lt;td&gt;3,550&lt;/td&gt;&lt;td&gt;805,263&lt;/td&gt;&lt;td&gt;226.83&lt;/td&gt;&lt;td&gt;88.94&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sent140&lt;/td&gt;&lt;td&gt;660,120&lt;/td&gt;&lt;td&gt;1, 600, 498&lt;/td&gt;&lt;td&gt;2.42&lt;/td&gt;&lt;td&gt;4.71&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Shakespeare&lt;/td&gt;&lt;td&gt;2,288&lt;/td&gt;&lt;td&gt;106, 126&lt;/td&gt;&lt;td&gt;46.38&lt;/td&gt;&lt;td&gt;91.71&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;This table is a brief statistic of the dataset (LEAF) that we are using in the experiments. To be specific, FEMNIST is a handwritten digit database, where image classification problem can be conducted based on this dataset. Sent140 is a text dataset, which allows you to discover the sentiment of a brand or product. Shakespeare is also a text dataset, however the objective here is try to learn the pattern of the sentences and generate similar text style.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-accuracy-for-fedavg-single-task-and-ring-dpa-sgd-multi-task-vs-round-number&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc2-round_number_hu1b1935e33a88baa3b47754e16403a12d_48889_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Accuracy for FedAvg Single-task and Ring DPA-SGD Multi-task vs Round number&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc2-round_number_hu1b1935e33a88baa3b47754e16403a12d_48889_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;520&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Accuracy for FedAvg Single-task and Ring DPA-SGD Multi-task vs Round number
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We compare the performance between FedAvg, the current state-of-the-art Federated Learning algorithm, and our DPA-SGD on Multi-Task Learning Framework (DPA-SGD-MTL) with a ring topology network. From the Figure, we clearly see that the training speed is around 20% faster than the FedAvg. The Ring DPA-SGD-MTL’s accuracy is also comparable to the FedAvg.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;





  



  
  











&lt;figure id=&#34;figure-accuracy-for-fedavg-mtl-and-dpa-sgd-mtl-vs-walk-clock-time&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc1-time_huf55eceac0cc195dd4c9e2c46301c67c2_47880_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Accuracy for FedAvg-MTL and DPA-SGD MTL vs walk clock time&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc1-time_huf55eceac0cc195dd4c9e2c46301c67c2_47880_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;634&#34; height=&#34;503&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Accuracy for FedAvg-MTL and DPA-SGD MTL vs walk clock time
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;





  



  
  











&lt;figure id=&#34;figure-accuracy-for-fedavg-mtl-and-dpa-sgd-mtl-vs-round-number&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc1-round_number_hubf99043acb542c03ffabbe3bd8a40698_46387_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Accuracy for FedAvg-MTL and DPA-SGD MTL vs round number&#34;&gt;


  &lt;img data-src=&#34;https://www.zijianhu.com/project/dpa_sgd/images/results/acc1-round_number_hubf99043acb542c03ffabbe3bd8a40698_46387_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;662&#34; height=&#34;503&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Accuracy for FedAvg-MTL and DPA-SGD MTL vs round number
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We also compared the performance between FedAvg on Multi-Task Learning Framework (FedAvg-MTL) and our DPA-SGD on Multi-Task Learning Framework (Ring DPA-SGD-MTL) with a ring topology network. From the Figure, we clearly see that the training speed is around 20% faster than the FedAvg-MTL. The Ring DPA-SGD-MTL’s accuracy is slightly lower but comparable than the FedAvg-MTL . This proves that our convergence analysis that when decentralized the topology of the gradient exchanging, due to the sparsity of the topology has a negative impact on the convergence of the model. We have to balance the performance of the model and the training speed in the federated learning setting.&lt;/p&gt;
&lt;p&gt;We will conduct more experiment on model performance and training speed on communication and computation heterogeneous setting (delay = 0ms, 300ms, 1s, 3s), different number of works (4, 8, 16, 32, 64, 128, 256, 512), and different topologies (0.3, 0.6, 0.9). Hopefully, we can finish these experiments before the NeurIPS deadline.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;h3 id=&#34;appendix-i-convergence-analysis&#34;&gt;Appendix I: Convergence Analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Convergence of DPA-SGD&lt;/strong&gt;: If the communication period $\tau$ satisfies:&lt;/p&gt;
&lt;p&gt;$$
T \geq K^{3} \left( \frac{1+\zeta^{2}}{1-\zeta^{2}} \tau - 1 \right)^{2}
$$&lt;/p&gt;
&lt;p&gt;then with learning rate $\eta = \frac{K}{LK} \sqrt{\frac{K}{T}}$ , the average-squared gradient norm after K iterations is bounded by&lt;/p&gt;
&lt;p&gt;$$
\mathbb{E} \Bigg[ \frac{1}{T} \sum\limits_{t=1}^T {\left\lVert \nabla F(\vec u_{t}) \right\rVert}^{2} \Bigg] \leq \frac{2[F(\vec{x}_{1})-F_{inf}]}{\eta T} + \frac{2\sigma^{2}}{\sqrt{KT}} \
= \mathcal{O}(\frac{1}{\sqrt{KT}})
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to choose $\tau$, $\zeta$&lt;/strong&gt;: for a small number of well-connected workers, larger $\tau$ is more preferable; for a large number of workers, using a sparse mixing matrix and small $\tau$ gives
better convergence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Effect of Extreme Large $K$&lt;/strong&gt;: the iteration number T will be extreme large. To guaran-tee the convergence, try to reduce the worker number through system-wised optimization:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Streaming training&lt;/li&gt;
&lt;li&gt;upload on-device data to the edge data center.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;appendix-ii-poster&#34;&gt;Appendix II: Poster&lt;/h3&gt;
&lt;figure&gt;&lt;a href=&#34;attachments/fl-dpa-sgd-poster.pdf&#34;&gt;&lt;img src=&#34;images/fl-dpa-sgd-poster.png&#34;&gt;&lt;/a&gt;&lt;figcaption&gt;Poster (click to see the pdf)&lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Infant-Robot Interaction as an Early Intervention Strategy</title>
      <link>https://www.zijianhu.com/project/baby/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/project/baby/</guid>
      <description>&lt;p&gt;&lt;em&gt;The following project description is taken from &lt;a href=&#34;http://robotics.usc.edu/interaction/sponsors/desc.php?name=babies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interaction Lab website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Infants engage in motor babbling that allows them to explore their space and learn what movements produce
desired outcomes. Less motor babbling from infants can lead to developmental delays.
Our goal is to develop a socially assistive, non-contact, infant-robot interaction system to provide
contingent positive feedback to increase exploration and expand early movement practice.&lt;/p&gt;
&lt;p&gt;Towards this end, we are collaborating with physical therapists to create approaches to predict the
developmental status of infants using wearable sensors; running user studies that explore various robot
rewards for contingent activities for the infant, as well as measuring the infant&amp;rsquo;s ability to mimic the
robot; and using reinforcement learning to adjust the difficulty of the task presented by the robot to
increase the infant&amp;rsquo;s engagement with the task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;development-detail&#34;&gt;Development Detail&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PqTkw2weVjU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;For project development, my contributions includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting and Tracking two &lt;a href=&#34;https://www.sphero.com/sphero-sprk-plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sphero SPRK+&lt;/a&gt; robots with a wall-mounted camera&lt;/li&gt;
&lt;li&gt;Object Detection: apply transfer learning to YOLOv3 pre-trained with MS-COCO dataset&lt;/li&gt;
&lt;li&gt;Visual Tracking:
&lt;ul&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/abs/1812.11703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SiamRPN&lt;/a&gt;: Since SiamRPN outputs tracking confidence, detection is used only when confidence is below a threshold&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://arxiv.org/pdf/1611.08461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSRT tracker&lt;/a&gt;: CSRT does not output tracking confidence; detection is conducted with a predefined frequency to update the tracking location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Getting Started with NAO &#43; ROS</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/getting-started/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/getting-started/</guid>
      <description>&lt;p&gt;Make sure you have installed all the dependencies and configured &lt;strong&gt;PYTHONPATH&lt;/strong&gt; system variable correctly&lt;/p&gt;
&lt;p&gt;*&lt;em&gt;see &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/installation&#34;&gt;installation guide&lt;/a&gt; for detail&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;controlling-robot&#34;&gt;Controlling Robot&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Turn on the robot. See &lt;a href=&#34;http://doc.aldebaran.com/2-1/nao/getting_out_of_the_box.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this guide&lt;/a&gt; for detail&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the robot bridge on your computer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ roslaunch nao_bringup nao_full_py.launch nao_ip:=&amp;lt;robot_ip&amp;gt; \
roscore_ip:=&amp;lt;roscore_ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start the robot&amp;rsquo;s default configuration with the following publisher:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;joint_states&lt;/li&gt;
&lt;li&gt;tf&lt;/li&gt;
&lt;li&gt;top camera&lt;/li&gt;
&lt;li&gt;bottom camera&lt;/li&gt;
&lt;li&gt;left sonar&lt;/li&gt;
&lt;li&gt;right sonar&lt;/li&gt;
&lt;li&gt;microphone&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To visualize the robot, open &lt;strong&gt;rviz&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosrun rviz rviz
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;In top bar, go to &lt;code&gt;File-&amp;gt;Open Config&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;navigate to &lt;code&gt;&amp;lt;your catkin workspace&amp;gt;/src/nao_robot/nao_description/config&lt;/code&gt; and open the file with &lt;strong&gt;.rviz&lt;/strong&gt; extension
&lt;ul&gt;
&lt;li&gt;make sure you have &lt;a href=&#34;http://wiki.ros.org/nao_meshes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_meshes&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;you should see something similar to the below screenshot
&lt;img src=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started?action=AttachFile&amp;amp;do=get&amp;amp;target=NaoRviz.png&#34; alt=&#34;NAO rviz&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Controlling the robot&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;execute &lt;code&gt;rosnode list&lt;/code&gt; to check if &lt;strong&gt;/nao_walker&lt;/strong&gt; node is running&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To turn on the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/enable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To turn off the motors&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rosservice call /body_stiffness/disable &amp;quot;{}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;once the motors are on, use the following command to move the robot in x-direction&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 1.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To stop the robot, run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rostopic pub -1 /cmd_vel geometry_msgs/Twist \
&#39;{linear: {x: 0.0, y: 0.0, z: 0.0}, \
angular: {x: 0.0, y: 0.0, z: 0.0}}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;next-naoqi-sdk-guidepostnao-tutorialnao-sdk&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/nao-sdk&#34;&gt;NAOqi SDK Guide&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Getting-Started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting started with ROS for Nao, including NAOqi and rviz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao_bringup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nao_bringup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: Installation</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/installation/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/installation/</guid>
      <description>&lt;h2 id=&#34;installing-the-sdk&#34;&gt;Installing the SDK&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Make sure you have the following installed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.python.org/download/releases/2.7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cmake.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMake&lt;/a&gt; version 2.8.3 or higher&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the following from &lt;a href=&#34;https://community.aldebaran.com/en/resources/software&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aldebaran Community&lt;/a&gt; website (&lt;em&gt;you need to register an account in order to download the files&lt;/em&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pynaoqi-python-2.7-naoqi-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;naoqi-sdk-2.1.2.x-linux64.tar.gz&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;[optional]&lt;/em&gt;&lt;/strong&gt; choregraphe-suite-[2.1.4 or 2.1.2].x-linux64.tar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command and replace &lt;strong&gt;2.1.2.x&lt;/strong&gt; with the version you downloaded&lt;/p&gt;
&lt;p&gt;Unzip the tar files&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir ~/naoqi
$ tar xzf &amp;lt;path to NAOqi C++ SDK&amp;gt;/naoqi-sdk-2.1.2.x-linux64.tar -C ~/naoqi/naoqi-sdk-2.1.2-linux64
$ tar xzf &amp;lt;path to NAOqi Python SDK&amp;gt;/pynaoqi-python2.7-2.1.2.x-linux64.tar -C ~/naoqi/pynaoqi-python2.7-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check the installation by executing NAOqi&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ~/naoqi/naoqi-sdk-2.1.2.17-linux64/naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see output similiar to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Starting NAOqi version 2.1.2.17
.
.
.
NAOqi is ready...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;strong&gt;CTRL-C&lt;/strong&gt; to exit&lt;/p&gt;
&lt;p&gt;Now we need to add &lt;strong&gt;NAOqi SDK&lt;/strong&gt; to system variables. Add the following lines at the end of &lt;strong&gt;~/.bashrc&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export PYTHONPATH=~/naoqi/pynaoqi-python2.7-2.1.2-linux64:$PYTHONPATH
$ export AL_DIR=~/naoqi/naoqi-sdk-2.1.2-linux64
$ export AL_DIR_SIM=~/naoqi/naoqi-sdk-2.1.2-linux64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Execute &lt;code&gt;source ~/.bashrc&lt;/code&gt; to apply the changes&lt;/p&gt;
&lt;p&gt;Verify &lt;em&gt;in python console&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import naoqi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if correctly installed, there should be no error&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-ros&#34;&gt;Install ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://wiki.ros.org/kinetic/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Official ROS Installation Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-nao-package-for-ros&#34;&gt;Install &lt;strong&gt;NAO package&lt;/strong&gt; for ROS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install the packages needed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;replace kinetic to your ROS version if needed&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-driver-base ros-kinetic-move-base-msgs \
ros-kinetic-octomap ros-kinetic-octomap-msgs ros-kinetic-humanoid-msgs \
ros-kinetic-humanoid-nav-msgs ros-kinetic-camera-info-manager \
ros-kinetic-camera-info-manager-py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the main package with &lt;code&gt;sudo apt-get install ros-kinetic-nao-robot&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for robot control&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install ros-kinetic-nao-bringup ros-kinetic-naoqi-pose \
ros-kinetic-nao-interaction ros-kinetic-nao-moveit-config \
ros-kinetic-naoqi-driver ros-kinetic-naoqi-driver-py \
ros-kinetic-naoqi-sensors-py ros-kinetic-nao-dcm-bringup \
ros-kinetic-moveit
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install packages for simulation&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Notice: to install nao_meshes package, you need to agree the policy&lt;/em&gt;
&lt;code&gt;sudo apt-get install ros-kinetic-rospack ros-kinetic-nao-meshes&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;next-getting-startedpostnao-tutorialgetting-started&#34;&gt;Next: &lt;a href=&#34;https://www.zijianhu.com/post/nao-tutorial/getting-started/&#34;&gt;Getting Started&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/python/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python SDK Install Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/dev/cpp/install_guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C++ SDK Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/nao/Tutorials/Installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Installation of ROS for usage with or on a NAO robot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NAO Tutorial: NAOqi SDK</title>
      <link>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 -0800</pubDate>
      <guid>https://www.zijianhu.com/post/nao-tutorial/nao-sdk/</guid>
      <description>&lt;h2 id=&#34;before-starting&#34;&gt;Before starting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you have &lt;strong&gt;Choregraphe suite&lt;/strong&gt; installed
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt; for detail&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--## Hello World Example--&gt;
&lt;h3 id=&#34;using-choregraphe&#34;&gt;Using Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python-in-choregraphe&#34;&gt;Using Python in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_script.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-dialog-topic-in-choregraphe&#34;&gt;Using Dialog topic in Choregraphe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe_dialog.htmll&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-python&#34;&gt;Using Python&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Follow &lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_python.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/installing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choregraphe Suite Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://doc.aldebaran.com/2-1/getting_started/helloworld_choregraphe.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hello World 1 - using Choregraphe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
